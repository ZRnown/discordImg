import os
import torch
import numpy as np
import threading
from typing import List, Optional, Union, Dict
import logging
from pathlib import Path
from PIL import Image
from transformers import AutoImageProcessor, AutoModel
from ultralytics import YOLO
try:
    from .config import config
except ImportError:
    from config import config
from functools import lru_cache
import hashlib

logger = logging.getLogger(__name__)

# ä½¿ç”¨ç±»çº§å•ä¾‹æ¨¡å¼
class FeatureExtractorSingleton:
    """ç‰¹å¾æå–å™¨å•ä¾‹ç®¡ç†å™¨"""
    _instance = None
    _lock = threading.Lock()

    @classmethod
    def get_instance(cls):
        with cls._lock:
            if cls._instance is None:
                logger.info("ğŸš€ åˆ›å»ºç‰¹å¾æå–å™¨å•ä¾‹å®ä¾‹...")
                cls._instance = DINOv2FeatureExtractor()
                logger.info("âœ… ç‰¹å¾æå–å™¨å•ä¾‹å®ä¾‹åˆ›å»ºå®Œæˆ")
        return cls._instance

def get_feature_extractor():
    """è·å–ç‰¹å¾æå–å™¨å•ä¾‹å®ä¾‹"""
    return FeatureExtractorSingleton.get_instance()

class DINOv2FeatureExtractor:
    """
    "çŒé¹°"æ¶æ„ç‰¹å¾æå–å™¨
    DINOv2 (å¤§è„‘) + YOLO-World (çœ¼ç›)
    ä¸“ä¸ºé‹ç±»è¯†åˆ«ä¼˜åŒ–ï¼Œè‡ªåŠ¨è£å‰ªé‹å­ä¸»ä½“åæå–é«˜ç²¾åº¦ç‰¹å¾
    """

    def __init__(self):
        self.device = torch.device(config.DEVICE)
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–çŒé¹°AIå¼•æ“ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½YOLOv8-Nano (çœ¼ç› - ä¸»ä½“æ£€æµ‹)
        self._load_yolo_detector()

        # åŠ è½½DINOv2 (å¤§è„‘ - ç‰¹å¾æå–)
        self._load_dino_model()

        # åˆå§‹åŒ–ç¼“å­˜ç”¨äºæ£€æµ‹ç»“æœ
        self._detection_cache = {}

    def _get_image_hash(self, image_path: str) -> str:
        """è®¡ç®—å›¾ç‰‡æ–‡ä»¶çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        try:
            with open(image_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„+ä¿®æ”¹æ—¶é—´ä½œä¸ºå¤‡ç”¨
            import os
            stat = os.stat(image_path)
            return hashlib.md5(f"{image_path}:{stat.st_mtime}".encode()).hexdigest()

    def _load_yolo_detector(self):
        """å¼ºåˆ¶åŠ è½½YOLO-Worldæ¨¡å‹ç”¨äºå•†å“è¯†åˆ«"""
        try:
            logger.info("ğŸ”¥ å¼ºåˆ¶åŠ è½½YOLO-Worldæ¨¡å‹...")

            # å¼ºåˆ¶ä½¿ç”¨YOLO-Worldï¼Œä¸å…è®¸é™çº§
            self.detector = YOLO('yolov8s-world.pt')

            # [æ ¸å¿ƒé…ç½®] å®šä¹‰å…¨è‡ªåŠ¨è¯†åˆ«çš„èŒƒå›´
            # ä¼˜åŒ–åçš„å•†å“ç±»åˆ«ï¼Œè¦†ç›–å¾®åº—/ä»£è´­åœºæ™¯95%çš„å•†å“
            # YOLO-World ä¼šè‡ªåŠ¨å¿½ç•¥äººè„¸ã€æ‰‹ã€å®¶å…·ã€èƒŒæ™¯
            self.target_classes = [
                # é‹ç±» (é«˜ä¼˜å…ˆçº§)
                "shoe", "sneaker", "boot", "sandal", "slipper", "heel",
                # æœè£… (é«˜ä¼˜å…ˆçº§)
                "shirt", "t-shirt", "jacket", "coat", "pants", "jeans",
                "dress", "skirt", "shorts", "hoodie", "sweater", "suit",
                # åŒ…è¢‹é…é¥° (ä¸­ä¼˜å…ˆçº§)
                "bag", "handbag", "backpack", "wallet", "belt", "hat", "cap",
                "watch", "jewelry", "necklace", "ring", "glasses",
                # ç”µå­äº§å“ (ä¸­ä¼˜å…ˆçº§)
                "phone", "laptop", "headphone", "camera", "watch",
                # å®¶å±…ç”¨å“ (ä½ä¼˜å…ˆçº§)
                "toy", "box", "bottle", "cup", "lamp"
            ]

            # å°†è¿™äº›ç±»åˆ«æ³¨å…¥æ¨¡å‹
            self.detector.set_classes(self.target_classes)

            logger.info("ğŸ‰ YOLO-Worldæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            logger.info(f"ğŸ¯ æ”¯æŒè‡ªåŠ¨è¯†åˆ« {len(self.target_classes)} ç§å•†å“ç±»åˆ«")
            logger.info(f"ğŸ“‹ YOLO-Worldç›®æ ‡ç±»åˆ«: {', '.join(self.target_classes[:10])}...")
            logger.info("âš¡ YOLO-Worldä¼˜åŒ–è¯´æ˜: ä½¿ç”¨å¤šç»´åº¦è¯„åˆ†(é¢ç§¯Ã—ç½®ä¿¡åº¦Ã—ä½ç½®Ã—ç±»åˆ«æƒé‡)ï¼Œæ˜¾è‘—æå‡è£å‰ªå‡†ç¡®ç‡")

            # éªŒè¯CLIPåº“æ˜¯å¦æ­£ç¡®å®‰è£…
            try:
                import clip
                logger.info(f"âœ… CLIPåº“ç‰ˆæœ¬éªŒè¯: {getattr(clip, '__version__', 'æœªçŸ¥')}")
                if hasattr(clip, 'load'):
                    logger.info("âœ… CLIP.loadæ–¹æ³•å¯ç”¨")
                else:
                    logger.warning("âš ï¸ CLIP.loadæ–¹æ³•ä¸å¯ç”¨ï¼Œå¯èƒ½å½±å“YOLO-Worldæ€§èƒ½")
            except ImportError as e:
                logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥CLIPåº“: {e}")

        except Exception as e:
            logger.error(f"ğŸ’¥ YOLO-Worldæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯CLIPç›¸å…³çš„é—®é¢˜ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
            if "clip" in str(e).lower():
                logger.warning("ğŸ” æ£€æµ‹åˆ°CLIPåº“é—®é¢˜ï¼Œå°è¯•å¤‡ç”¨åŠ è½½æ–¹å¼...")

                try:
                    # å°è¯•ä¸ä¾èµ–CLIPçš„åŠ è½½æ–¹å¼
                    import ultralytics
                    logger.info(f"Ultralyticsç‰ˆæœ¬: {ultralytics.__version__}")

                    # ç›´æ¥åˆ›å»ºYOLO-Worldå®ä¾‹ï¼Œä¸è®¾ç½®ç±»åˆ«
                    self.detector = YOLO('yolov8s-world.pt')
                    self.target_classes = None  # ä¸è®¾ç½®è‡ªå®šä¹‰ç±»åˆ«

                    logger.warning("âš ï¸ YOLO-Worldä»¥åŸºç¡€æ¨¡å¼åŠ è½½ (æ— è‡ªå®šä¹‰ç±»åˆ«)")
                    logger.warning("ğŸ“Š å½±å“: å°†ä½¿ç”¨YOLO-Worldçš„å†…ç½®80ç±»è¿›è¡Œæ£€æµ‹")
                    logger.info("âœ… YOLO-WorldåŸºç¡€æ¨¡å¼åŠ è½½æˆåŠŸ")

                except Exception as backup_error:
                    logger.error(f"ğŸ’¥ å¤‡ç”¨åŠ è½½æ–¹å¼ä¹Ÿå¤±è´¥: {backup_error}")
                    logger.error("ğŸ”¥ ç”¨æˆ·è¦æ±‚å¼ºåˆ¶ä½¿ç”¨YOLO-Worldï¼Œä½†æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ï¼")
                    logger.error("ğŸ’¡ æœ€ç»ˆè§£å†³æ–¹æ¡ˆ:")
                    logger.error("   1. pip uninstall clip torch torchvision ultralytics")
                    logger.error("   2. pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu")
                    logger.error("   3. pip install ultralytics")
                    logger.error("   4. pip install git+https://github.com/openai/CLIP.git")
                    raise RuntimeError("YOLO-WorldåŠ è½½å¤±è´¥ï¼Œæ‰€æœ‰å¤‡ç”¨æ–¹æ¡ˆå‡æ— æ•ˆ") from e
            else:
                # ä¸æ˜¯CLIPé—®é¢˜ï¼Œç›´æ¥æŠ¥é”™
                logger.error("ğŸ”¥ YOLO-WorldåŠ è½½å¤±è´¥ï¼Œé”™è¯¯ä¸ç›¸å…³CLIPåº“")
                logger.error("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œultralyticsç‰ˆæœ¬")
                raise RuntimeError("YOLO-WorldåŠ è½½å¤±è´¥") from e

    def _load_dino_model(self):
        """åŠ è½½DINOv2æ¨¡å‹ç”¨äºç‰¹å¾æå–"""
        try:
            model_name = config.DINO_MODEL_NAME
            logger.info(f"åŠ è½½DINOv2ç‰¹å¾æ¨¡å‹: {model_name}...")

            self.processor = AutoImageProcessor.from_pretrained(model_name)
            self.model = AutoModel.from_pretrained(model_name)

            # å®‰å…¨åœ°å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œé¿å…meta tensoré”™è¯¯
            try:
                if hasattr(self.model, 'to'):
                    self.model = self.model.to(self.device)
                else:
                    logger.warning("æ¨¡å‹æ²¡æœ‰to()æ–¹æ³•ï¼Œä½¿ç”¨åŸæ¨¡å‹")
            except Exception as device_error:
                logger.warning(f"æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡å¤±è´¥: {device_error}ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                try:
                    # å°è¯•ä½¿ç”¨to_emptyæ–¹æ³•
                    if hasattr(self.model, 'to_empty'):
                        self.model = self.model.to_empty(device=self.device)
                    else:
                        logger.error("æ¨¡å‹ä¸æ”¯æŒto_emptyæ–¹æ³•ï¼Œä½¿ç”¨CPU")
                        self.device = torch.device('cpu')
                        self.model = self.model.to(self.device)
                except Exception as fallback_error:
                    logger.error(f"æ‰€æœ‰è®¾å¤‡ç§»åŠ¨æ–¹æ³•éƒ½å¤±è´¥: {fallback_error}")
                    raise

            self.model.eval()
            logger.info("âœ… DINOv2æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ DINOv2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError("DINOv2æ¨¡å‹åŠ è½½å¤±è´¥") from e

    def _crop_main_object(self, image_path: str) -> Image.Image:
        """å…¨è‡ªåŠ¨è£å‰ªå•†å“ä¸»ä½“

        å…¨è‡ªåŠ¨è£å‰ªé€»è¾‘ï¼š
        1. åœ¨é¢„è®¾çš„å•†å“ç±»åˆ«ä¸­æ£€æµ‹æ‰€æœ‰ç‰©ä½“
        2. è‡ªåŠ¨è¿‡æ»¤æ‰èƒŒæ™¯ã€äººã€æ‰‹
        3. åœ¨å‰©ä¸‹çš„å•†å“ä¸­ï¼Œé€‰å‡ºæœ€æ˜¾è‘—çš„ä¸€ä¸ªï¼ˆæœ€å¤§+æœ€ä¸­å¿ƒï¼‰
        """
        try:
            if not config.USE_YOLO_CROP:
                return Image.open(image_path).convert("RGB")

            # æ£€æŸ¥ç¼“å­˜
            image_hash = self._get_image_hash(image_path)
            if image_hash in self._detection_cache:
                logger.debug("ä½¿ç”¨ç¼“å­˜çš„æ£€æµ‹ç»“æœ")
                cached_result = self._detection_cache[image_hash]
                if cached_result is None:
                    # ç¼“å­˜ä¸­è¡¨ç¤ºæœªæ£€æµ‹åˆ°å•†å“
                    return Image.open(image_path).convert("RGB")
                # è¿”å›ç¼“å­˜çš„è£å‰ªç»“æœ
                return cached_result

            # conf=0.1: é™ä½é—¨æ§›ï¼Œå®å¯å¤šæ£€ä¸è¦æ¼æ£€ï¼Œåæ­£æˆ‘ä»¬æœ‰é€»è¾‘è¿‡æ»¤
            results = self.detector(image_path, conf=0.1, verbose=False)

            if not results or len(results[0].boxes) == 0:
                logger.info("æœªæ£€æµ‹åˆ°é€šç”¨å•†å“ï¼Œé™çº§ä½¿ç”¨åŸå›¾")
                # ç¼“å­˜æœªæ£€æµ‹åˆ°å•†å“çš„ç»“æœ
                self._detection_cache[image_hash] = None
                return Image.open(image_path).convert("RGB")

            boxes = results[0].boxes
            img = Image.open(image_path).convert("RGB")
            img_w, img_h = img.size
            center_x, center_y = img_w / 2, img_h / 2

            # --- æ™ºèƒ½è¯„åˆ†é€»è¾‘ ---
            # åœ¨æ‰€æœ‰æ£€æµ‹åˆ°çš„"å•†å“"ä¸­ï¼Œé€‰å‡ºä¸»è§’

            best_box = None
            max_score = -1

            for box in boxes:
                # 1. è·å–åæ ‡
                coords = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]
                x1, y1, x2, y2 = coords

                # 2. è®¡ç®—é¢ç§¯
                width = x2 - x1
                height = y2 - y1
                area = width * height

                # 3. è®¡ç®—ç¦»å›¾ç‰‡ä¸­å¿ƒçš„è·ç¦»
                box_center_x = x1 + width / 2
                box_center_y = y1 + height / 2
                dist_to_center = ((box_center_x - center_x)**2 + (box_center_y - center_y)**2) ** 0.5

                # 4. ç»¼åˆè¯„åˆ†å…¬å¼ï¼š
                # é¢ç§¯è¶Šå¤§è¶Šå¥½ (æƒé‡ 0.7)
                # è¶Šé ä¸­å¿ƒè¶Šå¥½ (æƒé‡ 0.3)
                # è¿™ä¸ªå…¬å¼èƒ½ä¿è¯ï¼šå³ä½¿è§’è½é‡Œæœ‰ä¸ªå¤§åŒ…ï¼Œä¹Ÿä¼šä¼˜å…ˆé€‰ä¸­é—´çš„å°é‹å­
                norm_area = area / (img_w * img_h)
                norm_dist = 1 - (dist_to_center / ((img_w**2 + img_h**2)**0.5))

                score = (norm_area * 0.7) + (norm_dist * 0.3) + (float(box.conf) * 0.1)

                if score > max_score:
                    max_score = score
                    best_box = coords

            if best_box is None:
                logger.info("æœªæ‰¾åˆ°åˆé€‚çš„å•†å“æ¡†ï¼Œä½¿ç”¨åŸå›¾")
                # ç¼“å­˜å¤±è´¥ç»“æœ
                self._detection_cache[image_hash] = None
                return img

            # æ‰§è¡Œè£å‰ª
            x1, y1, x2, y2 = best_box

            # æ‰©å…… 5% - 10% çš„è¾¹ç¼˜ï¼Œä¿ç•™ä¸€ç‚¹ç‚¹ä¸Šä¸‹æ–‡
            pad_x = (x2 - x1) * 0.05
            pad_y = (y2 - y1) * 0.05

            crop_box = (
                max(0, x1 - pad_x),
                max(0, y1 - pad_y),
                min(img_w, x2 + pad_x),
                min(img_h, y2 + pad_y)
            )

            cropped_img = img.crop(crop_box)
            logger.info(f"æˆåŠŸè£å‰ªå•†å“åŒºåŸŸ: {crop_box}")

            # ç¼“å­˜æˆåŠŸç»“æœ
            self._detection_cache[image_hash] = cropped_img.copy()

            return cropped_img

        except Exception as e:
            logger.warning(f"è‡ªåŠ¨è£å‰ªå‡ºé”™: {e}, ä½¿ç”¨åŸå›¾")
            # ç¼“å­˜å¤±è´¥ç»“æœ
            try:
                image_hash = self._get_image_hash(image_path)
                self._detection_cache[image_hash] = None
            except:
                pass
            return Image.open(image_path).convert("RGB")
        """ä½¿ç”¨YOLO-Worldç²¾å‡†è£å‰ªé‹ç±»ä¸»ä½“"""
        try:
            if not config.USE_YOLO_CROP:
                return Image.open(image_path).convert("RGB")

            # æ£€æŸ¥ç¼“å­˜
            image_hash = self._get_image_hash(image_path)
            if image_hash in self._detection_cache:
                logger.debug("ä½¿ç”¨ç¼“å­˜çš„æ£€æµ‹ç»“æœ")
                cached_result = self._detection_cache[image_hash]
                if cached_result is None:
                    # ç¼“å­˜ä¸­è¡¨ç¤ºæœªæ£€æµ‹åˆ°é‹å­
                    return Image.open(image_path).convert("RGB")
                # è¿”å›ç¼“å­˜çš„è£å‰ªç»“æœ
                return cached_result

            # ä¼˜åŒ–æ£€æµ‹å‚æ•°ï¼Œæé«˜å‡†ç¡®ç‡
            # ä½¿ç”¨æ›´ä½çš„ç½®ä¿¡åº¦ä½†æ›´é«˜çš„è´¨é‡é˜ˆå€¼
            results = self.detector(image_path, conf=0.05, iou=0.5, verbose=False)

            if not results or len(results[0].boxes) == 0:
                logger.info("æœªæ£€æµ‹åˆ°é€šç”¨å•†å“ï¼Œé™çº§ä½¿ç”¨åŸå›¾")
                # ç¼“å­˜æœªæ£€æµ‹åˆ°å•†å“çš„ç»“æœ
                self._detection_cache[image_hash] = None
                return Image.open(image_path).convert("RGB")

            boxes = results[0].boxes
            img = Image.open(image_path).convert("RGB")
            img_w, img_h = img.size
            center_x, center_y = img_w / 2, img_h / 2

            # --- ä¼˜åŒ–å€™é€‰é€»è¾‘ (YOLO-Worldå¢å¼ºç‰ˆ) ---
            # å¤šç»´åº¦è¯„åˆ†ï¼šé¢ç§¯ã€ç½®ä¿¡åº¦ã€ä½ç½®ã€é•¿å®½æ¯”

            candidates = []
            for box in boxes:
                coords = box.xyxy[0].cpu().numpy()
                conf = float(box.conf)
                cls = int(box.cls)

                w = coords[2] - coords[0]
                h = coords[3] - coords[1]
                area = w * h

                # è¿‡æ»¤å¤ªå°æˆ–å¤ªå¤§bbox
                if area < img_w * img_h * 0.01:  # è‡³å°‘å å›¾ç‰‡é¢ç§¯1%
                    continue
                if area > img_w * img_h * 0.9:   # æœ€å¤šå å›¾ç‰‡é¢ç§¯90%
                    continue

                # è®¡ç®—é•¿å®½æ¯” (è¿‡æ»¤æç«¯æ¯”ä¾‹)
                aspect_ratio = max(w/h, h/w)
                if aspect_ratio > 5:  # å¤ªç»†é•¿
                    continue

                # è®¡ç®—ä¸­å¿ƒè·ç¦»æƒé‡ (è¶Šé è¿‘ä¸­å¿ƒè¶Šå¥½)
                box_cx = coords[0] + w/2
                box_cy = coords[1] + h/2
                dist_from_center = ((box_cx - center_x)**2 + (box_cy - center_y)**2) ** 0.5
                max_dist = ((img_w/2)**2 + (img_h/2)**2) ** 0.5
                center_weight = 1 - (dist_from_center / max_dist)

                # è®¡ç®—ç±»åˆ«æƒé‡ (é‹ç±»å’Œæœè£…æ›´é«˜æƒé‡)
                class_name = self.target_classes[cls] if cls < len(self.target_classes) else "unknown"
                class_weight = 1.5 if any(keyword in class_name.lower() for keyword in ['shoe', 'shirt', 'pants', 'dress']) else 1.0

                # ç»¼åˆè¯„åˆ†
                score = area * conf * center_weight * class_weight

                candidates.append({
                    'coords': coords,
                    'score': score,
                    'conf': conf,
                    'area': area,
                    'class': class_name
                })

            # é€‰æ‹©æœ€ä½³å€™é€‰
            if not candidates:
                logger.info("æœªæ‰¾åˆ°åˆé€‚çš„å•†å“æ¡†ï¼Œä½¿ç”¨åŸå›¾")
                return img

            best_candidate = max(candidates, key=lambda x: x['score'])
            best_box = best_candidate['coords']

            logger.info(f"é€‰æ‹©æœ€ä½³æ£€æµ‹æ¡†: {best_candidate['class']}, ç½®ä¿¡åº¦: {best_candidate['conf']:.3f}, å¾—åˆ†: {best_candidate['score']:.1f}")

            if best_box is None:
                logger.info("æœªæ‰¾åˆ°åˆé€‚çš„é‹å­æ¡†ï¼Œä½¿ç”¨åŸå›¾")
                return img

            # è£å‰ªé€‰ä¸­çš„é‹å­
            x1, y1, x2, y2 = best_box

            # æ‰©å……5%çš„è¾¹ç¼˜ï¼Œé˜²æ­¢åˆ‡æ‰è¾¹ç¼˜ç‰¹å¾
            pad_x = (x2 - x1) * 0.05
            pad_y = (y2 - y1) * 0.05

            crop_box = (
                max(0, x1 - pad_x),
                max(0, y1 - pad_y),
                min(img_w, x2 + pad_x),
                min(img_h, y2 + pad_y)
            )

            cropped_img = img.crop(crop_box)
            logger.info(f"æˆåŠŸè£å‰ªé‹å­åŒºåŸŸ: {crop_box}")

            # ç¼“å­˜ç»“æœ
            self._detection_cache[image_hash] = cropped_img.copy()

            return cropped_img

        except Exception as e:
            logger.warning(f"é‹å­è£å‰ªå¼‚å¸¸: {e}")
            # ç¼“å­˜å¤±è´¥ç»“æœ
            self._detection_cache[image_hash] = None
            return Image.open(image_path).convert("RGB")

    def extract_feature(self, image_path: Union[str, Path]) -> Optional[np.ndarray]:
        """æå–å•å¼ å›¾ç‰‡çš„ç‰¹å¾å‘é‡ (384ç»´æˆ–768ç»´)"""
        try:
            image_path = str(image_path)

            if not os.path.exists(image_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None

            # 1. YOLOè£å‰ªä¸»ä½“
            img = self._crop_main_object(image_path)

            # 2. é¢„å¤„ç†ï¼ˆDINOv2ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            inputs = self.processor(images=img, return_tensors="pt").to(self.device)

            # 3. ç‰¹å¾æå–
            with torch.no_grad():
                outputs = self.model(**inputs)

            # 4. è·å–CLS tokenç‰¹å¾ (DINOv2çš„æœ€ä½³å®è·µ)
            # outputs.last_hidden_state.shape: [1, num_patches+1, dim]
            # ç¬¬0ä¸ªæ˜¯CLS tokenï¼Œä»£è¡¨æ•´å¼ å›¾çš„è¯­ä¹‰
            embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()

            # 5. L2å½’ä¸€åŒ– (å¯¹ä½™å¼¦ç›¸ä¼¼åº¦è‡³å…³é‡è¦)
            norm = np.linalg.norm(embedding)
            if norm > 0:
                embedding = embedding / norm

            # 6. ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32 (FAISSè¦æ±‚)
            return embedding.astype('float32')

        except Exception as e:
            logger.error(f"DINOv2ç‰¹å¾æå–å¤±è´¥ {image_path}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def extract_features_batch(self, image_paths: List[Union[str, Path]]) -> List[Optional[np.ndarray]]:
        """æ‰¹é‡æå–ç‰¹å¾å‘é‡"""
        results = []
        for image_path in image_paths:
            feature = self.extract_feature(image_path)
            results.append(feature)
        return results

    def get_status(self) -> Dict:
        """è·å–AIæ¨¡å‹çŠ¶æ€å’Œæ€§èƒ½ä¿¡æ¯"""
        status = {
            'device': str(self.device),
            'yolo_available': self.detector is not None,
            'yolo_type': 'None'
        }

        if self.detector is not None:
            if self.target_classes and len(self.target_classes) > 20:
                status['yolo_type'] = 'YOLO-World'
                status['target_classes_count'] = len(self.target_classes)
                status['target_classes'] = self.target_classes[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
            else:
                status['yolo_type'] = 'YOLOv8-Nano'
                status['target_classes_count'] = len(self.target_classes) if self.target_classes else 0

        status['detection_cache_size'] = len(self._detection_cache)
        status['confidence_threshold'] = 0.05
        status['iou_threshold'] = 0.5

        # æ€§èƒ½æç¤º
        tips = []
        if self.detector is None:
            tips.append("YOLOè£å‰ªå·²ç¦ç”¨ï¼Œå»ºè®®ä¿®å¤YOLOåŠ è½½é—®é¢˜ä»¥æå‡å‡†ç¡®æ€§")
        elif status['yolo_type'] == 'YOLOv8-Nano':
            tips.append("å½“å‰ä½¿ç”¨YOLOv8-Nanoï¼Œå»ºè®®å‡çº§ä¾èµ–ä»¥å¯ç”¨YOLO-Worldè·å¾—æ›´å¥½æ•ˆæœ")

        if status['detection_cache_size'] > 1000:
            tips.append("æ£€æµ‹ç¼“å­˜è¾ƒå¤§ï¼Œè€ƒè™‘å®šæœŸæ¸…ç†ç¼“å­˜")

        status['performance_tips'] = tips if tips else ["AIæ¨¡å‹è¿è¡Œæ­£å¸¸"]

        return status

# å‘åå…¼å®¹çš„åˆ«å
class FeatureExtractor(DINOv2FeatureExtractor):
    """å‘åå…¼å®¹çš„åˆ«å"""
    pass

# å•ä¾‹æ¨¡å¼è·å–å®ä¾‹
_feature_extractor = None

def get_feature_extractor() -> DINOv2FeatureExtractor:
    global _feature_extractor
    if _feature_extractor is None:
        _feature_extractor = DINOv2FeatureExtractor()
    return _feature_extractor
