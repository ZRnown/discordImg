import os

# === æ€§èƒ½ä¼˜åŒ–é…ç½® ===
# å…è®¸åº•å±‚è®¡ç®—åº“æ¯ä¸ªä»»åŠ¡ä½¿ç”¨å°‘é‡æ ¸å¿ƒã€‚
# é…åˆä¸Šå±‚æœ‰é™å¹¶å‘ï¼ˆä¾‹å¦‚ 3 ä¸ªå¹¶å‘ä»»åŠ¡ï¼‰ï¼Œåœ¨ 10 æ ¸ CPU ä¸Šæ›´å®¹æ˜“åƒæ»¡ä½†ä¸æ‰“æ¶ã€‚
# å¯é€šè¿‡ç¯å¢ƒå˜é‡ AI_INTRA_THREADS è°ƒæ•´ã€‚
try:
    from .config import config as _cfg
except Exception:
    try:
        from config import config as _cfg
    except Exception:
        _cfg = None

_intra_threads = None
try:
    if _cfg is not None and hasattr(_cfg, 'AI_INTRA_THREADS'):
        _intra_threads = int(_cfg.AI_INTRA_THREADS)
except Exception:
    _intra_threads = None

if not _intra_threads or _intra_threads <= 0:
    _intra_threads = int(os.getenv('AI_INTRA_THREADS', '3'))

os.environ["OMP_NUM_THREADS"] = str(_intra_threads)
os.environ["MKL_NUM_THREADS"] = str(_intra_threads)
os.environ["OPENBLAS_NUM_THREADS"] = str(_intra_threads)
os.environ["VECLIB_MAXIMUM_THREADS"] = str(_intra_threads)
os.environ["NUMEXPR_NUM_THREADS"] = str(_intra_threads)

import warnings
warnings.filterwarnings("ignore", message="Could not initialize NNPACK")
import torch

# === æ·»åŠ è¿™æ®µä»£ç  ===
try:
    # æ˜¾å¼ç¦ç”¨ NNPACK
    torch.backends.nnpack.enabled = False
except Exception:
    pass
# =================
import numpy as np
import cv2  # OpenCV for color histogram and structure comparison
import threading
import inspect
from typing import List, Optional, Union, Dict
import logging
from pathlib import Path
from PIL import Image
from transformers import AutoImageProcessor, AutoModel
from ultralytics import YOLO
try:
    from .config import config
except ImportError:
    from config import config
from functools import lru_cache
import hashlib

logger = logging.getLogger(__name__)

# å…¨å±€å•ä¾‹å˜é‡
_global_extractor = None
_extractor_lock = threading.Lock()

class DINOv2FeatureExtractor:
    """
    "çŒé¹°"æ¶æ„ç‰¹å¾æå–å™¨
    DINOv2 (å¤§è„‘) + YOLO-World (çœ¼ç›)
    ä¸“ä¸ºé‹ç±»è¯†åˆ«ä¼˜åŒ–ï¼Œè‡ªåŠ¨è£å‰ªé‹å­ä¸»ä½“åæå–é«˜ç²¾åº¦ç‰¹å¾
    """

    def __init__(self):
        self.device = torch.device(config.DEVICE)
        # ä¿æŠ¤ YOLO/DINO æ¨ç†ï¼Œé¿å…å¤šçº¿ç¨‹åŒæ—¶è®¿é—®å¯¼è‡´æ¨¡å‹çŠ¶æ€æŸå
        self.inference_lock = threading.Lock()
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–çŒé¹°AIå¼•æ“ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

        # åŠ è½½YOLOv8-Nano (çœ¼ç› - ä¸»ä½“æ£€æµ‹)
        self._load_yolo_detector()

        # åŠ è½½DINOv2 (å¤§è„‘ - ç‰¹å¾æå–)
        self._load_dino_model()

        # åˆå§‹åŒ–ç¼“å­˜ç”¨äºæ£€æµ‹ç»“æœ
        self._detection_cache = {}

    def _get_image_hash(self, image_path: str) -> str:
        """è®¡ç®—å›¾ç‰‡æ–‡ä»¶çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        try:
            with open(image_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶è·¯å¾„+ä¿®æ”¹æ—¶é—´ä½œä¸ºå¤‡ç”¨
            import os
            stat = os.stat(image_path)
            return hashlib.md5(f"{image_path}:{stat.st_mtime}".encode()).hexdigest()

    def _load_yolo_detector(self):
        """å¼ºåˆ¶åŠ è½½YOLO-Worldæ¨¡å‹ç”¨äºå•†å“è¯†åˆ«"""
        try:
            # å‡å°‘æ—¥å¿—çº§åˆ«
            logging.getLogger("ultralytics").setLevel(logging.WARNING)
            self.detector = YOLO(config.YOLO_MODEL_PATH)

            # [æ ¸å¿ƒé…ç½®] å®šä¹‰å…¨è‡ªåŠ¨è¯†åˆ«çš„èŒƒå›´
            # ä¼˜åŒ–åçš„å•†å“ç±»åˆ«ï¼Œè¦†ç›–å¾®åº—/ä»£è´­åœºæ™¯95%çš„å•†å“
            # YOLO-World ä¼šè‡ªåŠ¨å¿½ç•¥äººè„¸ã€æ‰‹ã€å®¶å…·ã€èƒŒæ™¯
            self.target_classes = [
                # é‹ç±»
                "shoe", "sneaker", "boot", "sandal", "slipper", "high heels",
                # ä¸Šè£…
                "t-shirt", "shirt", "jacket", "coat", "hoodie", "sweater", "suit", "vest", "jersey",
                # ä¸‹è£…
                "pants", "jeans", "shorts", "skirt", "trousers", "sweatpants",
                # åŒ…è¢‹
                "bag", "handbag", "backpack", "wallet", "purse", "suitcase", "tote bag",
                # é…é¥°/å°ä»¶
                "watch", "wristwatch", "glasses", "sunglasses", "hat", "cap", "beanie",
                "belt", "tie", "scarf", "gloves", "socks",
                # é¦–é¥°
                "necklace", "ring", "earrings", "bracelet", "jewelry",
                # å…¶ä»–
                "toy", "box", "packaging"
            ]

            # ä¸åœ¨åˆå§‹åŒ–æ—¶è®¾ç½®ç±»åˆ«ï¼Œé¿å…æŸäº›ç‰ˆæœ¬å‡ºç°å‰¯ä½œç”¨

            logger.info("ğŸ‰ YOLO-Worldæ¨¡å‹åŠ è½½æˆåŠŸï¼")
            logger.info(f"ğŸ¯ æ”¯æŒè‡ªåŠ¨è¯†åˆ« {len(self.target_classes)} ç§å•†å“ç±»åˆ«")
            logger.info(f"ğŸ“‹ YOLO-Worldç›®æ ‡ç±»åˆ«: {', '.join(self.target_classes[:10])}...")
            logger.info("âš¡ YOLO-Worldä¼˜åŒ–è¯´æ˜: ä½¿ç”¨å¤šç»´åº¦è¯„åˆ†(é¢ç§¯Ã—ç½®ä¿¡åº¦Ã—ä½ç½®Ã—ç±»åˆ«æƒé‡)ï¼Œæ˜¾è‘—æå‡è£å‰ªå‡†ç¡®ç‡")

            # éªŒè¯CLIPåº“æ˜¯å¦æ­£ç¡®å®‰è£…
            try:
                import clip
                logger.info(f"âœ… CLIPåº“ç‰ˆæœ¬éªŒè¯: {getattr(clip, '__version__', 'æœªçŸ¥')}")
                if hasattr(clip, 'load'):
                    logger.info("âœ… CLIP.loadæ–¹æ³•å¯ç”¨")
                else:
                    logger.warning("âš ï¸ CLIP.loadæ–¹æ³•ä¸å¯ç”¨ï¼Œå¯èƒ½å½±å“YOLO-Worldæ€§èƒ½")
            except ImportError as e:
                logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥CLIPåº“: {e}")

        except Exception as e:
            logger.error(f"ğŸ’¥ YOLO-Worldæ¨¡å‹åŠ è½½å¤±è´¥: {e}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯CLIPç›¸å…³çš„é—®é¢˜ï¼Œå¦‚æœæ˜¯åˆ™å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
            if "clip" in str(e).lower():
                logger.warning("ğŸ” æ£€æµ‹åˆ°CLIPåº“é—®é¢˜ï¼Œå°è¯•å¤‡ç”¨åŠ è½½æ–¹å¼...")

                try:
                    # å°è¯•ä¸ä¾èµ–CLIPçš„åŠ è½½æ–¹å¼
                    import ultralytics
                    logger.info(f"Ultralyticsç‰ˆæœ¬: {ultralytics.__version__}")

                    # ç›´æ¥åˆ›å»ºYOLO-Worldå®ä¾‹ï¼Œä¸è®¾ç½®ç±»åˆ«
                    self.detector = YOLO('yolov8s-world.pt')
                    self.target_classes = None  # ä¸è®¾ç½®è‡ªå®šä¹‰ç±»åˆ«

                    logger.warning("âš ï¸ YOLO-Worldä»¥åŸºç¡€æ¨¡å¼åŠ è½½ (æ— è‡ªå®šä¹‰ç±»åˆ«)")
                    logger.warning("ğŸ“Š å½±å“: å°†ä½¿ç”¨YOLO-Worldçš„å†…ç½®80ç±»è¿›è¡Œæ£€æµ‹")
                    logger.info("âœ… YOLO-WorldåŸºç¡€æ¨¡å¼åŠ è½½æˆåŠŸ")

                except Exception as backup_error:
                    logger.error(f"ğŸ’¥ å¤‡ç”¨åŠ è½½æ–¹å¼ä¹Ÿå¤±è´¥: {backup_error}")
                    logger.error("ğŸ”¥ ç”¨æˆ·è¦æ±‚å¼ºåˆ¶ä½¿ç”¨YOLO-Worldï¼Œä½†æ‰€æœ‰åŠ è½½æ–¹å¼éƒ½å¤±è´¥ï¼")
                    logger.error("ğŸ’¡ æœ€ç»ˆè§£å†³æ–¹æ¡ˆ:")
                    logger.error("   1. pip uninstall clip torch torchvision ultralytics")
                    logger.error("   2. pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu")
                    logger.error("   3. pip install ultralytics")
                    logger.error("   4. pip install git+https://github.com/openai/CLIP.git")
                    raise RuntimeError("YOLO-WorldåŠ è½½å¤±è´¥ï¼Œæ‰€æœ‰å¤‡ç”¨æ–¹æ¡ˆå‡æ— æ•ˆ") from e
            else:
                # ä¸æ˜¯CLIPé—®é¢˜ï¼Œç›´æ¥æŠ¥é”™
                logger.error("ğŸ”¥ YOLO-WorldåŠ è½½å¤±è´¥ï¼Œé”™è¯¯ä¸ç›¸å…³CLIPåº“")
                logger.error("ğŸ’¡ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œultralyticsç‰ˆæœ¬")
                raise RuntimeError("YOLO-WorldåŠ è½½å¤±è´¥") from e

    def _load_dino_model(self):
        """åŠ è½½DINOv2æ¨¡å‹ç”¨äºç‰¹å¾æå–"""
        try:
            model_name = config.DINO_MODEL_NAME
            logger.info(f"åŠ è½½DINOv2ç‰¹å¾æ¨¡å‹: {model_name}...")

            self.processor = AutoImageProcessor.from_pretrained(model_name)
            self.model = self._load_pretrained_model(model_name, force_no_safetensors=False)
            if self._model_has_meta(self.model):
                logger.warning("æ£€æµ‹åˆ° meta tensorï¼Œå°è¯•ç¦ç”¨ safetensors é‡æ–°åŠ è½½")
                self.model = self._load_pretrained_model(model_name, force_no_safetensors=True)
            if self._model_has_meta(self.model):
                raise RuntimeError("æ¨¡å‹ä»å¤„äº meta çŠ¶æ€ï¼Œè¯·æ£€æŸ¥ transformers/torch ç‰ˆæœ¬æˆ–ç¼“å­˜")
            try:
                self.model.to(self.device)
            except Exception as device_error:
                if "meta" in str(device_error).lower():
                    logger.warning("æ£€æµ‹åˆ° meta tensorï¼Œæ”¹ç”¨CPUå¹¶é‡æ–°åŠ è½½æ¨¡å‹")
                    self.device = torch.device('cpu')
                    self.model = self._load_pretrained_model(model_name, force_no_safetensors=True)
                    self.model.to(self.device)
                else:
                    logger.warning(f"æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡å¤±è´¥: {device_error}ï¼Œæ”¹ç”¨CPU")
                    self.device = torch.device('cpu')
                    self.model.to(self.device)
            self.model.eval()
            logger.info("âœ… DINOv2æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ DINOv2æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError("DINOv2æ¨¡å‹åŠ è½½å¤±è´¥") from e

    def _load_pretrained_model(self, model_name: str, force_no_safetensors: bool) -> AutoModel:
        load_kwargs = {
            'low_cpu_mem_usage': False,
            'torch_dtype': torch.float32,
            'device_map': None
        }
        if force_no_safetensors:
            load_kwargs['use_safetensors'] = False

        try:
            sig = inspect.signature(AutoModel.from_pretrained)
            allowed = set(sig.parameters.keys())
            load_kwargs = {k: v for k, v in load_kwargs.items() if k in allowed}
        except Exception:
            pass

        return AutoModel.from_pretrained(model_name, **load_kwargs)

    @staticmethod
    def _model_has_meta(model: AutoModel) -> bool:
        try:
            return any(getattr(p, 'is_meta', False) for p in model.parameters())
        except Exception:
            return False

    def _crop_main_object(self, image_path: str) -> Image.Image:
        """å…¨è‡ªåŠ¨è£å‰ªå•†å“ä¸»ä½“ + [æ–°å¢] å°ºå¯¸ä¼˜åŒ–

        å…¨è‡ªåŠ¨è£å‰ªé€»è¾‘ï¼š
        1. åœ¨é¢„è®¾çš„å•†å“ç±»åˆ«ä¸­æ£€æµ‹æ‰€æœ‰ç‰©ä½“
        2. è‡ªåŠ¨è¿‡æ»¤æ‰èƒŒæ™¯ã€äººã€æ‰‹
        3. åœ¨å‰©ä¸‹çš„å•†å“ä¸­ï¼Œé€‰å‡ºæœ€æ˜¾è‘—çš„ä¸€ä¸ªï¼ˆæœ€å¤§+æœ€ä¸­å¿ƒï¼‰
        4. [æ–°å¢] ç¼©å°å›¾ç‰‡å°ºå¯¸ä»¥åŠ å¿«AIæ¨ç†é€Ÿåº¦
        """
        try:
            img = Image.open(image_path).convert("RGB")
            img_w, img_h = img.size

            if not config.USE_YOLO_CROP or self.detector is None:
                return self._center_crop(img)

            # æ£€æŸ¥ç¼“å­˜
            image_hash = self._get_image_hash(image_path)
            if image_hash in self._detection_cache:
                logger.debug("ä½¿ç”¨ç¼“å­˜çš„æ£€æµ‹ç»“æœ")
                cached_result = self._detection_cache[image_hash]
                if cached_result is None:
                    return self._center_crop(img)
                # è¿”å›ç¼“å­˜çš„è£å‰ªç»“æœ
                return cached_result

            # conf=0.05: é™ä½é—¨æ§›ï¼Œå®å¯å¤šæ£€ä¸è¦æ¼æ£€ï¼Œåæ­£æˆ‘ä»¬æœ‰é€»è¾‘è¿‡æ»¤
            with self.inference_lock:
                results = self.detector(image_path, conf=0.05, verbose=False)

            if not results or len(results[0].boxes) == 0:
                logger.debug("æœªæ£€æµ‹åˆ°é€šç”¨å•†å“ï¼Œä½¿ç”¨ä¸­å¿ƒè£å‰ªå…œåº•")
                self._detection_cache[image_hash] = None
                return self._center_crop(img)

            boxes = results[0].boxes
            center_x, center_y = img_w / 2, img_h / 2

            # --- æ™ºèƒ½è¯„åˆ†é€»è¾‘ ---
            # åœ¨æ‰€æœ‰æ£€æµ‹åˆ°çš„"å•†å“"ä¸­ï¼Œé€‰å‡ºä¸»è§’

            best_box = None
            max_score = -1

            for box in boxes:
                # 1. è·å–åæ ‡
                coords = box.xyxy[0].cpu().numpy()  # [x1, y1, x2, y2]
                x1, y1, x2, y2 = coords

                # 2. è®¡ç®—é¢ç§¯
                width = x2 - x1
                height = y2 - y1
                area = width * height
                if area < (img_w * img_h * 0.02):
                    continue

                # 3. è®¡ç®—ç¦»å›¾ç‰‡ä¸­å¿ƒçš„è·ç¦»
                box_center_x = x1 + width / 2
                box_center_y = y1 + height / 2
                dist_to_center = ((box_center_x - center_x)**2 + (box_center_y - center_y)**2) ** 0.5

                # 4. ç»¼åˆè¯„åˆ†å…¬å¼ï¼š
                # é¢ç§¯è¶Šå¤§è¶Šå¥½ (æƒé‡ 0.6)
                # è¶Šé ä¸­å¿ƒè¶Šå¥½ (æƒé‡ 0.4)
                # è¿™ä¸ªå…¬å¼èƒ½ä¿è¯ï¼šå³ä½¿è§’è½é‡Œæœ‰ä¸ªå¤§åŒ…ï¼Œä¹Ÿä¼šä¼˜å…ˆé€‰ä¸­é—´çš„å°é‹å­
                norm_area = area / (img_w * img_h)
                norm_dist = 1 - (dist_to_center / ((img_w**2 + img_h**2)**0.5))

                score = (norm_area * 0.6) + (norm_dist * 0.4) + (float(box.conf) * 0.1)

                if score > max_score:
                    max_score = score
                    best_box = coords

            if best_box is None:
                logger.info("æœªæ‰¾åˆ°åˆé€‚çš„å•†å“æ¡†ï¼Œä½¿ç”¨ä¸­å¿ƒè£å‰ªå…œåº•")
                self._detection_cache[image_hash] = None
                return self._center_crop(img)

            # æ‰§è¡Œè£å‰ª
            x1, y1, x2, y2 = best_box

            # æ‰©å…… 5% - 10% çš„è¾¹ç¼˜ï¼Œä¿ç•™ä¸€ç‚¹ç‚¹ä¸Šä¸‹æ–‡
            pad_x = (x2 - x1) * 0.05
            pad_y = (y2 - y1) * 0.05

            crop_box = (
                max(0, x1 - pad_x),
                max(0, y1 - pad_y),
                min(img_w, x2 + pad_x),
                min(img_h, y2 + pad_y)
            )

            cropped_img = img.crop(crop_box)
            logger.debug(f"æˆåŠŸè£å‰ªå•†å“åŒºåŸŸ: {crop_box}")

            # ä¼˜åŒ–ï¼šResize è£å‰ªåçš„å›¾ç‰‡
            final_img = self._resize_for_ai(cropped_img)

            # ç¼“å­˜æˆåŠŸç»“æœ
            self._detection_cache[image_hash] = final_img.copy()

            return final_img

        except Exception as e:
            logger.warning(f"è‡ªåŠ¨è£å‰ªå‡ºé”™: {e}, ä½¿ç”¨ä¸­å¿ƒè£å‰ª")
            # ç¼“å­˜å¤±è´¥ç»“æœ
            try:
                image_hash = self._get_image_hash(image_path)
                self._detection_cache[image_hash] = None
            except:
                pass
            return self._center_crop(Image.open(image_path).convert("RGB"))

    def _center_crop(self, img: Image.Image) -> Image.Image:
        """ä¸­å¿ƒè£å‰ªï¼šä¿ç•™ä¸­é—´ 80% åŒºåŸŸï¼Œé™ä½èƒŒæ™¯å¹²æ‰°"""
        w, h = img.size
        left = int(w * 0.1)
        top = int(h * 0.1)
        right = int(w * 0.9)
        bottom = int(h * 0.9)
        return self._resize_for_ai(img.crop((left, top, right, bottom)))

    def _resize_for_ai(self, img: Image.Image, max_size: int = 448) -> Image.Image:
        """[æ–°å¢] å°†å›¾ç‰‡ç¼©å°åˆ°é€‚åˆ AI æ¨ç†çš„å°ºå¯¸ï¼Œå¤§å¹…æå‡é€Ÿåº¦

        Args:
            img: è¾“å…¥å›¾ç‰‡
            max_size: æœ€å¤§å°ºå¯¸ï¼ˆé»˜è®¤448pxï¼‰ï¼Œé€‚åˆDINOv2ç‰¹å¾æå–

        Returns:
            ç¼©æ”¾åçš„å›¾ç‰‡
        """
        w, h = img.size
        if max(w, h) > max_size:
            scale = max_size / max(w, h)
            new_w = int(w * scale)
            new_h = int(h * scale)
            return img.resize((new_w, new_h), Image.Resampling.LANCZOS)
        return img

    def extract_feature(self, image_path: Union[str, Path]) -> Optional[np.ndarray]:
        """æå–å•å¼ å›¾ç‰‡çš„ç‰¹å¾å‘é‡ (384ç»´æˆ–768ç»´)"""
        try:
            image_path = str(image_path)

            if not os.path.exists(image_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return None

            # 1. YOLOè£å‰ªä¸»ä½“
            img = self._crop_main_object(image_path)

            # 2. é¢„å¤„ç†ï¼ˆDINOv2ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            with self.inference_lock:
                inputs = self.processor(images=img, return_tensors="pt").to(self.device)
                # 3. ç‰¹å¾æå–
                with torch.no_grad():
                    outputs = self.model(**inputs)

            # 4. è·å–CLS tokenç‰¹å¾ (DINOv2çš„æœ€ä½³å®è·µ)
            # outputs.last_hidden_state.shape: [1, num_patches+1, dim]
            # ç¬¬0ä¸ªæ˜¯CLS tokenï¼Œä»£è¡¨æ•´å¼ å›¾çš„è¯­ä¹‰
            embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()

            # 5. L2å½’ä¸€åŒ– (å¯¹ä½™å¼¦ç›¸ä¼¼åº¦è‡³å…³é‡è¦)
            norm = float(np.linalg.norm(embedding))
            if norm > 0:
                embedding = embedding / norm

            # 6. ç¡®ä¿æ•°æ®ç±»å‹ä¸ºfloat32 (FAISSè¦æ±‚)
            return embedding.astype('float32')

        except Exception as e:
            logger.error(f"DINOv2ç‰¹å¾æå–å¤±è´¥ {image_path}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def extract_features_batch(self, image_paths: List[Union[str, Path]]) -> List[Optional[np.ndarray]]:
        """æ‰¹é‡æå–ç‰¹å¾å‘é‡"""
        results = []
        for image_path in image_paths:
            feature = self.extract_feature(image_path)
            results.append(feature)
        return results

    def prepare_hybrid_query(self, img_path: str) -> Optional[Dict]:
        """é¢„å…ˆè®¡ç®—æŸ¥è¯¢å›¾çš„é¢œè‰²/æ¯”ä¾‹ç‰¹å¾ï¼Œä¾¿äºé‡æ’åºé˜¶æ®µå¤ç”¨"""
        try:
            img = cv2.imread(img_path)
            if img is None:
                logger.warning(f"æ— æ³•è¯»å–æŸ¥è¯¢å›¾ç‰‡: {img_path}")
                return None
            return self._build_hybrid_signature(img)
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢å›¾ç‰¹å¾é¢„è®¡ç®—å¤±è´¥: {e}")
            return None

    def _build_hybrid_signature(self, img: np.ndarray) -> Dict:
        """æ„å»ºç”¨äºæ··åˆç›¸ä¼¼åº¦çš„é¢œè‰²/æ¯”ä¾‹ç­¾å"""
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        hist = cv2.calcHist([hsv], [0, 1], None, [18, 4], [0, 180, 0, 256])
        cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)

        h, w = img.shape[:2]
        aspect_ratio = float(w) / float(h) if h else 1.0

        return {
            'hist': hist,
            'aspect_ratio': aspect_ratio
        }

    def calculate_hybrid_similarity(self, img_path1: str, img_path2: str, dino_score: float,
                                    query_signature: Optional[Dict] = None) -> dict:
        """
        ã€æ–°å¢ã€‘è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦ (Re-ranking)

        ç»¼åˆåˆ† = DINOè¯­ä¹‰åˆ†(70%) + é¢œè‰²åˆ†(15%) + å®½é«˜æ¯”åˆ†(15%)

        Args:
            img_path1: æŸ¥è¯¢å›¾ç‰‡è·¯å¾„
            img_path2: å€™é€‰å›¾ç‰‡è·¯å¾„
            dino_score: DINOv2åŸå§‹ç›¸ä¼¼åº¦åˆ†æ•°

        Returns:
            dict: {'score': ç»¼åˆåˆ†æ•°, 'details': {'dino': ..., 'color': ..., 'ratio': ...}}
        """
        try:
            if query_signature is None:
                img1 = cv2.imread(img_path1)
                if img1 is None:
                    logger.warning(f"æ— æ³•è¯»å–å›¾ç‰‡ï¼Œä½¿ç”¨åŸå§‹DINOåˆ†æ•°: {img_path1}")
                    return {'score': dino_score, 'details': {}}
                query_signature = self._build_hybrid_signature(img1)

            img2 = cv2.imread(img_path2)
            if img2 is None:
                logger.warning(f"æ— æ³•è¯»å–å›¾ç‰‡ï¼Œä½¿ç”¨åŸå§‹DINOåˆ†æ•°: {img_path2}")
                return {'score': dino_score, 'details': {}}

            candidate_signature = self._build_hybrid_signature(img2)

            # é¢œè‰²ç›¸ä¼¼åº¦ (H+S, é™ä½å…‰ç…§å½±å“)
            color_score = cv2.compareHist(query_signature['hist'], candidate_signature['hist'], cv2.HISTCMP_CORREL)
            color_score = max(0.0, color_score)

            # å®½é«˜æ¯”ç›¸ä¼¼åº¦ (è¿‡æ»¤è·¨å“ç±»è¯¯æŠ¥)
            ratio1 = query_signature['aspect_ratio']
            ratio2 = candidate_signature['aspect_ratio']
            ratio_score = min(ratio1, ratio2) / max(ratio1, ratio2) if ratio1 > 0 and ratio2 > 0 else 0.0

            # å¦‚æœDINOå¾ˆé«˜ï¼Œä¼˜å…ˆå°Šé‡è¯­ä¹‰/ç»“æ„é²æ£’æ€§
            if dino_score > 0.85:
                final_score = dino_score
            else:
                final_score = (dino_score * 0.70) + (color_score * 0.15) + (ratio_score * 0.15)

            logger.debug(f"ç»¼åˆè¯„åˆ†: DINO={dino_score:.3f}, Color={color_score:.3f}, Ratio={ratio_score:.3f}, Final={final_score:.3f}")

            return {
                'score': float(final_score),
                'details': {
                    'dino': float(dino_score),
                    'color': float(color_score),
                    'ratio': float(ratio_score)
                }
            }

        except Exception as e:
            logger.error(f"è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return {'score': dino_score, 'details': {}}

    def get_status(self) -> Dict:
        """è·å–AIæ¨¡å‹çŠ¶æ€å’Œæ€§èƒ½ä¿¡æ¯"""
        status = {
            'device': str(self.device),
            'yolo_available': self.detector is not None,
            'yolo_type': 'None'
        }

        if self.detector is not None:
            if self.target_classes and len(self.target_classes) > 20:
                status['yolo_type'] = 'YOLO-World'
                status['target_classes_count'] = len(self.target_classes)
                status['target_classes'] = self.target_classes[:10]  # åªæ˜¾ç¤ºå‰10ä¸ª
            else:
                status['yolo_type'] = 'YOLOv8-Nano'
                status['target_classes_count'] = len(self.target_classes) if self.target_classes else 0

        status['detection_cache_size'] = len(self._detection_cache)
        status['confidence_threshold'] = 0.05
        status['iou_threshold'] = 0.5

        # æ€§èƒ½æç¤º
        tips = []
        if self.detector is None:
            tips.append("YOLOè£å‰ªå·²ç¦ç”¨ï¼Œå»ºè®®ä¿®å¤YOLOåŠ è½½é—®é¢˜ä»¥æå‡å‡†ç¡®æ€§")
        elif status['yolo_type'] == 'YOLOv8-Nano':
            tips.append("å½“å‰ä½¿ç”¨YOLOv8-Nanoï¼Œå»ºè®®å‡çº§ä¾èµ–ä»¥å¯ç”¨YOLO-Worldè·å¾—æ›´å¥½æ•ˆæœ")

        if status['detection_cache_size'] > 1000:
            tips.append("æ£€æµ‹ç¼“å­˜è¾ƒå¤§ï¼Œè€ƒè™‘å®šæœŸæ¸…ç†ç¼“å­˜")

        status['performance_tips'] = tips if tips else ["AIæ¨¡å‹è¿è¡Œæ­£å¸¸"]

        return status

# å‘åå…¼å®¹çš„åˆ«å
class FeatureExtractor(DINOv2FeatureExtractor):
    """å‘åå…¼å®¹çš„åˆ«å"""
    pass

def get_feature_extractor() -> 'DINOv2FeatureExtractor':
    """å…¨å±€è·å–ç‰¹å¾æå–å™¨å®ä¾‹ï¼ˆçº¿ç¨‹å®‰å…¨å•ä¾‹ï¼‰"""
    global _global_extractor

    if _global_extractor is not None:
        return _global_extractor

    with _extractor_lock:
        # åŒé‡æ£€æŸ¥é”å®š
        if _global_extractor is None:
            logger.info("ğŸš€ [ç³»ç»Ÿ] åˆå§‹åŒ– AI æ¨¡å‹ (DINOv2 + YOLO)...")
            try:
                _global_extractor = DINOv2FeatureExtractor()
                logger.info("âœ… [ç³»ç»Ÿ] AI æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ [ç³»ç»Ÿ] AI æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                raise e
        return _global_extractor
