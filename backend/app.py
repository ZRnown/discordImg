# ============================================================
# ã€å¯åŠ¨ç¨³å®šæ€§ä¿®å¤ã€‘å¿…é¡»åœ¨ import torch ä¹‹å‰è®¾ç½®çº¿ç¨‹/ä»£ç†ç¯å¢ƒå˜é‡
# é¿å… OpenMP ä¸å¤šçº¿ç¨‹å†²çªå¯¼è‡´ Socket å…³é—­
# ============================================================
import os
import multiprocessing  # Windowså¤šè¿›ç¨‹å…¼å®¹æ€§å¿…éœ€

if 'AI_INTRA_THREADS' not in os.environ:
    os.environ['AI_INTRA_THREADS'] = '1'
_ai_threads = os.environ.get('AI_INTRA_THREADS', '1')
os.environ["OMP_NUM_THREADS"] = _ai_threads
os.environ["MKL_NUM_THREADS"] = _ai_threads
os.environ["OPENBLAS_NUM_THREADS"] = _ai_threads
os.environ["VECLIB_MAXIMUM_THREADS"] = _ai_threads
os.environ["NUMEXPR_NUM_THREADS"] = _ai_threads
os.environ.setdefault("NO_PROXY", "localhost,127.0.0.1")
os.environ.setdefault("TORCH_CPP_LOG_LEVEL", "ERROR")
os.environ.setdefault("TORCH_SHOW_CPP_STACKTRACES", "0")
os.environ.setdefault("GLOG_minloglevel", "2")

from flask import Flask, request, jsonify, Response, session
import numpy as np
import logging
import sys
from datetime import datetime
from threading import Lock

# è‡ªåŠ¨åŠ è½½.envæ–‡ä»¶
try:
    from dotenv import load_dotenv
    # ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½.envæ–‡ä»¶
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    env_path = os.path.join(project_root, '.env')
    if os.path.exists(env_path):
        load_dotenv(env_path)
        print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
    else:
        print("â„¹ï¸  æœªæ‰¾åˆ°.envæ–‡ä»¶ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")
except ImportError:
    print("â„¹ï¸  python-dotenvæœªå®‰è£…ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

try:
    from feature_extractor import get_feature_extractor, DINOv2FeatureExtractor
except ImportError:
    from .feature_extractor import get_feature_extractor, DINOv2FeatureExtractor
try:
    from database import db
    from config import config
except ImportError:
    from .database import db
    from .config import config
import requests
import json
from flask_cors import CORS
import queue
import threading
import time
from urllib.parse import quote
import hashlib
import uuid

# === å…¨å±€çŠ¶æ€å˜é‡ ===
ai_model_ready = False  # AIæ¨¡å‹æ˜¯å¦å·²å°±ç»ª
# å…¨å±€ AI å¹¶å‘æ§åˆ¶ï¼ˆè·¨å•†å“ï¼‰ï¼Œé¿å… CPU è¢«åŒæ—¶æ¨ç†ä»»åŠ¡æ‰“æ»¡
GLOBAL_AI_SEMAPHORE = threading.Semaphore(4)

# åœ¨åº”ç”¨å¯åŠ¨æ—¶ä»æ•°æ®åº“åŠ è½½ç³»ç»Ÿé…ç½®
def load_system_config():
    """ä»æ•°æ®åº“åŠ è½½ç³»ç»Ÿé…ç½®åˆ°å†…å­˜"""
    # åœ¨å‡½æ•°å†…éƒ¨å®šä¹‰loggerï¼Œå› ä¸ºæ­¤æ—¶å…¨å±€loggerå¯èƒ½è¿˜æ²¡æœ‰åˆå§‹åŒ–
    import logging
    func_logger = logging.getLogger(__name__)

    try:
        sys_config = db.get_system_config()
        config.DISCORD_SIMILARITY_THRESHOLD = sys_config['discord_similarity_threshold']
        config.DISCORD_CHANNEL_ID = sys_config['discord_channel_id']
        config.CNFANS_CHANNEL_ID = sys_config['cnfans_channel_id']
        config.ACBUY_CHANNEL_ID = sys_config['acbuy_channel_id']

        # åŠ è½½å…¨å±€å›å¤å»¶è¿Ÿé…ç½®
        reply_config = db.get_global_reply_config()
        config.GLOBAL_REPLY_MIN_DELAY = reply_config['min_delay']
        config.GLOBAL_REPLY_MAX_DELAY = reply_config['max_delay']

        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆä¾›æœºå™¨äººä½¿ç”¨ï¼‰
        discord_channel_id = sys_config['discord_channel_id']
        if discord_channel_id:
            os.environ['DISCORD_CHANNEL_ID'] = discord_channel_id

        func_logger.info("ç³»ç»Ÿé…ç½®å·²ä»æ•°æ®åº“åŠ è½½")
        func_logger.info(f"ä¸‹è½½çº¿ç¨‹: {config.DOWNLOAD_THREADS}")
        func_logger.info(f"ç‰¹å¾æå–çº¿ç¨‹: {config.FEATURE_EXTRACT_THREADS}")
        func_logger.info(f"Discordç›¸ä¼¼åº¦é˜ˆå€¼: {config.DISCORD_SIMILARITY_THRESHOLD} ({config.DISCORD_SIMILARITY_THRESHOLD*100:.0f}%)")
        func_logger.info(f"å…¨å±€å›å¤å»¶è¿Ÿè®¾ç½®ä¸º: {config.GLOBAL_REPLY_MIN_DELAY}-{config.GLOBAL_REPLY_MAX_DELAY}ç§’")
        func_logger.info(f"Discordé¢‘é“ID: {discord_channel_id or 'æœªè®¾ç½®(ç›‘å¬æ‰€æœ‰é¢‘é“)'}")
    except Exception as e:
        func_logger.warning(f"åŠ è½½ç³»ç»Ÿé…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")

def check_duplicate_image(new_features, existing_features_list, threshold=0.99):
    """
    æ£€æŸ¥æ–°å›¾ç‰‡çš„ç‰¹å¾å‘é‡æ˜¯å¦ä¸ç°æœ‰åˆ—è¡¨ä¸­çš„å›¾ç‰‡é‡å¤

    :param new_features: æ–°å›¾ç‰‡çš„ç‰¹å¾å‘é‡ (numpy array)
    :param existing_features_list: ç°æœ‰å›¾ç‰‡çš„ç‰¹å¾å‘é‡åˆ—è¡¨ (å¯ä»¥æ˜¯jsonå­—ç¬¦ä¸²åˆ—è¡¨æˆ–numpyåˆ—è¡¨)
    :param threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤99%
    :return: (is_duplicate, similarity_score)
    """
    if not existing_features_list:
        return False, 0.0

    try:
        # ç¡®ä¿ new_features æ˜¯ 1D æ•°ç»„
        new_features = np.array(new_features, dtype='float32').flatten()

        # é¢„è®¡ç®—æ–°å‘é‡çš„èŒƒæ•°
        norm_new = float(np.linalg.norm(new_features))
        if norm_new == 0:
            return False, 0.0

        for feat_item in existing_features_list:
            try:
                # å¤„ç†è¾“å…¥å¯èƒ½æ˜¯ JSON å­—ç¬¦ä¸²æˆ–å·²ç»æ˜¯ numpy æ•°ç»„çš„æƒ…å†µ
                if isinstance(feat_item, str):
                    feat_vec = np.array(json.loads(feat_item), dtype='float32').flatten()
                else:
                    feat_vec = np.array(feat_item, dtype='float32').flatten()

                norm_existing = float(np.linalg.norm(feat_vec))
                if norm_existing == 0:
                    continue

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                dot_product = float(np.dot(new_features, feat_vec))
                similarity = dot_product / (norm_new * norm_existing)

                if similarity > threshold:
                    return True, float(similarity)

            except Exception:
                continue

    except Exception as e:
        logger.error(f"å‘é‡æ¯”å¯¹å‡ºé”™: {e}")

    return False, 0.0

def process_and_save_image_core(product_id, image_url_or_file, index, existing_features=None, save_faiss_immediately=True):
    """
    æ ¸å¿ƒå›¾ç‰‡å¤„ç†å•å…ƒï¼šä¿å­˜ -> ç‰¹å¾æå– -> æŸ¥é‡ -> æ•°æ®åº“ -> FAISS

    :param product_id: å•†å“ID
    :param image_url_or_file: æˆ–è€…æ˜¯ URL å­—ç¬¦ä¸²ï¼Œæˆ–è€…æ˜¯ Flask çš„ FileStorage å¯¹è±¡
    :param index: å›¾ç‰‡ç´¢å¼•
    :param existing_features: ç°æœ‰ç‰¹å¾å‘é‡åˆ—è¡¨ï¼Œç”¨äºæŸ¥é‡
    :param save_faiss_immediately: æ˜¯å¦ç«‹å³ä¿å­˜FAISSç´¢å¼•ï¼ˆå•å¼ ä¸Šä¼ æ—¶ä¸ºTrueï¼Œæ‰¹é‡å¤„ç†æ—¶ä¸ºFalseï¼‰
    :return: å¤„ç†ç»“æœå­—å…¸
    """
    import os
    import time

    # 1. ç¡®å®šä¿å­˜è·¯å¾„ï¼ˆä½¿ç”¨é…ç½®çš„ç›®å½•ï¼‰
    timestamp = int(time.time() * 1000000)
    filename = f"{product_id}_{index}_{timestamp}.jpg"
    save_path = os.path.join(config.IMAGE_SAVE_DIR, str(product_id), filename)
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    img_db_id = None  # åˆå§‹åŒ–æ•°æ®åº“ ID

    try:
        # 2. ä¿å­˜æ–‡ä»¶
        if hasattr(image_url_or_file, 'save'):
            # æ˜¯ä¸Šä¼ çš„æ–‡ä»¶å¯¹è±¡ (FileStorage)
            image_url_or_file.save(save_path)
        else:
            # æ˜¯ URL å­—ç¬¦ä¸²
            import requests
            resp = requests.get(image_url_or_file, timeout=config.REQUEST_TIMEOUT, proxies={'http': None, 'https': None})
            if resp.status_code != 200:
                return {'success': False, 'error': f'Download failed: {resp.status_code}'}
            with open(save_path, 'wb') as f:
                f.write(resp.content)

        # éªŒè¯æ–‡ä»¶å¤§å°
        if os.path.getsize(save_path) == 0:
            os.remove(save_path)
            return {'success': False, 'error': 'Empty file'}

        # 3. ç‰¹å¾æå– (DINOv2 + YOLO)
        extractor = get_global_feature_extractor()
        if extractor is None:
            os.remove(save_path)
            return {'success': False, 'error': 'Feature extractor not initialized'}

        features = extractor.extract_feature(save_path)
        if features is None:
            os.remove(save_path)
            return {'success': False, 'error': 'Feature extraction failed'}

        # 4. æŸ¥é‡é€»è¾‘ (99.5%ç›¸ä¼¼åº¦)
        if existing_features:
            is_dup, score = check_duplicate_image(features, existing_features, threshold=0.995)
            if is_dup:
                os.remove(save_path)
                logger.info(f"ğŸš« å›¾ç‰‡é«˜åº¦ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {score:.4f})ï¼Œå·²è·³è¿‡: {filename}")
                return {'success': True, 'skipped': True}  # æ ‡è®°ä¸ºæˆåŠŸä½†è·³è¿‡ï¼Œä»¥å…æŠ¥é”™

        # 5. å…¥åº“ (SQLite)
        img_db_id = db.insert_image_record(product_id, save_path, index, features)

        # 6. å…¥åº“ (FAISS)
        try:
            from vector_engine import get_vector_engine
            engine = get_vector_engine()

            # === FAISS çº¿ç¨‹å®‰å…¨é” ===
            with faiss_lock:  # åŠ é”ï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªçº¿ç¨‹å†™å…¥ FAISS
                engine.add_vector(img_db_id, features)
                # æ€§èƒ½ä¼˜åŒ–ï¼šå•å¼ ä¸Šä¼ æ—¶ç«‹å³ä¿å­˜ï¼Œæ‰¹é‡å¤„ç†æ—¶å»¶è¿Ÿä¿å­˜
                if save_faiss_immediately:
                    engine.save()
        except Exception as faiss_err:
            logger.error(f"FAISS å…¥åº“å¤±è´¥: {faiss_err}")
            # FAISSå¤±è´¥æ—¶åˆ é™¤æ•°æ®åº“è®°å½•å’Œæ–‡ä»¶ï¼Œå›æ»šæ“ä½œ
            try:
                db.delete_image_record(img_db_id)
            except:
                pass
            if os.path.exists(save_path):
                os.remove(save_path)
            return {'success': False, 'error': f'FAISS error: {faiss_err}'}

        # 7. æ›´æ–°å¯¹æ¯”åˆ—è¡¨ï¼Œç¡®ä¿ä¸‹ä¸€å¼ å›¾èƒ½è·Ÿè¿™å¼ æ¯”
        if existing_features is not None:
            existing_features.append(features)  # å…³é”®ï¼šå®æ—¶åŠ å…¥åˆ—è¡¨

        # 8. å®Œæˆ
        return {
            'success': True,
            'image_path': save_path,
            'features': features,
            'index': index,
            'filename': filename,
            'db_id': img_db_id
        }

    except Exception as e:
        logger.error(f'å›¾ç‰‡å¤„ç†æ€»å‡ºé”™ï¼Œå°è¯•æ¸…ç†: {e}')
        if os.path.exists(save_path):
            os.remove(save_path)
        # å¦‚æœå·²ç»æ’å…¥æ•°æ®åº“ä½†åç»­å¤±è´¥ï¼Œéœ€è¦å›æ»š
        if img_db_id:
            try:
                db.delete_image_record(img_db_id)
            except:
                pass
        return {'success': False, 'error': str(e)}

# çº¿ç¨‹é…ç½®ç°åœ¨ç»Ÿä¸€åœ¨ config.py ä¸­ç®¡ç†

# ã€ä¿®å¤ã€‘ç§»é™¤å…¨å±€ load_system_config() è°ƒç”¨ï¼Œé˜²æ­¢å­è¿›ç¨‹é‡å¤åˆå§‹åŒ–
# load_system_config() ç°åœ¨åœ¨ initialize_runtime() ä¸­è°ƒç”¨

# === é‡æ„ï¼šåº—é“ºæŠ“å–çŠ¶æ€æ§åˆ¶ ===
# ç§»é™¤å…¨å±€çŠ¶æ€å˜é‡ï¼Œæ”¹ä¸ºæ•°æ®åº“æŒä¹…åŒ–å­˜å‚¨
# scrape_statusç°åœ¨é€šè¿‡db.get_scrape_status()å’Œdb.update_scrape_status()ç®¡ç†

# çº¿ç¨‹ç®¡ç†ï¼šè·Ÿè¸ªå½“å‰è¿è¡Œçš„æŠ“å–çº¿ç¨‹
current_scrape_thread = None
scrape_thread_lock = threading.Lock()
scrape_stop_event = threading.Event()  # æŠ“å–åœæ­¢äº‹ä»¶ï¼Œç”¨äºçº¿ç¨‹é—´é€šä¿¡

# FAISS çº¿ç¨‹å®‰å…¨é”ï¼šé˜²æ­¢å¤šçº¿ç¨‹åŒæ—¶å†™å…¥å‘é‡ç´¢å¼•å¯¼è‡´å´©æºƒ
faiss_lock = Lock()

# å…¨å±€å…³é—­äº‹ä»¶ï¼Œç”¨äºä¼˜é›…å…³é—­
shutdown_event = None

# ã€ä¿®å¤ã€‘ç§»é™¤å…¨å±€æ—¥å¿—é…ç½®ï¼Œé˜²æ­¢å­è¿›ç¨‹é‡å¤åˆå§‹åŒ–
# æ—¥å¿—é…ç½®ç°åœ¨åœ¨ initialize_runtime() ä¸­æ‰§è¡Œ

# æ—¥å¿—é˜Ÿåˆ—å’Œå®¢æˆ·ç«¯åˆ—è¡¨ï¼ˆæ•°æ®ç»“æ„ï¼Œéœ€è¦åœ¨å…¨å±€ï¼‰
log_queue = queue.Queue()
log_clients = []
all_logs = []

class QueueHandler(logging.Handler):
    """è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œå°†æ—¥å¿—å‘é€åˆ°é˜Ÿåˆ—"""
    def emit(self, record):
        try:
            # è¿‡æ»¤æ‰HTTPè¯·æ±‚æ—¥å¿—å’Œä¸é‡è¦çš„ç³»ç»Ÿæ—¥å¿—
            if self._should_filter_log(record):
                return

            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'level': record.levelname,
                'message': self.format(record),
                'module': record.module,
                'func': record.funcName
            }

            # æ·»åŠ åˆ°æ—¥å¿—åˆ—è¡¨ï¼ˆé™åˆ¶å¤§å°ï¼‰
            all_logs.append(log_entry)
            if len(all_logs) > 500:  # æœ€å¤šä¿å­˜500æ¡æ—¥å¿—
                all_logs.pop(0)

            log_queue.put(log_entry)

            # é€šçŸ¥æ‰€æœ‰è¿æ¥çš„å®¢æˆ·ç«¯
            for client_queue in log_clients[:]:  # å¤åˆ¶åˆ—è¡¨ä»¥é¿å…ä¿®æ”¹æ—¶çš„é—®é¢˜
                try:
                    client_queue.put(log_entry)
                except:
                    # å¦‚æœå®¢æˆ·ç«¯é˜Ÿåˆ—å·²æ»¡æˆ–æ–­å¼€ï¼Œç§»é™¤å®ƒ
                    if client_queue in log_clients:
                        log_clients.remove(client_queue)
        except Exception as e:
            print(f"æ—¥å¿—é˜Ÿåˆ—é”™è¯¯: {e}")

    def _should_filter_log(self, record):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿‡æ»¤æ‰è¿™æ¡æ—¥å¿—"""
        # è¿‡æ»¤Werkzeugçš„HTTPè¯·æ±‚æ—¥å¿—
        if record.module == '_internal':
            return True

        # è¿‡æ»¤åŒ…å«HTTPè¯·æ±‚æ¨¡å¼çš„æ—¥å¿—
        message = self.format(record)
        if any(pattern in message for pattern in [
            '"GET ', '"POST ', '"PUT ', '"DELETE ',
            'HTTP/1.1"', 'HTTP/1.0"',
            'werkzeug',
            '127.0.0.1 - -',  # è¿‡æ»¤è®¿é—®æ—¥å¿—
        ]):
            return True

        # è¿‡æ»¤ä¸€äº›ä¸é‡è¦çš„ç³»ç»Ÿæ—¥å¿—
        if record.module in ['urllib3', 'requests', 'aiohttp']:
            return True

        # 2. å…³é”®ä¿®å¤ï¼šå…è®¸ weidian_scraper å’Œ app çš„ INFO æ—¥å¿—é€šè¿‡
        # åªè¦æ˜¯è¿™äº›æ¨¡å—ï¼Œå³ä½¿æ˜¯ INFO çº§åˆ«ä¹Ÿå…è®¸é€šè¿‡
        whitelist_modules = [
            '__main__', 'app', 'database', 'bot',
            'weidian_scraper', 'feature_extractor',
            'vector_engine', 'migrate_data'
        ]

        if record.module in whitelist_modules:
            return False

        # å¯¹äºå…¶ä»–æœªçŸ¥æ¨¡å—ï¼Œåªæ˜¾ç¤ºWARNINGçº§åˆ«ä»¥ä¸Š
        if record.levelno < logging.WARNING:
            return True

        return False

# ã€ä¿®å¤ã€‘ç§»é™¤å…¨å±€é˜Ÿåˆ—å¤„ç†å™¨å’Œæ—¥å¿—çº§åˆ«è®¾ç½®ï¼Œé˜²æ­¢å­è¿›ç¨‹é‡å¤åˆå§‹åŒ–
# è¿™äº›é…ç½®ç°åœ¨åœ¨ initialize_runtime() ä¸­æ‰§è¡Œ

logger = logging.getLogger(__name__)

# æœºå™¨äººç›¸å…³å˜é‡
# [ä¿®æ”¹] ä» bot æ¨¡å—å¯¼å…¥åˆ—è¡¨ï¼Œç¡®ä¿ app.py å’Œ bot.py æ“ä½œåŒä¸€ä¸ªåˆ—è¡¨å¯¹è±¡
from bot import bot_clients, bot_tasks, get_all_cooldowns
bot_running = False  # æ ‡è®°æœºå™¨äººæ˜¯å¦æ­£åœ¨è¿è¡Œ

# å…¨å±€ç‰¹å¾æå–å™¨å®ä¾‹ï¼ˆåœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºï¼‰
feature_extractor_instance = None
feature_extractor_lock = threading.Lock()
feature_extractor_failed_at = 0.0

def initialize_feature_extractor():
    """åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ç‰¹å¾æå–å™¨ï¼Œç¡®ä¿å•ä¾‹æ¨¡å¼"""
    global feature_extractor_instance, feature_extractor_failed_at
    if feature_extractor_instance is None:
        with feature_extractor_lock:
            if feature_extractor_instance is None:
                if feature_extractor_failed_at:
                    now = time.time()
                    if now - feature_extractor_failed_at < 60:
                        print("âš ï¸ ç‰¹å¾æå–å™¨åˆå§‹åŒ–å¤±è´¥åå†·å´ä¸­ï¼Œç¨åå†è¯•")
                        return None
                print("ğŸš€ åˆå§‹åŒ–å…¨å±€ç‰¹å¾æå–å™¨å®ä¾‹...")
                try:
                    from feature_extractor import DINOv2FeatureExtractor
                    feature_extractor_instance = DINOv2FeatureExtractor()
                    print("âœ… å…¨å±€ç‰¹å¾æå–å™¨å®ä¾‹åˆå§‹åŒ–å®Œæˆ")
                except Exception as e:
                    print(f"âŒ ç‰¹å¾æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                    feature_extractor_instance = None
                    feature_extractor_failed_at = time.time()
    return feature_extractor_instance

def get_global_feature_extractor():
    """è·å–å…¨å±€ç‰¹å¾æå–å™¨å®ä¾‹"""
    global feature_extractor_instance
    if feature_extractor_instance is None:
        return initialize_feature_extractor()
    return feature_extractor_instance

# åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–
# ã€ä¿®å¤ã€‘æ³¨é‡Šæ‰æ¨¡å—çº§åˆ«çš„åˆå§‹åŒ–ï¼Œé¿å…å¤šè¿›ç¨‹ç¯å¢ƒä¸‹é‡å¤åˆå§‹åŒ–
# å®é™…çš„åˆå§‹åŒ–åœ¨ if __name__ == '__main__' å—ä¸­çš„é¢„çƒ­é˜¶æ®µæ‰§è¡Œ
# initialize_feature_extractor()

# ã€æ–°å¢ã€‘å®šä¹‰é¡¹ç›®å†…çš„ä¸´æ—¶æ–‡ä»¶ç›®å½• (åœ¨ backend/data/tmp)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEMP_DIR = os.path.join(BASE_DIR, 'data', 'tmp')
# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(TEMP_DIR, exist_ok=True)

# Flaské…ç½®åˆå§‹åŒ–ï¼ˆç®€åŒ–ç‰ˆ - è§£å†³HTTP IPè®¿é—®é—®é¢˜ï¼‰
app = Flask(__name__)
app.secret_key = config.SECRET_KEY

# CORS é…ç½®ï¼ˆå…è®¸æ‰€æœ‰æ¥æºï¼‰
CORS(app, origins=config.CORS_ORIGINS, supports_credentials=True)

# å¼ºåˆ¶æ›´æ–°é…ç½®ï¼Œè¦†ç›–é»˜è®¤çš„å®‰å…¨è®¾ç½®
app.config.update(
    SECRET_KEY=config.SECRET_KEY,
    SESSION_COOKIE_HTTPONLY=True,
    SESSION_COOKIE_SAMESITE=config.SESSION_COOKIE_SAMESITE,
    SESSION_COOKIE_SECURE=config.SESSION_COOKIE_SECURE,  # ç¡®ä¿æ˜¯False
    SESSION_COOKIE_DOMAIN=None,
    PERMANENT_SESSION_LIFETIME=config.SESSION_LIFETIME,  # 30å¤©ä¸è¿‡æœŸ
)

def initialize_runtime():
    """
    åˆå§‹åŒ–è¿è¡Œæ—¶ç¯å¢ƒ (æ—¥å¿—ã€é…ç½®ç­‰)
    åªåœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œé˜²æ­¢å­è¿›ç¨‹é‡å¤åˆå§‹åŒ–
    """
    print(f"ğŸ”§ [ç³»ç»Ÿ] æ­£åœ¨åˆå§‹åŒ–è¿è¡Œæ—¶ç¯å¢ƒ (PID: {os.getpid()})...")

    # 1. åŠ è½½ç³»ç»Ÿé…ç½®
    load_system_config()

    # 2. é…ç½®æ—¥å¿—ç³»ç»Ÿ
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # æ¸…é™¤ç°æœ‰çš„æ‰€æœ‰å¤„ç†å™¨ï¼ˆé˜²æ­¢é‡å¤ï¼‰
    if root_logger.handlers:
        root_logger.handlers = []

    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)

    # åˆ›å»ºå¹¶æ·»åŠ é˜Ÿåˆ—å¤„ç†å™¨
    queue_handler = QueueHandler()
    queue_handler.setLevel(logging.INFO)
    root_logger.addHandler(queue_handler)

    # å±è”½å™ªéŸ³æ—¥å¿—
    for lib in ['werkzeug', 'requests', 'ultralytics', 'aiohttp']:
        logging.getLogger(lib).setLevel(logging.WARNING)
    logging.getLogger('urllib3').setLevel(logging.ERROR)
    logging.getLogger('urllib3.connectionpool').setLevel(logging.ERROR)

    # 3. é‡ç½®æ•°æ®åº“çŠ¶æ€
    print("ğŸ§¹ [ç³»ç»Ÿ] æ­£åœ¨é‡ç½®æŠ“å–ä»»åŠ¡çŠ¶æ€...")
    try:
        db.update_scrape_status(
            is_scraping=False,
            stop_signal=False,
            message='ç³»ç»Ÿé‡å¯ï¼Œä»»åŠ¡çŠ¶æ€å·²é‡ç½®'
        )
        # é‡ç½®æ‰€æœ‰Discordè´¦å·çŠ¶æ€ä¸ºç¦»çº¿
        with db.get_connection() as conn:
            conn.execute("UPDATE discord_accounts SET status = 'offline'")
            conn.commit()
        print("âœ… [ç³»ç»Ÿ] æ•°æ®åº“çŠ¶æ€å·²é‡ç½®")
    except Exception as e:
        print(f"âš ï¸ [ç³»ç»Ÿ] çŠ¶æ€é‡ç½®å¤±è´¥: {e}")

    # 4. ã€å¼‚æ­¥ã€‘é¢„çƒ­AIæ¨¡å‹ï¼ˆä¸é˜»å¡Flaskå¯åŠ¨ï¼‰
    import threading
    def async_warmup_ai():
        global ai_model_ready
        try:
            print("ğŸ¤– [åå°] æ­£åœ¨é¢„çƒ­AIæ¨¡å‹...")
            get_global_feature_extractor()
            ai_model_ready = True
            print("âœ… [åå°] AIæ¨¡å‹é¢„çƒ­å®Œæˆï¼Œç³»ç»Ÿå·²å°±ç»ª")
        except Exception as e:
            print(f"âš ï¸ [åå°] AIé¢„çƒ­å¤±è´¥: {e}")
            ai_model_ready = False

    ai_warmup_thread = threading.Thread(target=async_warmup_ai, daemon=True)
    ai_warmup_thread.start()
    print("ğŸš€ [ç³»ç»Ÿ] AIæ¨¡å‹æ­£åœ¨åå°é¢„çƒ­ï¼ŒFlaskæœåŠ¡å³å°†å¯åŠ¨...")

    # 5. å¯åŠ¨åå°æ¸…ç†çº¿ç¨‹
    cleanup_thread = threading.Thread(target=run_cleanup_task, daemon=True)
    cleanup_thread.start()
    logger.info("ğŸš€ åå°æ¸…ç†ä»»åŠ¡å·²å¯åŠ¨")

    print(f"âœ… [ç³»ç»Ÿ] è¿è¡Œæ—¶ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

def extract_features(image_path):
    """ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æå–å›¾åƒç‰¹å¾"""
    try:
        extractor = get_global_feature_extractor()
        if extractor is None:
            logger.error("ç‰¹å¾æå–å™¨æœªåˆå§‹åŒ–")
            return None
        features = extractor.extract_feature(image_path)
        # å¦‚æœç‰¹å¾æå–å¤±è´¥ï¼Œè¿”å› Noneï¼ˆä¸Šå±‚å°†å¤„ç†å¹¶è¿”å›é”™è¯¯ï¼‰
        if features is None:
            logger.warning(f"ç‰¹å¾æå–å¤±è´¥: {image_path}")
            return None

        return features

    except Exception as e:
        logger.error(f"ç‰¹å¾æå–å¼‚å¸¸: {e}")
        return None

@app.route('/search_similar', methods=['POST'])
def search_similar():
    """æœç´¢ç›¸ä¼¼å›¾åƒ - ä½¿ç”¨ FAISS HNSW"""
    try:
        image_url = request.form.get('image_url')
        threshold = float(request.form.get('threshold', 0.6))  # DINOv2éœ€è¦æ›´é«˜çš„é˜ˆå€¼
        limit = int(request.form.get('limit', 5))  # è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª
        debug_enabled = bool(getattr(config, 'DEBUG', False))

        # è·å–ç”¨æˆ·åº—é“ºæƒé™è¿‡æ»¤ï¼ˆç”¨äºDiscordæœºå™¨äººï¼‰
        user_shops = None
        user_shops_json = request.form.get('user_shops')
        if user_shops_json:
            try:
                user_shops = json.loads(user_shops_json)
            except:
                user_shops = None

        if debug_enabled:
            logger.debug(f"Received threshold: {threshold}")
            logger.debug(f"User shops filter: {user_shops}")
            logger.debug(f"Form data: {list(request.form.keys())}")
            logger.debug(f"Files: {list(request.files.keys()) if request.files else 'No files'}")
            logger.debug(f"Content-Type: {request.content_type}")
            logger.debug(f"Method: {request.method}")
            logger.debug(f"image_url parameter: '{image_url}'")

        # å¤„ç†å›¾ç‰‡æ¥æº
        import uuid
        import os
        image_file = None  # åˆå§‹åŒ–å˜é‡ï¼Œé¿å…ä½œç”¨åŸŸé—®é¢˜

        if image_url:
            if debug_enabled:
                logger.debug(f"Processing image URL: {image_url}")
            # éªŒè¯URLæ ¼å¼
            if not image_url.startswith(('http://', 'https://')):
                return jsonify({'error': 'Invalid URL format, must start with http:// or https://'}), 400

            # ä»URLä¸‹è½½å›¾ç‰‡
            import requests
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = requests.get(image_url, timeout=15, headers=headers, stream=True)
                if debug_enabled:
                    logger.debug(f"URL response status: {response.status_code}")
                    logger.debug(f"Content-Type: {response.headers.get('content-type', 'unknown')}")

                if response.status_code != 200:
                    return jsonify({'error': f'Failed to download image from URL, status: {response.status_code}'}), 400

                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('content-type', '').lower()
                if not any(img_type in content_type for img_type in ['image/', 'application/octet-stream']):
                    if debug_enabled:
                        logger.debug(f"Warning - Content-Type '{content_type}' may not be an image")

                temp_filename = f"{uuid.uuid4()}.jpg"
                # ã€ä¿®æ”¹ã€‘ä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„ TEMP_DIR
                image_path = os.path.join(TEMP_DIR, temp_filename)

                with open(image_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(image_path)
                if debug_enabled:
                    logger.debug(f"Image downloaded to: {image_path}, size: {file_size} bytes")

                if file_size == 0:
                    os.remove(image_path)
                    return jsonify({'error': 'Downloaded file is empty'}), 400

                if file_size > 10 * 1024 * 1024:  # 10MB limit
                    os.remove(image_path)
                    return jsonify({'error': 'Image file too large (max 10MB)'}), 400

            except requests.exceptions.RequestException as e:
                if debug_enabled:
                    logger.debug(f"Network error downloading image: {str(e)}")
                return jsonify({'error': f'Network error downloading image: {str(e)}'}), 400
            except Exception as e:
                if debug_enabled:
                    logger.debug(f"Failed to download image: {str(e)}")
                return jsonify({'error': f'Failed to download image: {str(e)}'}), 400
        elif 'image' in request.files:
            if debug_enabled:
                logger.debug("No image_url provided, checking for uploaded file")
            image_file = request.files['image']
            if debug_enabled:
                logger.debug(f"Found uploaded file: {image_file.filename if image_file else 'None'}")

            temp_filename = f"{uuid.uuid4()}.jpg"
            # ã€ä¿®æ”¹ã€‘ä½¿ç”¨é¡¹ç›®ç›®å½•ä¸‹çš„ TEMP_DIR
            image_path = os.path.join(TEMP_DIR, temp_filename)
            image_file.save(image_path)

        else:
            if debug_enabled:
                logger.debug("No image_url and no uploaded file")
            return jsonify({'error': 'No image provided (url or file)'}), 400

        try:
            # æå–ç‰¹å¾ (ä½¿ç”¨ DINOv2 + YOLOv8)
            query_features = extract_features(image_path)

            if query_features is None:
                return jsonify({'error': 'Feature extraction failed'}), 500

            # è®°å½•ç”¨æˆ·æœç´¢æ¬¡æ•°ï¼ˆæœªç™»å½•åˆ™è·³è¿‡ï¼Œä¸å½±å“æœºå™¨äººè°ƒç”¨ï¼‰
            try:
                current_user = get_current_user()
                if current_user:
                    db.increment_user_image_search_count(current_user['id'])
            except Exception as e:
                logger.error(f"è®°å½•ç”¨æˆ·æœç´¢æ¬¡æ•°å¤±è´¥: {e}")

            # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨ FAISS HNSW å‘é‡æœç´¢ + ç»¼åˆè¯„åˆ†é‡æ’åº
            if debug_enabled:
                logger.debug(f"Searching with threshold: {threshold}, vector length: {len(query_features)}")

            # 1. æ‰©å¤§å¬å›èŒƒå›´ï¼šFAISS å…ˆæ‰¾å‰ 50 ä¸ªå€™é€‰ (Primary Search)
            # ä½¿ç”¨è¾ƒä½çš„é˜ˆå€¼å¬å›ï¼Œé˜²æ­¢æ¼æ‰å¯èƒ½çš„åŒ¹é…
            candidates_limit = 50
            raw_results = db.search_similar_images(query_features, limit=candidates_limit, threshold=0.05)
            if debug_enabled:
                logger.debug(f"FAISS recalled {len(raw_results) if raw_results else 0} candidates")

            # 2. é‡æ’åº (Re-ranking) - ç»¼åˆè¯„åˆ†
            refined_results = []

            if raw_results:
                # è·å–å…¨å±€ç‰¹å¾æå–å™¨å®ä¾‹ç”¨æ¥è®¡ç®—é¢œè‰²/ç»“æ„
                extractor = get_global_feature_extractor()
                query_signature = extractor.prepare_hybrid_query(image_path) if extractor else None

                for res in raw_results:
                    # è·å–å€™é€‰å›¾ç‰‡çš„æœ¬åœ°è·¯å¾„
                    candidate_img_path = res.get('image_path')

                    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåªèƒ½ç”¨åŸå§‹ DINO åˆ†æ•°
                    if not candidate_img_path or not os.path.exists(candidate_img_path):
                        final_score = res['similarity']
                        breakdown = {}
                        if debug_enabled:
                            logger.debug(f"Candidate image not found, using DINO score: {final_score:.3f}")
                    else:
                        # è®¡ç®—ç»¼åˆè¯„åˆ†
                        hybrid_data = extractor.calculate_hybrid_similarity(
                            image_path,  # ä¸Šä¼ çš„æŸ¥è¯¢å›¾ (ä¸´æ—¶æ–‡ä»¶)
                            candidate_img_path,  # æ•°æ®åº“é‡Œçš„å›¾
                            res['similarity'],  # åŸå§‹ DINO åˆ†æ•°
                            query_signature
                        )
                        final_score = hybrid_data['score']
                        breakdown = hybrid_data.get('details', {})

                    # æ›´æ–°åˆ†æ•°
                    res['original_similarity'] = res['similarity']  # ä¿ç•™åŸåˆ†ç”¨äºè°ƒè¯•
                    res['similarity'] = final_score  # æ›´æ–°ä¸ºç»¼åˆåˆ†
                    res['score_breakdown'] = breakdown

                    refined_results.append(res)

                # 3. æŒ‰æ–°çš„ç»¼åˆåˆ†æ•°é‡æ–°æ’åº
                refined_results.sort(key=lambda x: x['similarity'], reverse=True)
                if debug_enabled:
                    logger.debug(f"Re-ranking completed, best score: {refined_results[0]['similarity']:.3f}")

            # 4. åº”ç”¨ç”¨æˆ·é˜ˆå€¼å’Œåº—é“ºè¿‡æ»¤
            results = []
            for result in refined_results:
                similarity = result.get('similarity', 0)
                # åº”ç”¨ç”¨æˆ·ç›¸ä¼¼åº¦é˜ˆå€¼
                if similarity >= threshold:
                    # æ£€æŸ¥åº—é“ºæƒé™
                    if user_shops and result.get('shop_name') not in user_shops:
                        if debug_enabled:
                            logger.debug(f"Skipping result from shop {result.get('shop_name')} - not in user shops {user_shops}")
                        continue
                    results.append(result)
                    if len(results) >= limit:
                        break

            if debug_enabled:
                logger.debug(f"Filtered results count (threshold {threshold}): {len(results)}")
                if results:
                    logger.debug(
                        f"Best match similarity: {results[0]['similarity']:.3f} (original DINO: {results[0].get('original_similarity', 0):.3f})"
                    )
                logger.debug(f"Total indexed images: {db.get_total_indexed_images()}")

            # ä¸¥æ ¼æ‰§è¡Œé˜ˆå€¼ï¼šå¦‚æœæ²¡æœ‰æ»¡è¶³é˜ˆå€¼çš„ç»“æœï¼Œåˆ™è¿”å›ç©ºç»“æœ
            # ä¸å†ä½¿ç”¨ä»»ä½•ç¡¬ç¼–ç é˜ˆå€¼å…œåº•ï¼ˆä¾‹å¦‚ >0.8ï¼‰

            response_data = {
                'success': True,
                'results': [],
                'totalResults': 0,
                'message': f'æœªæ‰¾åˆ°ç›¸ä¼¼åº¦è¶…è¿‡{threshold*100:.0f}%çš„å•†å“',
                'searchTime': datetime.now().isoformat(),
                'debugInfo': {
                    'totalIndexedImages': db.get_total_indexed_images(),
                    'threshold': threshold,
                    'searchedVectors': len(results) if results else 0
                }
            }

            if results:
                # å¤„ç†å¤šä¸ªæœç´¢ç»“æœ
                processed_results = []

                # é¢„å…ˆå¯¼å…¥ jsonï¼Œé˜²æ­¢å¾ªç¯ä¸­æŠ¥é”™
                import json

                for i, result in enumerate(results):
                    # è·å–å®Œæ•´äº§å“ä¿¡æ¯
                    product_info = db._get_product_info_by_id(result['id'])

                    # è·å–å®é™…çš„å›¾ç‰‡URLåˆ—è¡¨
                    actual_images = []
                    if product_info:
                        with db.get_connection() as conn:
                            cursor = conn.cursor()
                            cursor.execute("SELECT image_index FROM product_images WHERE product_id = ? ORDER BY image_index", (result['id'],))
                            actual_images = [f"/api/image/{result['id']}/{row[0]}" for row in cursor.fetchall()]

                    # ç”Ÿæˆæ‰€æœ‰ç½‘ç«™çš„é“¾æ¥
                    weidian_id = None
                    if product_info and product_info.get('product_url'):
                        import re
                        match = re.search(r'itemID=(\d+)', product_info['product_url'])
                        if match:
                            weidian_id = match.group(1)

                    website_urls = []
                    if weidian_id:
                        website_urls = db.generate_website_urls(weidian_id)

                    selected_indexes = []
                    custom_urls = []
                    uploaded_reply_images = []
                    try:
                        if product_info and product_info.get('custom_reply_images'):
                            selected_indexes = json.loads(product_info.get('custom_reply_images') or '[]')
                        if product_info and product_info.get('custom_image_urls'):
                            custom_urls = json.loads(product_info.get('custom_image_urls') or '[]')
                        if product_info and product_info.get('uploaded_reply_images'):
                            uploaded_reply_images = json.loads(product_info.get('uploaded_reply_images') or '[]')
                    except Exception:
                        selected_indexes = []
                        custom_urls = []
                        uploaded_reply_images = []

                    result_data = {
                        'rank': i + 1,
                        'similarity': float(result['similarity']),
                        'originalSimilarity': float(result.get('original_similarity', result['similarity'])),  # åŸå§‹DINOåˆ†æ•°
                        'scoreBreakdown': result.get('score_breakdown', {}),  # è¯„åˆ†è¯¦æƒ…
                        'imageIndex': result['image_index'],
                        'matchedImage': f"/api/image/{result['id']}/{result['image_index']}",
                        'product': {
                            'id': result['id'],
                            'title': product_info['title'] if product_info else result.get('title', ''),
                            'englishTitle': product_info.get('english_title', ''),
                            'weidianUrl': product_info['product_url'] if product_info else result.get('product_url', ''),
                            'cnfansUrl': product_info.get('cnfans_url', ''),
                            'acbuyUrl': product_info.get('acbuy_url', ''),
                            'ruleEnabled': product_info.get('ruleEnabled', True) if product_info else True,
                            # ä¿®å¤ï¼šæœºå™¨äººéœ€è¦ imageSource å’Œ uploaded_reply_images æ‰èƒ½å‘é€æœ¬åœ°å›¾ç‰‡
                            'imageSource': product_info.get('image_source', 'product') if product_info else 'product',
                            'custom_reply_text': product_info.get('custom_reply_text', '') if product_info else '',
                            'replyScope': product_info.get('reply_scope', 'all') if product_info else 'all',
                            'uploaded_reply_images': uploaded_reply_images,
                            'selectedImageIndexes': selected_indexes,
                            'customImageUrls': custom_urls,
                            'images': actual_images if actual_images else [f"/api/image/{result['id']}/{result['image_index']}"],  # ä½¿ç”¨å®é™…å›¾ç‰‡åˆ—è¡¨
                            'websiteUrls': website_urls  # æ·»åŠ æ‰€æœ‰ç½‘ç«™çš„é“¾æ¥
                        }
                    }
                    processed_results.append(result_data)

                # ä¿å­˜æœ€ä½³åŒ¹é…çš„æœç´¢å†å²
                if processed_results:
                    best_match = processed_results[0]
                    db.add_search_history(
                        query_image_path=image_path,
                        matched_product_id=best_match['product']['id'],
                        matched_image_index=best_match['imageIndex'],
                        similarity=best_match['similarity'],
                        threshold=threshold
                    )

                response_data = {
                    'success': True,
                    'results': processed_results,
                    'totalResults': len(processed_results),
                    'searchTime': datetime.now().isoformat(),
                    'debugInfo': {
                        'totalIndexedImages': db.get_total_indexed_images(),
                        'threshold': threshold,
                        'limit': limit,
                        'searchedVectors': len(results) if results else 0
                    }
                }

            return jsonify(response_data)

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(image_path):
                os.unlink(image_path)

    except Exception as e:
        logger.error(f"æœç´¢å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/scrape', methods=['POST'])
def scrape_product():
    """æŠ“å–å•†å“å¹¶å»ºç«‹ç´¢å¼•"""
    try:
        logger.info("æ”¶åˆ°å•†å“æŠ“å–è¯·æ±‚")
        data = request.get_json(silent=True) or {}
        if data is None:
            logger.error("è¯·æ±‚ä½“ä¸ºç©º")
            return jsonify({'error': 'Invalid request body'}), 400

        logger.info(f"è¯·æ±‚æ•°æ®: {data}")

        # æ”¯æŒä¸¤ç§è¾“å…¥æ–¹å¼ï¼šå®Œæ•´URLæˆ–å•†å“ID
        url = data.get('url')
        weidian_id = data.get('weidianId')

        if not url and not weidian_id:
            logger.error("ç¼ºå°‘URLæˆ–weidianId")
            return jsonify({'error': 'URL or weidianId is required'}), 400

        # å¦‚æœæä¾›äº†weidianIdï¼Œæ„é€ URL
        if weidian_id and not url:
            url = f"https://weidian.com/item.html?itemID={weidian_id}"
            logger.info(f"æ„é€ URL: {url}")

        # éªŒè¯URLæ ¼å¼
        if 'weidian.com' not in url:
            logger.error(f"ä¸æ”¯æŒçš„URLæ ¼å¼: {url}")
            return jsonify({'error': 'åªæ”¯æŒå¾®åº—å•†å“é“¾æ¥'}), 400

        logger.info(f"å¼€å§‹æŠ“å–å•†å“: {url}")

        item_id = None
        if weidian_id:
            item_id = str(weidian_id)
        else:
            try:
                import re
                item_id_match = re.search(r'itemID=(\d+)', url)
                if item_id_match:
                    item_id = item_id_match.group(1)
            except Exception:
                item_id = None

        # å…ˆæŒ‰item_idå»é‡ï¼Œé¿å…æ— æ•ˆæŠ“å–
        if item_id and db.get_product_by_item_id(item_id):
            return jsonify({'error': 'å•†å“å·²å­˜åœ¨', 'existing': True}), 409

        # æ£€æŸ¥å•†å“æ˜¯å¦å·²å­˜åœ¨
        existing = db.get_product_by_url(url)
        if existing:
            return jsonify({'error': 'å•†å“å·²å­˜åœ¨', 'existing': True}), 409

        # ä½¿ç”¨çœŸæ­£çš„çˆ¬è™«
        from weidian_scraper import get_weidian_scraper
        scraper = get_weidian_scraper()

        # æŠ“å–å•†å“ä¿¡æ¯
        product_info = scraper.scrape_product_info(url)

        if not product_info:
            return jsonify({'error': 'å•†å“ä¿¡æ¯æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®'}), 500

        # ç”Ÿæˆacbuyé“¾æ¥
        acbuy_url = ''
        if product_info['weidian_url']:
            # ä»weidian_urlä¸­æå–itemID
            import re
            item_id_match = re.search(r'itemID=(\d+)', product_info['weidian_url'])
            if item_id_match:
                item_id = item_id_match.group(1)
                # æ„å»ºacbuyé“¾æ¥
                encoded_url = product_info['weidian_url'].replace(':', '%3A').replace('/', '%2F').replace('?', '%3F').replace('=', '%3D').replace('&', '%26')
                acbuy_url = f'https://www.acbuy.com/product?url={encoded_url}&id={item_id}&source=WD'

        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨å…¨å±€å»¶è¿Ÿé…ç½®ï¼‰
        product_id = db.insert_product({
            'product_url': product_info['weidian_url'],
            'title': product_info['title'],
            'description': product_info['description'],
            'english_title': product_info.get('english_title') or '',
            'cnfans_url': product_info.get('cnfans_url') or '',
            'acbuy_url': acbuy_url,
            'shop_name': product_info.get('shop_name', ''),  # ä»product_infoè·å–åº—é“ºåç§°
            'ruleEnabled': True  # é»˜è®¤å¯ç”¨è‡ªåŠ¨å›å¤è§„åˆ™
        })

        # ä¸‹è½½å›¾ç‰‡å¹¶å»ºç«‹å‘é‡ç´¢å¼•
        if product_info['images']:
            logger.info(f"ä¸‹è½½ {len(product_info['images'])} å¼ å›¾ç‰‡å¹¶å»ºç«‹ç´¢å¼•")

            # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
            import os
            images_dir = os.path.join(os.path.dirname(__file__), 'data', 'scraped_images', product_info['id'])
            os.makedirs(images_dir, exist_ok=True)

            # ä¸‹è½½å›¾ç‰‡
            saved_image_paths = scraper.download_images(
                product_info['images'],
                images_dir,
                product_info['id']
            )
            # ä¸ºæ¯å¼ å›¾ç‰‡å»ºç«‹å‘é‡ç´¢å¼•
            # æ³¨æ„ï¼šYOLOè£å‰ªå·²é›†æˆåœ¨DINOv2ç‰¹å¾æå–è¿‡ç¨‹ä¸­ï¼Œæ— éœ€é¢å¤–æ­¥éª¤
            # ä½¿ç”¨å…¨å±€ç‰¹å¾æå–å™¨
            extractor = get_global_feature_extractor()
            if extractor is None:
                logger.error("ç‰¹å¾æå–å™¨æœªåˆå§‹åŒ–")
                return

            # ä¸²è¡Œå»ºç«‹å‘é‡ç´¢å¼• (SQLiteä¸æ”¯æŒå¤šçº¿ç¨‹å†™å…¥)
            # ä½†å…ˆä½¿ç”¨å¤šçº¿ç¨‹è¿›è¡Œç‰¹å¾æå–ï¼Œç„¶åä¸²è¡Œæ’å…¥æ•°æ®åº“
            import concurrent.futures
            try:
                from vector_engine import get_vector_engine
            except ImportError:
                from .vector_engine import get_vector_engine
            engine = get_vector_engine()

            def extract_features_only(img_path):
                """åªæå–ç‰¹å¾ï¼Œä¸æ’å…¥æ•°æ®åº“"""
                try:
                    features = extractor.extract_feature(img_path)
                    return features
                except Exception as e:
                    logger.error(f"ç‰¹å¾æå–å¤±è´¥ {img_path}: {e}")
                    return None

            # ç¬¬ä¸€æ­¥ï¼šå¤šçº¿ç¨‹ç‰¹å¾æå–
            logger.info("å¼€å§‹å¤šçº¿ç¨‹ç‰¹å¾æå–...")
            features_list = []
            max_workers = min(config.FEATURE_EXTRACT_THREADS, len(saved_image_paths))

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ç‰¹å¾æå–ä»»åŠ¡
                future_to_image = {
                    executor.submit(extract_features_only, img_path): (i, img_path)
                    for i, img_path in enumerate(saved_image_paths)
                }

                # æ”¶é›†ç‰¹å¾æå–ç»“æœ
                for future in concurrent.futures.as_completed(future_to_image):
                    i, img_path = future_to_image[future]
                    try:
                        features = future.result()
                        features_list.append((i, img_path, features))
                    except Exception as e:
                        logger.error(f"ç‰¹å¾æå–å¼‚å¸¸ {img_path}: {e}")
                        features_list.append((i, img_path, None))

            # æŒ‰ç´¢å¼•æ’åºç»“æœ
            features_list.sort(key=lambda x: x[0])

            # ç¬¬äºŒæ­¥ï¼šä¸²è¡Œæ’å…¥æ•°æ®åº“å’ŒFAISSç´¢å¼•
            logger.info("å¼€å§‹ä¸²è¡Œæ•°æ®åº“æ’å…¥å’Œç´¢å¼•å»ºç«‹...")
            indexed_images = []

            for i, img_path, features in features_list:
                try:
                    if features is None:
                        logger.error(f"è·³è¿‡å›¾ç‰‡ {i}: ç‰¹å¾æå–å¤±è´¥")
                        continue

                    # æ’å…¥æ•°æ®åº“è®°å½•
                    image_db_id = db.insert_image_record(product_id, img_path, i)
                    if not image_db_id:
                        logger.error(f"å›¾ç‰‡ {i} å…ƒæ•°æ®æ’å…¥å¤±è´¥")
                        continue

                    # æ’å…¥FAISSå‘é‡ç´¢å¼•
                    with faiss_lock:  # FAISS çº¿ç¨‹å®‰å…¨é”
                        success = engine.add_vector(image_db_id, features)
                    if success:
                        indexed_images.append(f"{i}.jpg")
                        logger.info(f"å›¾ç‰‡ {i} ç´¢å¼•å»ºç«‹æˆåŠŸ")
                    else:
                        logger.error(f"å›¾ç‰‡ {i} ç´¢å¼•å»ºç«‹å¤±è´¥")

                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡ {i} æ—¶å‡ºé”™: {e}")
                    continue

            # æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡å¤„ç†å¤±è´¥
            if len(indexed_images) != len(saved_image_paths):
                failed_count = len(saved_image_paths) - len(indexed_images)
                logger.warning(f"æœ‰ {failed_count} å¼ å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")

            # å¦‚æœä¸€å¼ å›¾ç‰‡éƒ½æ²¡æˆåŠŸï¼Œè®¤ä¸ºæ˜¯é”™è¯¯
            if not indexed_images:
                logger.error("æ‰€æœ‰å›¾ç‰‡å¤„ç†éƒ½å¤±è´¥äº†")
                try:
                    db.delete_product_images(product_id)
                except Exception as del_e:
                    logger.error(f"å›æ»šåˆ é™¤å¤±è´¥: {del_e}")
                return jsonify({'error': 'All image processing failed'}), 500

            # å®æ—¶ä¿å­˜FAISSç´¢å¼•
            engine.save()

            logger.info(f"å…±å»ºç«‹ {len(indexed_images)} å¼ å›¾ç‰‡çš„ç´¢å¼•")
        else:
            logger.warning("æœªæ‰¾åˆ°å•†å“å›¾ç‰‡")

        # è¿”å›å®Œæ•´çš„å•†å“ä¿¡æ¯
        result = {
            'id': product_id,
            'weidianId': product_info['id'],  # æ·»åŠ å¾®åº—å•†å“ID
            'product_url': product_info['weidian_url'],
            'title': product_info['title'],
            'englishTitle': product_info['english_title'],
            'weidianUrl': product_info['weidian_url'],
            'cnfansUrl': product_info['cnfans_url'],
            'description': product_info['description'],
            'ruleEnabled': True,  # é»˜è®¤å¯ç”¨è§„åˆ™
            'createdAt': datetime.now().isoformat(),
            'images': product_info['images']  # è¿”å›å›¾ç‰‡URLåˆ—è¡¨
        }

        logger.info(f"å•†å“æŠ“å–å®Œæˆ: {product_info['title']}")
        return jsonify(result)

    except Exception as e:
        logger.error(f"æŠ“å–å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# Discord è´¦å·ç®¡ç† API
# ===== ç”¨æˆ·è®¤è¯å’Œæƒé™ç®¡ç†API =====

def get_current_user():
    """è·å–å½“å‰ç™»å½•ç”¨æˆ·"""
    user_id = session.get('user_id')
    if user_id:
        return db.get_user_by_id(user_id)
    return None

def require_admin():
    """æ£€æŸ¥æ˜¯å¦ä¸ºç®¡ç†å‘˜"""
    user = get_current_user()
    return user and user.get('role') == 'admin'

def can_manage_shops():
    """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰ç®¡ç†åº—é“ºçš„æƒé™ï¼ˆç®¡ç†å‘˜æˆ–æœ‰åˆ†é…çš„åº—é“ºï¼‰"""
    user = get_current_user()
    if not user:
        return False
    # ç®¡ç†å‘˜å¯ä»¥ç®¡ç†æ‰€æœ‰åº—é“º
    if user.get('role') == 'admin':
        return True
    # æ™®é€šç”¨æˆ·å¦‚æœæœ‰åˆ†é…çš„åº—é“ºï¼Œä¹Ÿå¯ä»¥ç®¡ç†
    user_shops = user.get('shops', [])
    return len(user_shops) > 0

def require_login():
    """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
    # å¼€å‘æ¨¡å¼ä¸‹è·³è¿‡è®¤è¯
    if config.DEBUG:
        # å¼€å‘æ¨¡å¼ä¸‹è‡ªåŠ¨è®¾ç½®ä¸ºadminç”¨æˆ·
        if 'user_id' not in session:
            session['user_id'] = 1  # é»˜è®¤adminç”¨æˆ·ID
        return True
    return get_current_user() is not None

@app.route('/api/bot/cooldowns', methods=['GET'])
def get_bot_cooldowns():
    """è·å–å½“å‰æ‰€æœ‰è´¦å·çš„å†·å´çŠ¶æ€"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        cooldowns = get_all_cooldowns()
        return jsonify({'cooldowns': cooldowns})
    except Exception as e:
        logger.error(f"è·å–å†·å´çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'cooldowns': []}), 500

@app.route('/api/auth/login', methods=['POST'])
def login():
    """ç”¨æˆ·ç™»å½•"""
    try:
        data = request.get_json()
        if not data or not data.get('username') or not data.get('password'):
            return jsonify({'error': 'ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º'}), 400

        username = data['username']
        password = data['password']

        user = db.authenticate_user(username, password)
        if user:
            session['user_id'] = user['id']
            # ä¸è¿”å›å¯†ç å“ˆå¸Œ
            user_info = {k: v for k, v in user.items() if k != 'password_hash'}
            return jsonify({'user': user_info, 'message': 'ç™»å½•æˆåŠŸ'})
        else:
            return jsonify({'error': 'ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯'}), 401
    except Exception as e:
        logger.error(f"ç™»å½•å¤±è´¥: {e}")
        return jsonify({'error': 'ç™»å½•å¤±è´¥'}), 500

@app.route('/api/auth/logout', methods=['POST'])
def logout():
    """ç”¨æˆ·ç™»å‡º"""
    session.pop('user_id', None)
    return jsonify({'message': 'å·²ç™»å‡º'})

@app.route('/api/auth/me', methods=['GET'])
def get_current_user_info():
    """è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯"""
    user = get_current_user()
    if user:
        # ä¸è¿”å›å¯†ç å“ˆå¸Œ
        user_info = {k: v for k, v in user.items() if k != 'password_hash'}
        return jsonify({'user': user_info})
    return jsonify({'error': 'æœªç™»å½•'}), 401

@app.route('/api/users', methods=['GET'])
def get_users():
    """è·å–æ‰€æœ‰ç”¨æˆ·ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        users = db.get_all_users()
        # ä¸è¿”å›å¯†ç å“ˆå¸Œ
        for user in users:
            user.pop('password_hash', None)
        return jsonify({'users': users})
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/users', methods=['POST'])
def create_user():
    """åˆ›å»ºæ–°ç”¨æˆ·ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        if not data or not data.get('username') or not data.get('password'):
            return jsonify({'error': 'ç”¨æˆ·åå’Œå¯†ç ä¸èƒ½ä¸ºç©º'}), 400

        username = data['username'].strip()
        password = data['password'].strip()
        role = data.get('role', 'user')
        shop_ids = data.get('shops', [])

        if len(password) < 6:
            return jsonify({'error': 'å¯†ç é•¿åº¦è‡³å°‘6ä½'}), 400

        from werkzeug.security import generate_password_hash
        password_hash = generate_password_hash(password)

        if db.create_user(username, password_hash, role):
            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT id, username, role, is_active, image_search_count, created_at FROM users WHERE username = ?", (username,))
                user = cursor.fetchone()

            if user:
                user_dict = dict(user)
                if shop_ids:
                    db.update_user_shops(user_dict['id'], shop_ids)
                return jsonify({'user': user_dict, 'message': 'ç”¨æˆ·åˆ›å»ºæˆåŠŸ'})
            else:
                return jsonify({'error': 'ç”¨æˆ·åˆ›å»ºåæ— æ³•æ£€ç´¢ä¿¡æ¯'}), 500
        else:
            return jsonify({'error': 'ç”¨æˆ·åå·²å­˜åœ¨æˆ–æ•°æ®åº“é”™è¯¯'}), 400
    except Exception as e:
        logger.error(f"åˆ›å»ºç”¨æˆ·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/users/<int:user_id>', methods=['DELETE'])
def delete_user(user_id):
    """åˆ é™¤ç”¨æˆ·ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        current_user = get_current_user()
        if current_user['id'] == user_id:
            return jsonify({'error': 'ä¸èƒ½åˆ é™¤è‡ªå·±çš„è´¦å·'}), 400

        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        user = db.get_user_by_id(user_id)
        if not user:
            return jsonify({'error': 'ç”¨æˆ·ä¸å­˜åœ¨'}), 404

        # åˆ é™¤ç”¨æˆ·
        if db.delete_user(user_id):
            logger.info(f"ç®¡ç†å‘˜ {current_user['username']} åˆ é™¤äº†ç”¨æˆ· {user['username']}")
            return jsonify({'message': 'ç”¨æˆ·åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'ç”¨æˆ·åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ é™¤ç”¨æˆ·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === æ–°å¢ï¼šç®¡ç†å‘˜ä¿®æ”¹ç”¨æˆ·å¯†ç  ===
@app.route('/api/users/<int:user_id>/password', methods=['PUT'])
def reset_user_password(user_id):
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        new_password = data.get('password')
        if not new_password:
            return jsonify({'error': 'å¯†ç ä¸èƒ½ä¸ºç©º'}), 400

        from werkzeug.security import generate_password_hash
        password_hash = generate_password_hash(new_password)

        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("UPDATE users SET password_hash = ? WHERE id = ?", (password_hash, user_id))
            conn.commit()

        return jsonify({'success': True, 'message': 'å¯†ç å·²é‡ç½®'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# === æ–°å¢ï¼šç½‘ç«™é…ç½®ç®¡ç†API ===
@app.route('/api/websites', methods=['GET'])
def get_website_configs():
    """è·å–æ‰€æœ‰ç½‘ç«™é…ç½®åŠå…¶ç”¨æˆ·ç›¸å…³çš„é¢‘é“ç»‘å®šå’Œè´¦å·ç»‘å®š"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        configs = db.get_website_configs()

        # ä¸ºæ¯ä¸ªé…ç½®æ·»åŠ ç»‘å®šä¿¡æ¯
        for config in configs:
            config_id = config['id']

            # 1) è´¦å·ç»‘å®šï¼šåªè¿”å›å½“å‰ç”¨æˆ·è‡ªå·±çš„ç»‘å®š
            config['accounts'] = db.get_website_account_bindings(config_id, current_user['id'])

            # 2) é¢‘é“ç»‘å®šï¼šåªè¿”å›å½“å‰ç”¨æˆ·è‡ªå·±çš„ç»‘å®š
            config['channels'] = db.get_website_channel_bindings(config_id, current_user['id'])

            # 3) ç”¨æˆ·çº§åˆ«çš„è½®æ¢è®¾ç½®
            user_settings = db.get_user_website_settings(current_user['id'], config_id)
            config['rotation_interval'] = user_settings.get('rotation_interval', 180)
            config['rotation_enabled'] = user_settings.get('rotation_enabled', 1)

        return jsonify({'websites': configs})
    except Exception as e:
        logger.error(f"è·å–ç½‘ç«™é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites', methods=['POST'])
def add_website_config():
    """æ·»åŠ ç½‘ç«™é…ç½®"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        name = data.get('name')
        display_name = data.get('display_name')
        url_template = data.get('url_template')
        id_pattern = data.get('id_pattern')
        badge_color = data.get('badge_color', 'blue')
        reply_template = data.get('reply_template') or '{url}'
        image_similarity_threshold = data.get('image_similarity_threshold', None)
        blocked_role_ids = data.get('blocked_role_ids', None)

        if not all([name, display_name, url_template, id_pattern]):
            return jsonify({'error': 'æ‰€æœ‰å­—æ®µéƒ½æ˜¯å¿…å¡«çš„'}), 400

        def _normalize_similarity(value):
            if value is None or value == '':
                return None
            try:
                val = float(value)
            except (TypeError, ValueError):
                raise ValueError('ç›¸ä¼¼åº¦å¿…é¡»æ˜¯æ•°å­—')
            if not (0.0 <= val <= 1.0):
                raise ValueError('ç›¸ä¼¼åº¦å¿…é¡»åœ¨0.0-1.0ä¹‹é—´')
            return val

        def _normalize_blocked_roles(value):
            if value is None:
                return '[]'
            roles = []
            if isinstance(value, list):
                roles = [str(r).strip() for r in value if str(r).strip()]
            elif isinstance(value, str):
                try:
                    parsed = json.loads(value)
                    if isinstance(parsed, list):
                        roles = [str(r).strip() for r in parsed if str(r).strip()]
                    else:
                        roles = [s.strip() for s in value.split(',') if s.strip()]
                except Exception:
                    roles = [s.strip() for s in value.split(',') if s.strip()]
            return json.dumps(roles, ensure_ascii=False)

        try:
            image_similarity_threshold = _normalize_similarity(image_similarity_threshold)
            blocked_role_ids = _normalize_blocked_roles(blocked_role_ids)
        except ValueError as e:
            return jsonify({'error': str(e)}), 400

        if db.add_website_config(
            name,
            display_name,
            url_template,
            id_pattern,
            badge_color,
            reply_template,
            image_similarity_threshold,
            blocked_role_ids
        ):
            return jsonify({'success': True, 'message': 'ç½‘ç«™é…ç½®å·²æ·»åŠ '})
        else:
            return jsonify({'error': 'æ·»åŠ å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ·»åŠ ç½‘ç«™é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>', methods=['PUT'])
def update_website_config(config_id):
    """æ›´æ–°ç½‘ç«™é…ç½®"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        name = data.get('name')
        display_name = data.get('display_name')
        url_template = data.get('url_template')
        id_pattern = data.get('id_pattern')
        badge_color = data.get('badge_color', 'blue')
        reply_template = data.get('reply_template') or '{url}'
        image_similarity_threshold = data.get('image_similarity_threshold', None)
        blocked_role_ids = data.get('blocked_role_ids', None)

        if not all([name, display_name, url_template, id_pattern]):
            return jsonify({'error': 'æ‰€æœ‰å­—æ®µéƒ½æ˜¯å¿…å¡«çš„'}), 400

        def _normalize_similarity(value):
            if value is None or value == '':
                return None
            try:
                val = float(value)
            except (TypeError, ValueError):
                raise ValueError('ç›¸ä¼¼åº¦å¿…é¡»æ˜¯æ•°å­—')
            if not (0.0 <= val <= 1.0):
                raise ValueError('ç›¸ä¼¼åº¦å¿…é¡»åœ¨0.0-1.0ä¹‹é—´')
            return val

        def _normalize_blocked_roles(value):
            if value is None:
                return '[]'
            roles = []
            if isinstance(value, list):
                roles = [str(r).strip() for r in value if str(r).strip()]
            elif isinstance(value, str):
                try:
                    parsed = json.loads(value)
                    if isinstance(parsed, list):
                        roles = [str(r).strip() for r in parsed if str(r).strip()]
                    else:
                        roles = [s.strip() for s in value.split(',') if s.strip()]
                except Exception:
                    roles = [s.strip() for s in value.split(',') if s.strip()]
            return json.dumps(roles, ensure_ascii=False)

        try:
            image_similarity_threshold = _normalize_similarity(image_similarity_threshold)
            blocked_role_ids = _normalize_blocked_roles(blocked_role_ids)
        except ValueError as e:
            return jsonify({'error': str(e)}), 400

        if db.update_website_config(
            config_id,
            name,
            display_name,
            url_template,
            id_pattern,
            badge_color,
            reply_template,
            image_similarity_threshold,
            blocked_role_ids
        ):
            return jsonify({'success': True, 'message': 'ç½‘ç«™é…ç½®å·²æ›´æ–°'})
        else:
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°ç½‘ç«™é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>', methods=['DELETE'])
def delete_website_config(config_id):
    """åˆ é™¤ç½‘ç«™é…ç½®"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        if db.delete_website_config(config_id):
            return jsonify({'success': True, 'message': 'ç½‘ç«™é…ç½®å·²åˆ é™¤'})
        else:
            return jsonify({'error': 'åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ é™¤ç½‘ç«™é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/channels', methods=['GET'])
def get_website_channels(config_id):
    """è·å–ç½‘ç«™ç»‘å®šçš„é¢‘é“ï¼ˆæŒ‰ç”¨æˆ·è¿‡æ»¤ï¼‰"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        channels = db.get_website_channel_bindings(config_id, current_user['id'])
        return jsonify({'channels': channels})
    except Exception as e:
        logger.error(f"è·å–ç½‘ç«™é¢‘é“å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/channels', methods=['POST'])
def add_website_channel(config_id):
    """æ·»åŠ ç½‘ç«™é¢‘é“ç»‘å®š"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        data = request.get_json()
        channel_id = data.get('channel_id')

        if not channel_id:
            return jsonify({'error': 'é¢‘é“IDä¸èƒ½ä¸ºç©º'}), 400

        # ã€ä¿®å¤ã€‘å¦‚æœè¾“å…¥çš„æ˜¯å®Œæ•´çš„Discord URLï¼Œæå–é¢‘é“ID
        # Discord URLæ ¼å¼: https://discord.com/channels/{server_id}/{channel_id}
        if 'discord.com/channels/' in channel_id:
            # æå–URLä¸­çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºé¢‘é“ID
            parts = channel_id.rstrip('/').split('/')
            if len(parts) >= 1:
                channel_id = parts[-1]

        # éªŒè¯é¢‘é“IDæ˜¯å¦ä¸ºçº¯æ•°å­—
        if not channel_id.isdigit():
            return jsonify({'error': 'æ— æ•ˆçš„é¢‘é“IDæ ¼å¼'}), 400

        current_user = get_current_user()
        if db.add_website_channel_binding(config_id, channel_id, current_user['id']):
            return jsonify({'success': True, 'message': 'é¢‘é“ç»‘å®šå·²æ·»åŠ '})
        else:
            return jsonify({'error': 'æ·»åŠ å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ·»åŠ ç½‘ç«™é¢‘é“ç»‘å®šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/channels/<channel_id>', methods=['DELETE'])
def remove_website_channel(config_id, channel_id):
    """ç§»é™¤ç½‘ç«™é¢‘é“ç»‘å®š"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        # ã€ä¿®å¤ã€‘å¦‚æœchannel_idæ˜¯å®Œæ•´çš„Discord URLï¼Œæå–é¢‘é“ID
        # Discord URLæ ¼å¼: https://discord.com/channels/{server_id}/{channel_id}
        if 'discord.com/channels/' in channel_id:
            # æå–URLä¸­çš„æœ€åä¸€éƒ¨åˆ†ä½œä¸ºé¢‘é“ID
            parts = channel_id.rstrip('/').split('/')
            if len(parts) >= 1:
                channel_id = parts[-1]

        current_user = get_current_user()

        # ã€ä¿®å¤ã€‘ç®¡ç†å‘˜å¯ä»¥åˆ é™¤ä»»ä½•é¢‘é“ï¼Œæ™®é€šç”¨æˆ·åªèƒ½åˆ é™¤è‡ªå·±çš„
        if current_user.get('role') == 'admin':
            # ç®¡ç†å‘˜ï¼šåˆ é™¤è¯¥é¢‘é“çš„æ‰€æœ‰ç»‘å®š
            success = db.remove_website_channel_binding_admin(config_id, channel_id)
        else:
            # æ™®é€šç”¨æˆ·ï¼šåªåˆ é™¤è‡ªå·±çš„ç»‘å®š
            success = db.remove_website_channel_binding(config_id, channel_id, current_user['id'])

        if success:
            return jsonify({'success': True, 'message': 'é¢‘é“ç»‘å®šå·²ç§»é™¤'})
        else:
            return jsonify({'error': 'ç§»é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"ç§»é™¤ç½‘ç«™é¢‘é“ç»‘å®šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# ===== ç½‘ç«™è´¦å·ç»‘å®šAPI =====

@app.route('/api/websites/<int:config_id>/accounts', methods=['GET'])
def get_website_accounts(config_id):
    """è·å–ç½‘ç«™ç»‘å®šçš„è´¦å·ï¼ˆæŒ‰ç”¨æˆ·è¿‡æ»¤ï¼‰"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        accounts = db.get_website_account_bindings(config_id, current_user['id'])
        return jsonify({'accounts': accounts})
    except Exception as e:
        logger.error(f"è·å–ç½‘ç«™è´¦å·ç»‘å®šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/accounts', methods=['POST'])
def add_website_account(config_id):
    """ä¸ºç½‘ç«™ç»‘å®šè´¦å·"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        data = request.get_json()
        account_id = data.get('account_id')
        role = data.get('role', 'both')  # 'listener', 'sender', 'both'

        if not account_id or role not in ['listener', 'sender', 'both']:
            return jsonify({'error': 'æ— æ•ˆçš„è´¦å·IDæˆ–è§’è‰²'}), 400

        # æƒé™æ£€æŸ¥ï¼šç¡®ä¿è¯¥è´¦å·å±äºå½“å‰ç”¨æˆ·
        current_user = get_current_user()
        # è·å–è¯¥è´¦å·çš„è¯¦æƒ…
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT user_id FROM discord_accounts WHERE id = ?", (account_id,))
            row = cursor.fetchone()
            if not row:
                return jsonify({'error': 'è´¦å·ä¸å­˜åœ¨'}), 404

            account_owner_id = row[0]

            # å¦‚æœä¸æ˜¯ç®¡ç†å‘˜ï¼Œä¸”è´¦å·ä¸å±äºå½“å‰ç”¨æˆ·ï¼Œæ‹’ç»
            if current_user['role'] != 'admin' and account_owner_id != current_user['id']:
                return jsonify({'error': 'æ‚¨æ— æƒæ“ä½œæ­¤è´¦å·'}), 403

        if db.add_website_account_binding(config_id, account_id, role, current_user['id']):
            return jsonify({'success': True, 'message': f'è´¦å·ç»‘å®šæˆåŠŸï¼Œè§’è‰²: {role}'})
        else:
            return jsonify({'error': 'ç»‘å®šå¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ·»åŠ ç½‘ç«™è´¦å·ç»‘å®šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/accounts/<int:account_id>', methods=['DELETE'])
def remove_website_account(config_id, account_id):
    """ç§»é™¤ç½‘ç«™è´¦å·ç»‘å®š"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        if db.remove_website_account_binding(config_id, account_id, current_user['id']):
            return jsonify({'success': True, 'message': 'è´¦å·ç»‘å®šå·²ç§»é™¤'})
        else:
            return jsonify({'error': 'ç§»é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"ç§»é™¤ç½‘ç«™è´¦å·ç»‘å®šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/rotation', methods=['GET'])
def get_website_rotation(config_id):
    """è·å–ç”¨æˆ·çš„ç½‘ç«™è½®æ¢é…ç½®"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        settings = db.get_user_website_settings(current_user['id'], config_id)
        return jsonify({
            'rotation_interval': settings['rotation_interval'],
            'rotation_enabled': settings['rotation_enabled']
        })
    except Exception as e:
        logger.error(f"è·å–ç½‘ç«™è½®æ¢é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/rotation', methods=['PUT'])
def update_website_rotation(config_id):
    """æ›´æ–°ç”¨æˆ·çš„ç½‘ç«™è½®æ¢é…ç½®ï¼ˆé—´éš”å’Œå¯ç”¨çŠ¶æ€ï¼‰"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        data = request.get_json()
        messages = []

        rotation_interval = data.get('rotation_interval')
        rotation_enabled = data.get('rotation_enabled')

        # éªŒè¯å‚æ•°
        if rotation_interval is not None and rotation_interval <= 0:
            return jsonify({'error': 'è½®æ¢é—´éš”å¿…é¡»å¤§äº0ç§’'}), 400
        if rotation_enabled is not None and rotation_enabled not in [0, 1]:
            return jsonify({'error': 'è½®æ¢å¯ç”¨çŠ¶æ€å¿…é¡»æ˜¯0æˆ–1'}), 400

        # ä½¿ç”¨ç”¨æˆ·çº§åˆ«çš„è®¾ç½®æ–¹æ³•
        if db.update_user_website_rotation(current_user['id'], config_id, rotation_interval, rotation_enabled):
            if rotation_interval is not None:
                messages.append(f'è½®æ¢é—´éš”å·²è®¾ç½®ä¸º {rotation_interval} ç§’')
            if rotation_enabled is not None:
                status_text = 'å¯ç”¨' if rotation_enabled else 'ç¦ç”¨'
                messages.append(f'è½®æ¢åŠŸèƒ½å·²{status_text}')
            return jsonify({'success': True, 'message': '; '.join(messages) if messages else 'è®¾ç½®å·²æ›´æ–°'})
        else:
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°ç½‘ç«™è½®æ¢é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/filters', methods=['GET'])
def get_website_filters(config_id):
    """è·å–ç”¨æˆ·çš„ç½‘ç«™æ¶ˆæ¯è¿‡æ»¤æ¡ä»¶"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        settings = db.get_user_website_settings(current_user['id'], config_id)

        import json
        filters = json.loads(settings.get('message_filters', '[]'))
        return jsonify({'filters': filters})
    except Exception as e:
        logger.error(f"è·å–ç½‘ç«™è¿‡æ»¤æ¡ä»¶å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/websites/<int:config_id>/filters', methods=['PUT'])
def update_website_filters(config_id):
    """æ›´æ–°ç”¨æˆ·çš„ç½‘ç«™æ¶ˆæ¯è¿‡æ»¤æ¡ä»¶"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        data = request.get_json()
        filters = data.get('filters', [])

        # éªŒè¯è¿‡æ»¤æ¡ä»¶æ ¼å¼
        for filter_item in filters:
            if not isinstance(filter_item, dict) or 'filter_type' not in filter_item or 'filter_value' not in filter_item:
                return jsonify({'error': 'è¿‡æ»¤æ¡ä»¶æ ¼å¼æ— æ•ˆ'}), 400

        import json
        filters_json = json.dumps(filters)

        if db.update_user_website_filters(current_user['id'], config_id, filters_json):
            return jsonify({'success': True, 'message': f'å·²æ›´æ–° {len(filters)} ä¸ªè¿‡æ»¤æ¡ä»¶'})
        else:
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°ç½‘ç«™è¿‡æ»¤æ¡ä»¶å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/products/<int:product_id>/urls', methods=['GET'])
def get_product_urls(product_id):
    """è·å–å•†å“çš„æ‰€æœ‰ç½‘ç«™URL"""
    try:
        # è·å–å•†å“ä¿¡æ¯
        product = db._get_product_info_by_id(product_id)
        if not product:
            return jsonify({'error': 'å•†å“ä¸å­˜åœ¨'}), 404

        # ä»å•†å“URLä¸­æå–å¾®åº—ID
        weidian_url = product.get('product_url', '')
        weidian_id = None

        if 'itemID=' in weidian_url:
            # æå–itemIDå‚æ•°
            import re
            match = re.search(r'itemID=([^&]+)', weidian_url)
            if match:
                weidian_id = match.group(1)

        if not weidian_id:
            return jsonify({'urls': []})

        # ç”Ÿæˆæ‰€æœ‰ç½‘ç«™çš„URL
        urls = db.generate_website_urls(weidian_id)
        return jsonify({'urls': urls})
    except Exception as e:
        logger.error(f"è·å–å•†å“URLå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼ˆä¸éœ€è¦è®¤è¯ï¼Œå¿«é€Ÿå“åº”ï¼‰===
@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œè¿”å›åç«¯å’ŒAIæ¨¡å‹çŠ¶æ€"""
    return jsonify({
        'status': 'ok',
        'backend': 'running',
        'ai_ready': ai_model_ready,
        'timestamp': datetime.now().isoformat()
    })

# === æ–°å¢ï¼šç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯API ===
@app.route('/api/system/stats', methods=['GET'])
def get_system_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯ (å¸¦æƒé™éš”ç¦»)"""
    try:
        user = get_current_user()
        if not user:
            return jsonify({
                'shop_count': 0,
                'product_count': 0,
                'image_count': 0,
                'user_count': 0,
                'total_replies': 0,
                'daily_replies_total': 0
            })
        stats = db.get_system_stats(user['id'], user['role'])
        return jsonify(stats)
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/system/cleanup-orphaned-images', methods=['POST'])
def cleanup_orphaned_images():
    """æ¸…ç†å­¤ç«‹çš„å›¾ç‰‡è®°å½•"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        deleted_count = db.cleanup_orphaned_images()
        return jsonify({
            'message': f'æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} æ¡å­¤ç«‹è®°å½•',
            'deleted_count': deleted_count
        })
    except Exception as e:
        logger.error(f"æ¸…ç†å­¤ç«‹å›¾ç‰‡è®°å½•å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === æ–°å¢ï¼šå…¬å‘Šç®¡ç†API ===
@app.route('/api/announcements', methods=['GET'])
def get_announcements():
    """è·å–æ‰€æœ‰å…¬å‘Š"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        announcements = db.get_active_announcements()
        return jsonify({'announcements': announcements})
    except Exception as e:
        logger.error(f"è·å–å…¬å‘Šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/announcements', methods=['POST'])
def create_announcement():
    """åˆ›å»ºå…¬å‘Š"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        title = data.get('title')
        content = data.get('content')

        if not title or not content:
            return jsonify({'error': 'æ ‡é¢˜å’Œå†…å®¹éƒ½æ˜¯å¿…å¡«çš„'}), 400

        if db.create_announcement(title, content):
            return jsonify({'success': True, 'message': 'å…¬å‘Šåˆ›å»ºæˆåŠŸ'})
        else:
            return jsonify({'error': 'åˆ›å»ºå¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ›å»ºå…¬å‘Šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/announcements/<int:announcement_id>', methods=['PUT'])
def update_announcement(announcement_id):
    """æ›´æ–°å…¬å‘Š"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        title = data.get('title')
        content = data.get('content')
        is_active = data.get('is_active', True)

        if not title or not content:
            return jsonify({'error': 'æ ‡é¢˜å’Œå†…å®¹éƒ½æ˜¯å¿…å¡«çš„'}), 400

        if db.update_announcement(announcement_id, title, content, is_active):
            return jsonify({'success': True, 'message': 'å…¬å‘Šæ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°å…¬å‘Šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/announcements/<int:announcement_id>', methods=['DELETE'])
def delete_announcement(announcement_id):
    """åˆ é™¤å…¬å‘Š"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        if db.delete_announcement(announcement_id):
            return jsonify({'success': True, 'message': 'å…¬å‘Šåˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ é™¤å…¬å‘Šå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === æ–°å¢ï¼šæ¶ˆæ¯è¿‡æ»¤è§„åˆ™API ===
@app.route('/api/message-filters', methods=['GET'])
def get_message_filters():
    """è·å–æ¶ˆæ¯è¿‡æ»¤è§„åˆ™"""
    try:
        filters = db.get_message_filters()
        return jsonify({'filters': filters})
    except Exception as e:
        logger.error(f"è·å–æ¶ˆæ¯è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/message-filters', methods=['POST'])
def add_message_filter():
    """æ·»åŠ æ¶ˆæ¯è¿‡æ»¤è§„åˆ™"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        filter_type = data.get('filter_type')
        filter_value = data.get('filter_value')

        if not filter_type or (filter_type != 'image' and not filter_value):
            return jsonify({'error': 'è¿‡æ»¤ç±»å‹å’Œå€¼éƒ½æ˜¯å¿…å¡«çš„'}), 400

        if filter_type == 'image' and not filter_value:
            filter_value = ''

        if db.add_message_filter(filter_type, filter_value):
            return jsonify({'success': True, 'message': 'è¿‡æ»¤è§„åˆ™æ·»åŠ æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ·»åŠ å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ·»åŠ æ¶ˆæ¯è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/message-filters/<int:filter_id>', methods=['PUT'])
def update_message_filter(filter_id):
    """æ›´æ–°æ¶ˆæ¯è¿‡æ»¤è§„åˆ™"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        filter_type = data.get('filter_type')
        filter_value = data.get('filter_value')
        is_active = data.get('is_active', True)

        if not filter_type or (filter_type != 'image' and not filter_value):
            return jsonify({'error': 'è¿‡æ»¤ç±»å‹å’Œå€¼éƒ½æ˜¯å¿…å¡«çš„'}), 400

        if filter_type == 'image' and not filter_value:
            filter_value = ''

        if db.update_message_filter(filter_id, filter_type, filter_value, is_active):
            return jsonify({'success': True, 'message': 'è¿‡æ»¤è§„åˆ™æ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°æ¶ˆæ¯è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/message-filters/<int:filter_id>', methods=['DELETE'])
def delete_message_filter(filter_id):
    """åˆ é™¤æ¶ˆæ¯è¿‡æ»¤è§„åˆ™"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        if db.delete_message_filter(filter_id):
            return jsonify({'success': True, 'message': 'è¿‡æ»¤è§„åˆ™åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ é™¤æ¶ˆæ¯è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === æ–°å¢ï¼šè‡ªå®šä¹‰å›å¤å†…å®¹API ===
@app.route('/api/custom-replies', methods=['GET'])
def get_custom_replies():
    """è·å–è‡ªå®šä¹‰å›å¤å†…å®¹"""
    try:
        replies = db.get_custom_replies()
        return jsonify({'replies': replies})
    except Exception as e:
        logger.error(f"è·å–è‡ªå®šä¹‰å›å¤å†…å®¹å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/custom-replies', methods=['POST'])
def add_custom_reply():
    """æ·»åŠ è‡ªå®šä¹‰å›å¤å†…å®¹"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        reply_type = data.get('reply_type')
        content = data.get('content')
        image_url = data.get('image_url')
        priority = data.get('priority', 0)

        if not reply_type:
            return jsonify({'error': 'å›å¤ç±»å‹æ˜¯å¿…å¡«çš„'}), 400

        if db.add_custom_reply(reply_type, content, image_url, priority):
            return jsonify({'success': True, 'message': 'è‡ªå®šä¹‰å›å¤æ·»åŠ æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ·»åŠ å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ·»åŠ è‡ªå®šä¹‰å›å¤å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/custom-replies/<int:reply_id>', methods=['PUT'])
def update_custom_reply(reply_id):
    """æ›´æ–°è‡ªå®šä¹‰å›å¤å†…å®¹"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        reply_type = data.get('reply_type')
        content = data.get('content')
        image_url = data.get('image_url')
        priority = data.get('priority', 0)
        is_active = data.get('is_active', True)

        if not reply_type:
            return jsonify({'error': 'å›å¤ç±»å‹æ˜¯å¿…å¡«çš„'}), 400

        if db.update_custom_reply(reply_id, reply_type, content, image_url, priority, is_active):
            return jsonify({'success': True, 'message': 'è‡ªå®šä¹‰å›å¤æ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°è‡ªå®šä¹‰å›å¤å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/custom-replies/<int:reply_id>', methods=['DELETE'])
def delete_custom_reply(reply_id):
    """åˆ é™¤è‡ªå®šä¹‰å›å¤å†…å®¹"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        if db.delete_custom_reply(reply_id):
            return jsonify({'success': True, 'message': 'è‡ªå®šä¹‰å›å¤åˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ é™¤è‡ªå®šä¹‰å›å¤å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/users/<int:user_id>/shops', methods=['PUT'])
def update_user_shops(user_id):
    """æ›´æ–°ç”¨æˆ·åº—é“ºæƒé™ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        shop_ids = data.get('shops', [])

        if db.update_user_shops(user_id, shop_ids):
            return jsonify({'message': 'æƒé™æ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'error': 'æƒé™æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°ç”¨æˆ·æƒé™å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts', methods=['GET'])
def get_accounts():
    """è·å–æ‰€æœ‰ Discord è´¦å·"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    current_user = get_current_user()
    try:
        # æ‰€æœ‰ç”¨æˆ·ï¼ˆåŒ…æ‹¬ç®¡ç†å‘˜ï¼‰åªèƒ½çœ‹åˆ°è‡ªå·±çš„è´¦å·
        accounts = db.get_discord_accounts_by_user(current_user['id'])

        return jsonify({'accounts': accounts})
    except Exception as e:
        logger.error(f"è·å–è´¦å·åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/products', methods=['GET'])
def list_products():
    """åˆ—å‡ºç”¨æˆ·æœ‰æƒé™çš„å•†å“åŠå…¶å›¾ç‰‡"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    current_user = get_current_user()
    try:
        # è·å–åˆ†é¡µå‚æ•°
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 50))  # é»˜è®¤æ¯é¡µ50æ¡
        offset = (page - 1) * limit
        keyword = (request.args.get('keyword') or '').strip()
        search_type = request.args.get('search_type', 'all')
        shop_name = (request.args.get('shop_name') or '').strip() or None

        # æ ¹æ®ç”¨æˆ·æƒé™è·å–å•†å“ï¼ˆæ”¯æŒåˆ†é¡µï¼‰
        if current_user['role'] == 'admin':
            # ç®¡ç†å‘˜å¯ä»¥çœ‹åˆ°æ‰€æœ‰å•†å“
            # é¿å…åˆ·å±ï¼šä¸è®°å½•å¸¸è§„åˆ—è¡¨æŸ¥è¯¢
            result = db.get_products_by_user_shops(
                None,
                limit=limit,
                offset=offset,
                keyword=keyword,
                search_type=search_type,
                shop_name=shop_name
            )
        else:
            # æ™®é€šç”¨æˆ·åªèƒ½çœ‹åˆ°è‡ªå·±ç®¡ç†çš„åº—é“ºçš„å•†å“
            user_shops = current_user.get('shops', [])
            logger.info(f"æ™®é€šç”¨æˆ· {current_user['username']} è·å–åº—é“ºå•†å“ (é¡µ{page}, æ¯é¡µ{limit}æ¡)ï¼Œåˆ†é…çš„åº—é“º: {user_shops}")
            result = db.get_products_by_user_shops(
                user_shops,
                limit=limit,
                offset=offset,
                keyword=keyword,
                search_type=search_type,
                shop_name=shop_name
            )

            # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®åº“ä¸­çš„å•†å“å’Œåº—é“ºåŒ¹é…æƒ…å†µ
            if user_shops:
                with db.get_connection() as conn:
                    cursor = conn.cursor()
                    placeholders = ','.join('?' * len(user_shops))
                    cursor.execute(f"SELECT COUNT(*) FROM products WHERE shop_name IN ({placeholders})", user_shops)
                    matching_products = cursor.fetchone()[0]
                    logger.info(f"æ•°æ®åº“ä¸­åŒ¹é…çš„å•†å“æ•°é‡: {matching_products}")

                    # åˆ—å‡ºæ‰€æœ‰åº—é“ºåç§°
                    cursor.execute("SELECT DISTINCT shop_name FROM products")
                    all_shop_names = [row[0] for row in cursor.fetchall()]
                    logger.info(f"æ•°æ®åº“ä¸­çš„æ‰€æœ‰åº—é“ºåç§°: {all_shop_names}")

        # é¿å…åˆ·å±ï¼šä¸è®°å½•å¸¸è§„åˆ—è¡¨æŸ¥è¯¢ç»“æœ

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯åˆ°å“åº”ä¸­
        response_data = {
            'products': result['products'],
            'total': result['total'],
            'debug': {
                'user_role': current_user['role'],
                'user_shops': current_user.get('shops', []),
                'is_admin': current_user['role'] == 'admin',
                'keyword': keyword,
                'search_type': search_type,
                'shop_name': shop_name
            }
        }

        # æ·»åŠ ç¼“å­˜å¤´ä»¥ä¼˜åŒ–æ€§èƒ½ï¼ˆ5åˆ†é’Ÿç¼“å­˜ï¼‰
        response = jsonify(response_data)
        response.headers['Cache-Control'] = 'private, max-age=300'
        return response
    except Exception as e:
        logger.error(f"åˆ—å‡ºå•†å“å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/products', methods=['PUT'])
def update_product():
    """æ›´æ–°å•†å“ä¿¡æ¯"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    current_user = get_current_user()

    # æå–å…¬å…±æƒé™æ£€æŸ¥é€»è¾‘
    def check_permission(product_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™æ›´æ–°æŒ‡å®šå•†å“"""
        if current_user['role'] == 'admin':
            return True

        user_shop_ids = current_user.get('shops', [])
        product = db.get_product_by_id(int(product_id))

        if not product:
            return False

        # å°†åº—é“ºIDè½¬æ¢ä¸ºåº—é“ºåç§°è¿›è¡Œå¯¹æ¯”
        allowed_shop_names = []
        for shop_id in user_shop_ids:
            shop_info = db.get_shop_by_id(shop_id)
            if shop_info:
                allowed_shop_names.append(shop_info['name'])

        # å¯¹æ¯”å•†å“æ‰€å±åº—é“ºåæ˜¯å¦åœ¨ç”¨æˆ·å…è®¸çš„åº—é“ºååˆ—è¡¨ä¸­
        if product.get('shop_name') in allowed_shop_names:
            return True

        return False

    def get_full_product_data(pid):
        """è·å–å¹¶æ ¼å¼åŒ–å®Œæ•´çš„å•†å“æ•°æ®ï¼Œç”¨äºå‰ç«¯çŠ¶æ€æ›´æ–°"""
        product = db._get_product_info_by_id(pid)
        if not product:
            return None

        # è·å–æ‰€æœ‰å›¾ç‰‡
        images_data = db.get_product_images(pid)
        # æŒ‰ç´¢å¼•æ’åºå¹¶ç”ŸæˆURL
        sorted_images = sorted(images_data, key=lambda x: x['image_index'])
        image_urls = [f"/api/image/{pid}/{img['image_index']}" for img in sorted_images]

        # æ ¼å¼åŒ–å­—æ®µä»¥åŒ¹é…å‰ç«¯éœ€æ±‚ (CamelCase)
        weidian_id = ''
        try:
            if 'itemID=' in product.get('product_url', ''):
                weidian_id = product.get('product_url', '').split('itemID=')[1]
            elif product.get('item_id'):
                weidian_id = product.get('item_id')
        except:
            pass

        # è§£æè‡ªå®šä¹‰å›¾ç‰‡URLå’Œç´¢å¼•
        selected_indexes = []
        custom_urls = []
        uploaded_reply_image_urls = []
        try:
            if product.get('custom_reply_images'):
                selected_indexes = json.loads(product.get('custom_reply_images'))
            if product.get('custom_image_urls'):
                custom_urls = json.loads(product.get('custom_image_urls'))
            # è§£æä¸Šä¼ çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡
            if product.get('uploaded_reply_images'):
                uploaded_filenames = json.loads(product.get('uploaded_reply_images'))
                # ç”Ÿæˆå›¾ç‰‡URLæ•°ç»„
                uploaded_reply_image_urls = [f"/api/custom_reply_image/{pid}/{filename}" for filename in uploaded_filenames]
        except:
            pass

        # å…³é”®ï¼šå¿…é¡»è¿”å›å‰ç«¯éœ€è¦çš„æ¯ä¸€ä¸ªå­—æ®µï¼Œå¦åˆ™å‰ç«¯ä¼šå˜ç™½
        return {
            'id': product['id'],
            'title': product.get('title', ''),
            'englishTitle': product.get('english_title', ''),
            'weidianUrl': product.get('product_url', ''),
            'cnfansUrl': product.get('cnfans_url', ''),
            'acbuyUrl': product.get('acbuy_url', ''),
            'shopName': product.get('shop_name', 'æœªçŸ¥åº—é“º'),
            'description': product.get('description', ''),

            # è§„åˆ™ç›¸å…³
            'ruleEnabled': bool(product.get('ruleEnabled', True)),
            'customReplyText': product.get('custom_reply_text', ''),
            'imageSource': product.get('image_source', 'product'),
            'replyScope': product.get('reply_scope', 'all'),

            # å›¾ç‰‡ç›¸å…³
            'selectedImageIndexes': selected_indexes,
            'customImageUrls': custom_urls,
            'images': image_urls, # åŒ…å«æ‰€æœ‰å•†å“å›¾ç‰‡
            'uploadedImages': uploaded_reply_image_urls, # ä¸Šä¼ çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡URLæ•°ç»„

            'weidianId': weidian_id,
            'createdAt': product.get('created_at')
        }

    # ---------------------------------------------------------
    # åœºæ™¯ A: åŒ…å«æ–‡ä»¶ä¸Šä¼  (Multipart)
    # ---------------------------------------------------------
    if request.content_type and 'multipart/form-data' in request.content_type:
        product_id = request.form.get('id')
        if not product_id:
            return jsonify({'error': 'å•†å“IDä¸èƒ½ä¸ºç©º'}), 400

        try:
            pid_int = int(product_id)
            if not check_permission(pid_int):
                return jsonify({'error': 'æ— æƒé™æ›´æ–°æ­¤å•†å“'}), 403

            # 1. å¤„ç†ä¸Šä¼ çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡
            # æ³¨æ„ï¼šè¿™äº›å›¾ç‰‡åªç”¨äºè‡ªå®šä¹‰å›å¤ï¼Œä¸æ·»åŠ åˆ°å•†å“å›¾é›†å’ŒFAISSç´¢å¼•

            # 1.1 è·å–è¦ä¿ç•™çš„å·²æœ‰å›¾ç‰‡æ–‡ä»¶ååˆ—è¡¨ï¼ˆä»å‰ç«¯ä¼ æ¥ï¼‰
            existing_filenames_to_keep = []
            if 'existingUploadedImageUrls' in request.form:
                try:
                    # å‰ç«¯å‘é€çš„æ˜¯URLæ•°ç»„çš„JSONå­—ç¬¦ä¸²ï¼Œéœ€è¦æå–æ–‡ä»¶å
                    existing_urls = json.loads(request.form.get('existingUploadedImageUrls'))
                    for url in existing_urls:
                        # URLæ ¼å¼: /api/custom_reply_image/{product_id}/{filename}
                        # æå–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºæ–‡ä»¶å
                        filename = url.split('/')[-1]
                        existing_filenames_to_keep.append(filename)
                except:
                    pass

            # 1.2 å¤„ç†æ–°ä¸Šä¼ çš„æ–‡ä»¶
            new_uploaded_filenames = []
            if 'uploadedImages' in request.files:
                import uuid
                import os

                # åˆ›å»ºè‡ªå®šä¹‰å›å¤å›¾ç‰‡ç›®å½•
                custom_reply_dir = os.path.join('data', 'custom_reply_images', str(pid_int))
                os.makedirs(custom_reply_dir, exist_ok=True)

                files = request.files.getlist('uploadedImages')
                for file in files:
                    if file and file.filename:
                        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                        filename = f"{uuid.uuid4()}_{file.filename}"
                        file_path = os.path.join(custom_reply_dir, filename)

                        # ä¿å­˜æ–‡ä»¶ï¼ˆä¸æ·»åŠ åˆ°å•†å“å›¾é›†ï¼Œä¸æå–ç‰¹å¾ï¼Œä¸åŠ å…¥FAISSï¼‰
                        file.save(file_path)
                        new_uploaded_filenames.append(filename)

                if new_uploaded_filenames:
                    logger.info(f"ä¿å­˜äº† {len(new_uploaded_filenames)} å¼ æ–°çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡åˆ° {custom_reply_dir}")

            # 1.3 åˆå¹¶å·²æœ‰å›¾ç‰‡å’Œæ–°ä¸Šä¼ çš„å›¾ç‰‡
            all_uploaded_filenames = existing_filenames_to_keep + new_uploaded_filenames
            if existing_filenames_to_keep:
                logger.info(f"ä¿ç•™äº† {len(existing_filenames_to_keep)} å¼ å·²æœ‰çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡")

            # 2. æ„å»ºæ›´æ–°æ•°æ®
            updates = {}

            # å¦‚æœæœ‰ä¸Šä¼ çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡ï¼ˆå·²æœ‰çš„æˆ–æ–°ä¸Šä¼ çš„ï¼‰ï¼Œå°†æ–‡ä»¶ååˆ—è¡¨å­˜å‚¨åˆ°æ•°æ®åº“
            if all_uploaded_filenames:
                updates['uploaded_reply_images'] = json.dumps(all_uploaded_filenames)
            for key in ['title', 'englishTitle', 'ruleEnabled', 'customReplyText', 'imageSource', 'replyScope']:
                value = request.form.get(key)
                if value is not None:
                    if key == 'englishTitle':
                        updates['english_title'] = value
                    elif key == 'ruleEnabled':
                        # å…¼å®¹å­—ç¬¦ä¸² 'true'/'false' å’Œ '1'/'0'
                        if str(value).lower() in ['true', '1']:
                            updates['ruleEnabled'] = 1
                        else:
                            updates['ruleEnabled'] = 0
                    elif key == 'customReplyText':
                        updates['custom_reply_text'] = value
                    elif key == 'imageSource':
                        updates['image_source'] = value
                    elif key == 'replyScope':
                        updates['reply_scope'] = value
                    else:
                        updates[key] = value

            # 3. å¤„ç†æ•°ç»„æ•°æ® (JSON)
            if 'selectedImageIndexes' in request.form:
                updates['custom_reply_images'] = request.form.get('selectedImageIndexes') # å·²ç»æ˜¯JSONå­—ç¬¦ä¸²

            if 'customImageUrls' in request.form:
                updates['custom_image_urls'] = request.form.get('customImageUrls') # å·²ç»æ˜¯JSONå­—ç¬¦ä¸²

            # 4. æ‰§è¡Œæ›´æ–°
            if updates:
                db.update_product(pid_int, updates)

            # 5. è¿”å›å®Œæ•´æ•°æ® (è§£å†³é—ªçƒé—®é¢˜)
            full_product = get_full_product_data(pid_int)
            return jsonify({'message': 'å•†å“æ›´æ–°æˆåŠŸ', 'product': full_product})

        except Exception as e:
            logger.error(f"æ›´æ–°å•†å“å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500
    # ---------------------------------------------------------
    # åœºæ™¯ B: ä»… JSON æ•°æ®æ›´æ–°
    # ---------------------------------------------------------
    else:
        data = request.get_json()
        if not data or not data.get('id'):
            return jsonify({'error': 'å•†å“IDä¸èƒ½ä¸ºç©º'}), 400

        product_id = data['id']

        try:
            if not check_permission(product_id):
                return jsonify({'error': 'æ— æƒé™æ›´æ–°æ­¤å•†å“'}), 403

            updates = {}
            if 'title' in data:
                updates['title'] = data['title']
            if 'englishTitle' in data:
                updates['english_title'] = data['englishTitle']
            if 'ruleEnabled' in data:
                updates['ruleEnabled'] = 1 if data['ruleEnabled'] else 0
            if 'customReplyText' in data:
                updates['custom_reply_text'] = data['customReplyText']
            if 'replyScope' in data:
                updates['reply_scope'] = data['replyScope']
            if 'selectedImageIndexes' in data:
                updates['custom_reply_images'] = json.dumps(data['selectedImageIndexes'])
            if 'customImageUrls' in data:
                updates['custom_image_urls'] = json.dumps(data['customImageUrls'])
            if 'imageSource' in data:
                updates['image_source'] = data['imageSource']

            if updates:
                db.update_product(product_id, updates)

            # è¿”å›å®Œæ•´æ•°æ®
            full_product = get_full_product_data(product_id)
            return jsonify({'message': 'å•†å“æ›´æ–°æˆåŠŸ', 'product': full_product})

        except Exception as e:
            logger.error(f"æ›´æ–°å•†å“å¤±è´¥: {e}")
            return jsonify({'error': 'æ›´æ–°å¤±è´¥'}), 500


@app.route('/api/backfill_products', methods=['POST'])
def backfill_products():
    """ä¸ºå·²å­˜åœ¨ä½†ç¼ºå°‘è‹±åæˆ– cnfans é“¾æ¥çš„å•†å“å›å¡«æ•°æ®"""
    try:
        from weidian_scraper import get_weidian_scraper
        scraper = get_weidian_scraper()

        updated = []
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id, product_url, english_title, cnfans_url FROM products")
            rows = cursor.fetchall()

            for row in rows:
                pid = row['id']
                url = row['product_url']
                need_english = not row['english_title']
                need_cnfans = not row['cnfans_url']
                if not (need_english or need_cnfans):
                    continue

                product_info = scraper.scrape_product_info(url)
                if not product_info:
                    logger.warning(f"å›å¡«å¤±è´¥ï¼Œæ— æ³•æŠ“å–: {url}")
                    continue

                english = product_info.get('english_title') or ''
                cnfans = product_info.get('cnfans_url') or ''

                cursor.execute("""
                    UPDATE products
                    SET english_title = ?, cnfans_url = ?
                    WHERE id = ?
                """, (english, cnfans, pid))
                conn.commit()
                updated.append(pid)

        return jsonify({'updated': updated, 'count': len(updated)})
    except Exception as e:
        logger.error(f"å›å¡«å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/rebuild_index', methods=['POST'])
def rebuild_index():
    """é‡å»ºFAISSç´¢å¼•ï¼Œæ¸…ç†è¢«åˆ é™¤çš„å‘é‡"""
    try:
        try:
            from vector_engine import get_vector_engine
        except ImportError:
            from .vector_engine import get_vector_engine
        from feature_extractor import get_feature_extractor

        logger.info("å¼€å§‹é‡å»ºFAISSç´¢å¼•...")

        # è·å–æ‰€æœ‰æœ‰æ•ˆçš„å›¾ç‰‡è®°å½•ï¼ˆç¡®ä¿å›¾ç‰‡æ–‡ä»¶å­˜åœ¨ï¼‰
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT pi.id, pi.product_id, pi.image_path, pi.image_index
                FROM product_images pi
                JOIN products p ON pi.product_id = p.id
                ORDER BY pi.id
            """)
            all_records = cursor.fetchall()

        # è¿‡æ»¤å‡ºæ–‡ä»¶å­˜åœ¨çš„è®°å½•
        image_records = []
        for record in all_records:
            if os.path.exists(record['image_path']):
                image_records.append(record)
            else:
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {record['image_path']}")

        if not image_records:
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡è®°å½•'}), 400

        logger.info(f"æ‰¾åˆ° {len(image_records)} å¼ å›¾ç‰‡è®°å½•")

        # é‡æ–°æå–ç‰¹å¾å¹¶é‡å»ºç´¢å¼•
        extractor = get_feature_extractor()
        engine = get_vector_engine()

        # åˆ›å»ºæ–°ç´¢å¼•
        vectors_data = []
        for record in image_records:
            try:
                image_path = record['image_path']
                if not os.path.exists(image_path):
                    logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                    continue

                # æå–ç‰¹å¾
                features = extractor.extract_feature(image_path)
                if features is not None:
                    vectors_data.append((record['id'], features))
                    logger.info(f"é‡æ–°æå–ç‰¹å¾: {record['id']}")
                else:
                    logger.warning(f"ç‰¹å¾æå–å¤±è´¥: {image_path}")

            except Exception as e:
                logger.error(f"å¤„ç†å›¾ç‰‡ {record['id']} å¤±è´¥: {e}")
                continue

        # é‡å»ºç´¢å¼•
        success = engine.rebuild_index(vectors_data)
        if success:
            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŒ…å« {len(vectors_data)} ä¸ªå‘é‡")
            return jsonify({
                'success': True,
                'message': f'ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŒ…å« {len(vectors_data)} ä¸ªæœ‰æ•ˆå‘é‡',
                'total_vectors': len(vectors_data)
            })
        else:
            return jsonify({'error': 'ç´¢å¼•é‡å»ºå¤±è´¥'}), 500

    except Exception as e:
        logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/rebuild_vectors', methods=['POST'])
def rebuild_vectors():
    """ä¸ºå·²æœ‰å•†å“ï¼ˆæˆ–ç¼ºå¤±å‘é‡çš„å›¾ç‰‡ï¼‰é‡å»ºç‰¹å¾å¹¶æ’å…¥ FAISS"""
    try:
        extractor = get_feature_extractor()
        rebuilt = []
        failed = []

        with db.get_connection() as conn:
            cursor = conn.cursor()
            # æŸ¥æ‰¾æ‰€æœ‰ product_images ä¸­ milvus_id ä¸ºç©ºæˆ– NULL çš„è®°å½•
            cursor.execute("SELECT id, product_id, image_path, image_index FROM product_images WHERE milvus_id IS NULL OR milvus_id = ''")
            rows = cursor.fetchall()

        for row in rows:
            pid = row['product_id']
            img_path = row['image_path']
            idx = row['image_index']
            try:
                features = extractor.extract_feature(img_path)
                if features is None:
                    logger.error(f"é‡å»ºç‰¹å¾å¤±è´¥: {img_path}")
                    failed.append({'product_id': pid, 'image_index': idx})
                    continue

                success = db.insert_image_vector(product_id=pid, image_path=img_path, image_index=idx, vector=features)
                if success:
                    rebuilt.append({'product_id': pid, 'image_index': idx})
                else:
                    failed.append({'product_id': pid, 'image_index': idx})
            except Exception as e:
                logger.error(f"é‡å»ºå‘é‡å‡ºé”™: {e}")
                failed.append({'product_id': pid, 'image_index': idx})

        return jsonify({'rebuilt': rebuilt, 'failed': failed, 'count': len(rebuilt)})
    except Exception as e:
        logger.error(f"é‡å»ºå‘é‡å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500



@app.route('/api/image/<int:product_id>/<int:image_index>', methods=['GET'])
def serve_product_image(product_id: int, image_index: int):
    """è¿”å›æŒ‡å®šå•†å“æŒ‡å®šåºå·çš„å›¾ç‰‡æ–‡ä»¶ï¼ˆç”¨äºå‰ç«¯ç¼©ç•¥å›¾/æŸ¥çœ‹ï¼‰"""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT image_path FROM product_images WHERE product_id = ? AND image_index = ?", (product_id, image_index))
            row = cursor.fetchone()
            if not row:
                return jsonify({'error': 'Image not found'}), 404
            image_path = row[0]

        # å®‰å…¨æ£€æŸ¥å¹¶è¿”å›æ–‡ä»¶
        from flask import send_file
        if not os.path.exists(image_path):
            return jsonify({'error': 'Image file missing'}), 404
        return send_file(image_path, mimetype='image/jpeg')
    except Exception as e:
        logger.error(f"serve_product_image å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/custom_reply_image/<int:product_id>/<filename>', methods=['GET'])
def serve_custom_reply_image(product_id: int, filename: str):
    """è¿”å›æŒ‡å®šå•†å“çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡æ–‡ä»¶"""
    try:
        # ä»æ•°æ®åº“è¯»å– uploaded_reply_images å­—æ®µï¼ŒéªŒè¯æ–‡ä»¶å
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT uploaded_reply_images FROM products WHERE id = ?", (product_id,))
            row = cursor.fetchone()
            if not row or not row[0]:
                return jsonify({'error': 'Product not found or no uploaded images'}), 404

            # è§£æ JSON æ•°ç»„
            try:
                uploaded_filenames = json.loads(row[0])
            except:
                return jsonify({'error': 'Invalid image data'}), 500

            # å®‰å…¨æ£€æŸ¥ï¼šéªŒè¯æ–‡ä»¶åæ˜¯å¦åœ¨åˆ—è¡¨ä¸­
            if filename not in uploaded_filenames:
                return jsonify({'error': 'Image not found'}), 404

        # æ„å»ºæ–‡ä»¶è·¯å¾„
        import os
        image_path = os.path.join('data', 'custom_reply_images', str(product_id), filename)

        # å®‰å…¨æ£€æŸ¥å¹¶è¿”å›æ–‡ä»¶
        from flask import send_file
        if not os.path.exists(image_path):
            return jsonify({'error': 'Image file missing'}), 404
        return send_file(image_path, mimetype='image/jpeg')
    except Exception as e:
        logger.error(f"serve_custom_reply_image å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

def verify_discord_token(token):
    """éªŒè¯Discord tokenå¹¶è·å–ç”¨æˆ·ä¿¡æ¯"""
    try:
        headers = {
            'Authorization': f'Bot {token}' if token.startswith('Bot ') else token,
            'User-Agent': 'DiscordBot/1.0'
        }

        # é¦–å…ˆå°è¯•ä½œä¸ºBot tokenéªŒè¯
        response = requests.get('https://discord.com/api/v10/users/@me', headers=headers, timeout=10)

        if response.status_code == 401:
            # å¦‚æœBot tokenå¤±è´¥ï¼Œå°è¯•ä½œä¸ºUser token
            if not token.startswith('Bot '):
                headers['Authorization'] = f'Bot {token}'
                response = requests.get('https://discord.com/api/v10/users/@me', headers=headers, timeout=10)

        if response.status_code == 200:
            user_data = response.json()
            return {
                'valid': True,
                'username': f"{user_data.get('username', 'Unknown')}#{user_data.get('discriminator', '0000')}",
                'user_id': user_data.get('id'),
                'avatar': user_data.get('avatar'),
                'bot': user_data.get('bot', False)
            }
        else:
            return {
                'valid': False,
                'error': f'HTTP {response.status_code}: {response.text}'
            }
    except requests.exceptions.RequestException as e:
        return {
            'valid': False,
            'error': f'ç½‘ç»œé”™è¯¯: {str(e)}'
        }
    except Exception as e:
        return {
            'valid': False,
            'error': f'éªŒè¯å¤±è´¥: {str(e)}'
        }

@app.route('/api/accounts', methods=['POST'])
def add_account():
    """æ·»åŠ æ–°çš„ Discord è´¦å·"""
    try:
        # è·å–å½“å‰ç™»å½•ç”¨æˆ·
        current_user = get_current_user()
        if not current_user:
            return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400
        token = data.get('token')
        username = data.get('username', '')

        if not token:
            return jsonify({'error': 'Token is required'}), 400

        # éªŒè¯tokenå¹¶è·å–çœŸå®ç”¨æˆ·å
        logger.info("æ­£åœ¨éªŒè¯Discord token...")
        token_info = verify_discord_token(token)

        if not token_info['valid']:
            return jsonify({'error': f'TokenéªŒè¯å¤±è´¥: {token_info["error"]} è¯·æ£€æŸ¥tokenæ˜¯å¦æ­£ç¡®'}), 400

        # å¦‚æœæ²¡æœ‰æä¾›ç”¨æˆ·åï¼Œä½¿ç”¨ä»tokenè·å–çš„ç”¨æˆ·å
        if not username:
            username = token_info['username']
            logger.info(f"è‡ªåŠ¨è·å–ç”¨æˆ·å: {username}")

        with db.get_connection() as conn:
            cursor = conn.cursor()

            # é¦–å…ˆæ£€æŸ¥tokenæ˜¯å¦å·²å­˜åœ¨
            cursor.execute("SELECT id, username, user_id FROM discord_accounts WHERE token = ?", (token,))
            existing_account = cursor.fetchone()

            if existing_account:
                # å¦‚æœtokenå·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦å±äºå½“å‰ç”¨æˆ·
                if existing_account[2] == current_user['id']:
                    # å±äºå½“å‰ç”¨æˆ·ï¼Œæ›´æ–°ä¿¡æ¯
                    cursor.execute("""
                        UPDATE discord_accounts
                        SET username = ?, status = 'offline', updated_at = CURRENT_TIMESTAMP
                        WHERE token = ?
                    """, (username, token))
                    account_id = existing_account[0]
                    logger.info(f"æ›´æ–°ç°æœ‰è´¦å·: {username} (ç”¨æˆ·ID: {current_user['id']})")
                else:
                    # å±äºå…¶ä»–ç”¨æˆ·ï¼Œè¿”å›é”™è¯¯
                    return jsonify({'error': 'æ­¤Discord tokenå·²è¢«å…¶ä»–ç”¨æˆ·ä½¿ç”¨'}), 400
            else:
                # tokenä¸å­˜åœ¨ï¼Œæ’å…¥æ–°è®°å½•
                cursor.execute("""
                    INSERT INTO discord_accounts (username, token, status, user_id)
                    VALUES (?, ?, 'offline', ?)
                """, (username, token, current_user['id']))
                account_id = cursor.lastrowid
                logger.info(f"æ·»åŠ æ–°è´¦å·: {username} (ç”¨æˆ·ID: {current_user['id']})")

            # è·å–è´¦å·ä¿¡æ¯
            cursor.execute("SELECT id, username, token, status, last_active, user_id FROM discord_accounts WHERE id = ?", (account_id,))
            account = cursor.fetchone()
            conn.commit()

        logger.info(f"è´¦å·æ·»åŠ æˆåŠŸ: {username} (ç”¨æˆ·ID: {current_user['id']})")
        return jsonify({
            'id': account[0],
            'username': account[1],
            'token': account[2],
            'status': account[3],
            'lastActive': account[4],
            'user_id': account[5],
            'verified': True
        })
    except Exception as e:
        logger.error(f"æ·»åŠ è´¦å·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/<int:account_id>/user', methods=['PUT'])
def assign_account_to_user(account_id):
    """å°†Discordè´¦å·åˆ†é…ç»™ç”¨æˆ·ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        data = request.get_json()
        user_id = data.get('user_id')

        if db.update_discord_account_user(account_id, user_id):
            return jsonify({'message': 'è´¦å·åˆ†é…æˆåŠŸ'})
        else:
            return jsonify({'error': 'è´¦å·åˆ†é…å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ†é…è´¦å·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/<int:account_id>', methods=['DELETE'])
def delete_account(account_id):
    """åˆ é™¤ Discord è´¦å·"""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("DELETE FROM discord_accounts WHERE id = ?", (account_id,))
            conn.commit()

        return jsonify({'success': True})
    except Exception as e:
        logger.error(f"åˆ é™¤è´¦å·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/<int:account_id>/status', methods=['PUT'])
def update_account_status(account_id):
    """æ›´æ–°è´¦å·çŠ¶æ€"""
    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400
        status = data.get('status')

        if status not in ['online', 'offline']:
            return jsonify({'error': 'Invalid status'}), 400

        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE discord_accounts
                SET status = ?, updated_at = datetime('now')
                WHERE id = ?
            """, (status, account_id))
            conn.commit()

        return jsonify({'success': True, 'status': status})
    except Exception as e:
        logger.error(f"æ›´æ–°è´¦å·çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/current', methods=['GET'])
def get_current_account():
    """è·å–å½“å‰å¯ç”¨çš„ Discord è´¦å· (çŠ¶æ€ä¸ºonlineçš„ç¬¬ä¸€ä¸ª)"""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT id, username, token, status, last_active
                FROM discord_accounts
                WHERE status = 'online'
                ORDER BY last_active DESC NULLS LAST, created_at ASC
                LIMIT 1
            """)
            account = cursor.fetchone()

            if account:
                return jsonify({
                    'id': account[0],
                    'username': account[1],
                    'token': account[2],
                    'status': account[3],
                    'lastActive': account[4]
                })
            else:
                return jsonify({'error': 'No active account found'}), 404
    except Exception as e:
        logger.error(f"è·å–å½“å‰è´¦å·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/verify-all', methods=['POST'])
def verify_all_accounts():
    """é‡æ–°éªŒè¯æ‰€æœ‰è´¦å·"""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            # è·å–æ‰€æœ‰è´¦å·
            cursor.execute("SELECT id, username, token FROM discord_accounts")
            accounts = cursor.fetchall()

            verified_count = 0
            invalid_count = 0
            results = []

            for account in accounts:
                account_id, username, token = account
                logger.info(f"æ­£åœ¨éªŒè¯è´¦å·: {username}")

                token_info = verify_discord_token(token)

                if token_info['valid']:
                    # æ›´æ–°ç”¨æˆ·åï¼ˆå¦‚æœæœ‰å˜åŒ–ï¼‰
                    new_username = token_info['username']
                    if new_username != username:
                        cursor.execute("""
                            UPDATE discord_accounts
                            SET username = ?
                            WHERE id = ?
                        """, (new_username, account_id))
                        logger.info(f"ç”¨æˆ·åå·²æ›´æ–°: {username} -> {new_username}")

                    verified_count += 1
                    results.append({
                        'id': account_id,
                        'username': new_username,
                        'valid': True
                    })
                else:
                    invalid_count += 1
                    results.append({
                        'id': account_id,
                        'username': username,
                        'valid': False,
                        'error': token_info['error']
                    })

            conn.commit()

        return jsonify({
            'success': True,
            'total': len(accounts),
            'verified': verified_count,
            'invalid': invalid_count,
            'results': results
        })
    except Exception as e:
        logger.error(f"æ‰¹é‡éªŒè¯è´¦å·å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/bulk-status', methods=['POST'])
def bulk_update_status():
    """æ‰¹é‡å¼€å¯æˆ–åœæ­¢ç”¨æˆ·è‡ªå·±çš„è´¦å·"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400

        new_status = data.get('status')
        if new_status not in ['online', 'offline']:
            return jsonify({'error': 'Invalid status. Must be "online" or "offline"'}), 400

        current_user = get_current_user()

        with db.get_connection() as conn:
            cursor = conn.cursor()

            if new_status == 'online':
                cursor.execute("""
                    UPDATE discord_accounts
                    SET status = 'online', last_active = ?
                    WHERE user_id = ?
                """, (datetime.now(), current_user['id']))
            else:
                cursor.execute("""
                    UPDATE discord_accounts
                    SET status = 'offline'
                    WHERE user_id = ?
                """, (current_user['id'],))

            updated_count = cursor.rowcount
            conn.commit()

        logger.info(f"æ‰¹é‡æ›´æ–°è´¦å·çŠ¶æ€: {updated_count} ä¸ªè´¦å·è®¾ç½®ä¸º {new_status}")

        return jsonify({
            'success': True,
            'updated_count': updated_count,
            'new_status': new_status
        })
    except Exception as e:
        logger.error(f"æ‰¹é‡æ›´æ–°è´¦å·çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/accounts/rotation', methods=['GET'])
def get_rotation_config():
    """è·å–è´¦å·è½®æ¢é…ç½®"""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT enabled, rotation_interval, current_account_id
                FROM account_rotation_config
                LIMIT 1
            """)
            row = cursor.fetchone()

        if row:
            return jsonify({
                'enabled': row[0],
                'rotationInterval': row[1],
                'currentAccountId': row[2]
            })
        return jsonify({'enabled': False, 'rotationInterval': 10})
    except Exception as e:
        logger.error(f"è·å–è½®æ¢é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/user/settings', methods=['GET'])
def get_user_settings():
    """è·å–å½“å‰ç”¨æˆ·çš„ä¸ªæ€§åŒ–è®¾ç½®"""
    user = get_current_user()
    if not user:
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        settings = db.get_user_settings(user['id'])
        return jsonify(settings)
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·è®¾ç½®å¤±è´¥: {e}")
        return jsonify({'error': 'è·å–è®¾ç½®å¤±è´¥'}), 500

@app.route('/api/user/settings', methods=['PUT'])
def update_user_settings():
    """æ›´æ–°å½“å‰ç”¨æˆ·çš„ä¸ªæ€§åŒ–è®¾ç½®"""
    user = get_current_user()
    if not user:
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'Invalid request body'}), 400

        # è°ƒè¯•å»¶è¿Ÿè®¾ç½®
        min_delay = data.get('global_reply_min_delay')
        max_delay = data.get('global_reply_max_delay')
        logger.info(f"ç”¨æˆ·è®¾ç½®å»¶è¿Ÿ - æœ€å°: {min_delay}, æœ€å¤§: {max_delay}")

        # å¤„ç†å¼€å…³è®¾ç½®ï¼ˆboolean è½¬ integerï¼‰
        keyword_reply = data.get('keyword_reply_enabled')
        image_reply = data.get('image_reply_enabled')
        if keyword_reply is not None:
            keyword_reply = 1 if keyword_reply else 0
        if image_reply is not None:
            image_reply = 1 if image_reply else 0

        success = db.update_user_settings(
            user_id=user['id'],
            download_threads=data.get('download_threads'),
            feature_extract_threads=data.get('feature_extract_threads'),
            discord_similarity_threshold=data.get('discord_similarity_threshold'),
            global_reply_min_delay=min_delay,
            global_reply_max_delay=max_delay,
            user_blacklist=data.get('user_blacklist'),
            keyword_filters=data.get('keyword_filters'),
            keyword_reply_enabled=keyword_reply,
            image_reply_enabled=image_reply,
            global_reply_template=data.get('global_reply_template'),
            numeric_filter_keyword=data.get('numeric_filter_keyword'),
            filter_size_min=data.get('filter_size_min'),
            filter_size_max=data.get('filter_size_max')
        )

        if success:
            return jsonify({'message': 'è®¾ç½®æ›´æ–°æˆåŠŸ'})
        else:
            return jsonify({'error': 'è®¾ç½®æ›´æ–°å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°ç”¨æˆ·è®¾ç½®å¤±è´¥: {e}")
        return jsonify({'error': 'æ›´æ–°è®¾ç½®å¤±è´¥'}), 500

@app.route('/api/accounts/rotation', methods=['POST'])
def update_rotation_config():
    """æ›´æ–°è´¦å·è½®æ¢é…ç½®"""
    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400
        enabled = data.get('enabled', False)
        rotation_interval = data.get('rotationInterval', 10)

        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE account_rotation_config
                SET enabled = ?, rotation_interval = ?, updated_at = datetime('now')
                WHERE id = 1
            """, (enabled, rotation_interval))
            conn.commit()

        return jsonify({'success': True, 'enabled': enabled, 'rotationInterval': rotation_interval})
    except Exception as e:
        logger.error(f"æ›´æ–°è½®æ¢é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/get_indexed_ids', methods=['GET'])
def get_indexed_ids():
    """è·å–å·²å»ºç«‹ç´¢å¼•çš„å•†å“URLåˆ—è¡¨"""
    try:
        indexed_urls = db.get_indexed_product_urls()
        return jsonify({'indexedIds': indexed_urls})
    except Exception as e:
        logger.error(f"è·å–å·²ç´¢å¼•IDå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === ä¿®å¤ï¼šæ‰¹é‡åˆ é™¤ API ===
@app.route('/api/products/batch', methods=['DELETE'])
def batch_delete_products():
    """æ‰¹é‡åˆ é™¤å•†å“ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    try:
        data = request.get_json()
        ids = data.get('ids', [])
        if not ids:
            return jsonify({'error': 'No IDs provided'}), 400

        logger.info(f"å¼€å§‹æ‰¹é‡åˆ é™¤ {len(ids)} ä¸ªå•†å“")

        result = db.delete_products_bulk(ids)
        deleted_count = result.get('deleted_count', 0)
        failed_ids = result.get('missing_ids', [])
        file_failed_count = result.get('file_failed_count', 0)

        logger.info(f"æ‰¹é‡åˆ é™¤å®Œæˆ: {deleted_count}/{len(ids)} ä¸ªå•†å“æˆåŠŸåˆ é™¤")

        response = {'success': True, 'count': deleted_count, 'total': len(ids)}
        warnings = []
        if failed_ids:
            response['failed_ids'] = failed_ids
            warnings.append(f'{len(failed_ids)} ä¸ªå•†å“ä¸å­˜åœ¨')
        if file_failed_count:
            warnings.append(f'{file_failed_count} ä¸ªå›¾ç‰‡æ–‡ä»¶åˆ é™¤å¤±è´¥')
        if warnings:
            response['warning'] = 'ï¼Œ'.join(warnings)

        return jsonify(response)
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/products/batch-delete-all', methods=['DELETE'])
def batch_delete_all_products():
    """åˆ é™¤æ‰€æœ‰å•†å“ï¼ˆå…¨é€‰åˆ é™¤ï¼‰"""
    try:
        if not require_login():
            return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

        current_user = get_current_user()
        data = request.get_json(silent=True) or {}
        keyword = (request.args.get('keyword') or data.get('keyword') or '').strip()
        search_type = request.args.get('search_type') or data.get('search_type') or 'all'
        shop_name = (request.args.get('shop_name') or data.get('shop_name') or '').strip() or None

        if current_user['role'] == 'admin':
            all_ids = db.get_product_ids_by_user_shops(
                None,
                keyword=keyword,
                search_type=search_type,
                shop_name=shop_name
            )
        else:
            user_shops = current_user.get('shops', [])
            all_ids = db.get_product_ids_by_user_shops(
                user_shops,
                keyword=keyword,
                search_type=search_type,
                shop_name=shop_name
            )

        if not all_ids:
            try:
                with db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM product_images")
                    remaining_images = cursor.fetchone()[0] or 0

                if remaining_images == 0:
                    try:
                        from vector_engine import get_vector_engine
                    except ImportError:
                        from .vector_engine import get_vector_engine
                    engine = get_vector_engine()
                    if engine.count() > 0:
                        engine.rebuild_index([])
            except Exception as e:
                logger.warning(f"æ¸…ç†ç©ºç´¢å¼•å¤±è´¥: {e}")

            return jsonify({'success': True, 'count': 0, 'message': 'æ²¡æœ‰å•†å“éœ€è¦åˆ é™¤'})

        logger.info(f"å¼€å§‹åˆ é™¤æ‰€æœ‰ {len(all_ids)} ä¸ªå•†å“")

        result = db.delete_products_bulk(all_ids)
        deleted_count = result.get('deleted_count', 0)
        failed_ids = result.get('missing_ids', [])
        file_failed_count = result.get('file_failed_count', 0)

        logger.info(f"å…¨é€‰åˆ é™¤å®Œæˆ: {deleted_count}/{len(all_ids)} ä¸ªå•†å“æˆåŠŸåˆ é™¤")

        response = {
            'success': True,
            'count': deleted_count,
            'total': len(all_ids),
            'message': f'æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªå•†å“'
        }

        warnings = []
        if failed_ids:
            response['failed_ids'] = failed_ids
            warnings.append(f'{len(failed_ids)} ä¸ªå•†å“ä¸å­˜åœ¨')
        if file_failed_count:
            warnings.append(f'{file_failed_count} ä¸ªå›¾ç‰‡æ–‡ä»¶åˆ é™¤å¤±è´¥')
        if warnings:
            response['warning'] = 'ï¼Œ'.join(warnings)

        return jsonify(response)
    except Exception as e:
        logger.error(f"å…¨é€‰åˆ é™¤å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/products/<int:product_id>', methods=['GET'])
def get_product(product_id):
    """è·å–å•ä¸ªå•†å“çš„è¯¦ç»†ä¿¡æ¯"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        product = db._get_product_info_by_id(product_id)
        if not product:
            return jsonify({'error': 'å•†å“ä¸å­˜åœ¨'}), 404

        # è·å–å•†å“å›¾ç‰‡
        images = db.get_product_images(product_id)
        product['images'] = [f"/api/image/{product_id}/{img['image_index']}" for img in images]

        return jsonify(product)
    except Exception as e:
        logger.error(f"è·å–å•†å“å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/products/<int:product_id>', methods=['DELETE'])
def delete_product(product_id):
    """åˆ é™¤å•†å“åŠå…¶æ‰€æœ‰ç›¸å…³æ•°æ®"""
    try:
        # åˆ é™¤å•†å“åŠå…¶å‘é‡æ•°æ®
        if db.delete_product_images(product_id):
            return jsonify({'success': True, 'message': f'å•†å“ {product_id} å·²åˆ é™¤'})
        else:
            return jsonify({'error': 'åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"åˆ é™¤å•†å“å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === ä¿®å¤ï¼šå•†å“å›¾ç‰‡ä¸Šä¼  API ===
@app.route('/api/products/<int:product_id>/images', methods=['POST'])
def upload_product_image(product_id):
    """ä¸Šä¼ æ–°å›¾ç‰‡åˆ°å•†å“ï¼ˆè°ƒç”¨å®Œæ•´çš„æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼‰"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    file = request.files.get('image')
    if not file:
        return jsonify({'error': 'æ— æ–‡ä»¶'}), 400

    try:
        # è·å–ç°æœ‰ç‰¹å¾ç”¨äºæŸ¥é‡
        existing_images = db.get_product_images(product_id)
        existing_feats = [img['features'] for img in existing_images if img.get('features') is not None]

        # è·å–ä¸‹ä¸€ä¸ª index
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT MAX(image_index) FROM product_images WHERE product_id = ?", (product_id,))
            row = cursor.fetchone()
            next_index = (row[0] + 1) if row and row[0] is not None else 0

        # è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°ï¼ˆç°åœ¨åŒ…å«å®Œæ•´çš„æ•°æ®åº“å’ŒFAISSæ“ä½œï¼‰
        result = process_and_save_image_core(product_id, file, next_index, existing_feats)

        if not result['success']:
            return jsonify({'error': result['error']}), 400

        # è¿”å›æ›´æ–°åçš„å•†å“ä¿¡æ¯
        product = db._get_product_info_by_id(product_id)

        # è·å–æ‰€æœ‰å›¾ç‰‡
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT image_index FROM product_images WHERE product_id = ? ORDER BY image_index", (product_id,))
            images = [f"/api/image/{product_id}/{row[0]}" for row in cursor.fetchall()]

        product['images'] = images

        # æ ¼å¼åŒ–ä»¥åŒ¹é…å‰ç«¯
        product['weidianId'] = product.get('product_url', '').split('itemID=')[1] if 'itemID=' in product.get('product_url', '') else ''
        product['weidianUrl'] = product.get('product_url')
        product['englishTitle'] = product.get('english_title')
        product['cnfansUrl'] = product.get('cnfans_url')
        product['ruleEnabled'] = product.get('ruleEnabled')
        product['matchType'] = 'fuzzy'

        return jsonify({'success': True, 'product': product})

    except Exception as e:
        logger.error(f"ä¸Šä¼ å›¾ç‰‡å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

# === ä¿®å¤ï¼šåˆ é™¤å›¾ç‰‡åè¿”å›æœ€æ–° Product å¯¹è±¡ ===
@app.route('/api/products/<int:product_id>/images/<int:image_index>', methods=['DELETE'])
def delete_product_image(product_id, image_index):
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        logger.info(f"å¼€å§‹åˆ é™¤å›¾ç‰‡: product_id={product_id}, image_index={image_index}")


        # éªŒè¯å‚æ•°
        try:
            product_id = int(product_id)
            image_index = int(image_index)
        except ValueError:
            return jsonify({'error': 'å‚æ•°æ ¼å¼é”™è¯¯'}), 400

        # è°ƒç”¨æ•°æ®åº“åˆ é™¤é€»è¾‘
        success = db.delete_image_vector(product_id, image_index)

        if not success:
            logger.warning(f"åˆ é™¤å›¾ç‰‡å¤±è´¥: product_id={product_id}, image_index={image_index}")
            return jsonify({'error': 'åˆ é™¤å¤±è´¥ï¼Œå›¾ç‰‡å¯èƒ½ä¸å­˜åœ¨'}), 404

        # è·å–æœ€æ–°å•†å“ä¿¡æ¯
        product = db._get_product_info_by_id(product_id)

        if not product:
            logger.error(f"åˆ é™¤åå•†å“ä¸å­˜åœ¨: product_id={product_id}")
            return jsonify({'error': 'å•†å“ä¸å­˜åœ¨'}), 404

        # è·å–å‰©ä½™æ‰€æœ‰å›¾ç‰‡
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT image_index FROM product_images WHERE product_id = ? ORDER BY image_index", (product_id,))
            image_indices = [row[0] for row in cursor.fetchall()]
            images = [f"/api/image/{product_id}/{idx}" for idx in image_indices]

        product['images'] = images

        # æ ¼å¼åŒ–å•†å“ä¿¡æ¯
        try:
            if 'itemID=' in product.get('product_url', ''):
                product['weidianId'] = product.get('product_url', '').split('itemID=')[1]
            else:
                product['weidianId'] = ''
        except:
            product['weidianId'] = ''

        product['weidianUrl'] = product.get('product_url')
        product['englishTitle'] = product.get('english_title')
        product['cnfansUrl'] = product.get('cnfans_url')
        product['acbuyUrl'] = product.get('acbuy_url')
        product['ruleEnabled'] = product.get('ruleEnabled')

        logger.info(f"åˆ é™¤å›¾ç‰‡æˆåŠŸ: product_id={product_id}, image_index={image_index}, å‰©ä½™å›¾ç‰‡æ•°é‡={len(images)}")

        return jsonify({'success': True, 'product': product})

    except Exception as e:
        logger.error(f"åˆ é™¤å›¾ç‰‡å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/cleanup/images', methods=['POST'])
def cleanup_images():
    """æ¸…ç†æœªä½¿ç”¨çš„å›¾ç‰‡æ–‡ä»¶"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        if current_user['role'] != 'admin':
            return jsonify({'error': 'åªæœ‰ç®¡ç†å‘˜å¯ä»¥æ‰§è¡Œæ¸…ç†æ“ä½œ'}), 403

        # è·å–æ¸…ç†å‚æ•°
        data = request.get_json() or {}
        days_old = data.get('days_old', 30)

        # æ‰§è¡Œæ¸…ç†
        deleted_count = db.cleanup_unused_images(days_old)

        return jsonify({
            'success': True,
            'message': f'æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {deleted_count} ä¸ªæœªä½¿ç”¨çš„å›¾ç‰‡æ–‡ä»¶',
            'deleted_count': deleted_count
        })

    except Exception as e:
        logger.error(f"å›¾ç‰‡æ¸…ç†å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/system/ai-status', methods=['GET'])
def get_ai_status():
    """è·å–AIç³»ç»Ÿå®Œæ•´çŠ¶æ€å’Œè¯Šæ–­ä¿¡æ¯"""
    try:
        extractor = get_global_feature_extractor()
        if extractor is None:
            return {'error': 'ç‰¹å¾æå–å™¨æœªåˆå§‹åŒ–'}
        ai_status = extractor.get_status()

        # è·å–FAISSçŠ¶æ€
        try:
            from vector_engine import get_vector_engine
        except ImportError:
            from .vector_engine import get_vector_engine
        faiss_engine = get_vector_engine()
        faiss_status = faiss_engine.get_stats()

        # ç»¼åˆçŠ¶æ€
        overall_status = {
            'ai_model_status': ai_status,
            'vector_engine_status': faiss_status,
            'system_health': 'è‰¯å¥½' if ai_status['yolo_available'] and faiss_status['total_vectors'] >= 0 else 'éœ€è¦ä¼˜åŒ–',
            'recommendations': []
        }

        # ç”Ÿæˆå»ºè®®
        recommendations = []
        recommendations.extend(ai_status.get('performance_tips', []))
        recommendations.extend(faiss_status.get('performance_tips', []))

        # é¢å¤–çš„ç³»ç»Ÿçº§å»ºè®®
        if not ai_status['yolo_available']:
            recommendations.append("YOLOè£å‰ªåŠŸèƒ½å·²ç¦ç”¨ï¼Œå›¾åƒè¯†åˆ«å‡†ç¡®ç‡ä¼šé™ä½")
        if faiss_status['total_vectors'] == 0:
            recommendations.append("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œå»ºè®®æ·»åŠ å•†å“æ•°æ®")
        if faiss_status['ef_construction'] == 'ä¸æ”¯æŒ' or faiss_status['ef_search'] == 'ä¸æ”¯æŒ':
            recommendations.append("FAISSç‰ˆæœ¬è¾ƒæ—§ï¼Œå»ºè®®å‡çº§ä»¥è·å¾—æœ€ä½³æœç´¢æ€§èƒ½")

        overall_status['recommendations'] = recommendations[:5]  # æœ€å¤šæ˜¾ç¤º5æ¡å»ºè®®

        return jsonify(overall_status)
    except Exception as e:
        logger.error(f"è·å–AIçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/system/rebuild-index', methods=['POST'])
def rebuild_faiss_index():
    """é‡å»ºFAISSç´¢å¼•ï¼Œæ¸…ç†å·²åˆ é™¤çš„å‘é‡"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()
        if current_user['role'] != 'admin':
            return jsonify({'error': 'åªæœ‰ç®¡ç†å‘˜å¯ä»¥é‡å»ºç´¢å¼•'}), 403

        try:
            from vector_engine import get_vector_engine
        except ImportError:
            from .vector_engine import get_vector_engine
        engine = get_vector_engine()

        # è·å–æ‰€æœ‰æœ‰æ•ˆçš„å›¾ç‰‡æ•°æ®
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id, image_path FROM product_images WHERE id IS NOT NULL")
            all_images = cursor.fetchall()

        # é‡æ–°æå–æ‰€æœ‰ç‰¹å¾
        valid_vectors = []
        for row in all_images:
            try:
                extractor = get_global_feature_extractor()
                if extractor is None:
                    logger.error("ç‰¹å¾æå–å™¨æœªåˆå§‹åŒ–")
                    continue
                features = extractor.extract_feature(row['image_path'])
                if features is not None:
                    valid_vectors.append((row['id'], features))
            except Exception as e:
                logger.warning(f"é‡æ–°æå–ç‰¹å¾å¤±è´¥ {row['image_path']}: {e}")

        # é‡å»ºç´¢å¼•
        engine.rebuild_index(valid_vectors)

        return jsonify({
            'success': True,
            'message': f'ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŒ…å« {len(valid_vectors)} ä¸ªå‘é‡',
            'total_vectors': len(valid_vectors)
        })

    except Exception as e:
        logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/config', methods=['GET'])
def get_config():
    """è·å–ç³»ç»Ÿé…ç½®ä¿¡æ¯"""
    try:
        from config import config
        return jsonify({
            'version': '1.0.0',
            'features': {
                'multithread_scraping': True,
                'ai_image_processing': True,
                'discord_bot': True,
                'real_time_monitoring': True
            },
            'limits': {
                'max_scrape_threads': config.SCRAPE_THREADS,
                'max_download_threads': config.DOWNLOAD_THREADS,
                'max_feature_threads': config.FEATURE_EXTRACT_THREADS
            }
        })
    except Exception as e:
        logger.error(f"è·å–é…ç½®ä¿¡æ¯å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/config/discord-threshold', methods=['GET'])
def get_discord_threshold():
    """è·å–Discordç›¸ä¼¼åº¦é˜ˆå€¼"""
    try:
        sys_config = db.get_system_config()
        threshold = sys_config['discord_similarity_threshold']
        return jsonify({
            'threshold': threshold,
            'threshold_percentage': threshold * 100
        })
    except Exception as e:
        logger.error(f"è·å–Discordé˜ˆå€¼å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/config/discord-threshold', methods=['POST'])
def update_discord_threshold():
    """æ›´æ–°Discordç›¸ä¼¼åº¦é˜ˆå€¼"""
    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400
        threshold = float(data.get('threshold', 0.4))

        # éªŒè¯èŒƒå›´
        if not (0.0 <= threshold <= 1.0):
            return jsonify({'error': 'é˜ˆå€¼å¿…é¡»åœ¨0.0-1.0ä¹‹é—´'}), 400

        # ä¿å­˜åˆ°æ•°æ®åº“
        if db.update_system_config(discord_similarity_threshold=threshold):
            # åŒæ—¶æ›´æ–°å†…å­˜ä¸­çš„é…ç½®

            return jsonify({
                'message': f'Discordç›¸ä¼¼åº¦é˜ˆå€¼å·²æ›´æ–°ä¸º {threshold}',
                'threshold': threshold
            })

        return jsonify({'error': 'æ›´æ–°é…ç½®å¤±è´¥'}), 500
    except ValueError as e:
        return jsonify({'error': 'é˜ˆå€¼å¿…é¡»æ˜¯æ•°å­—'}), 400
    except Exception as e:
        logger.error(f"æ›´æ–°Discordé˜ˆå€¼å¤±è´¥: {e}")
        return jsonify({'error': 'æ›´æ–°é…ç½®å¤±è´¥'}), 500

@app.route('/api/config/scrape-threads', methods=['GET', 'POST'])
def config_scrape_threads():
    """é…ç½®æŠ“å–å¤šçº¿ç¨‹æ•°é‡"""
    if request.method == 'GET':
        config = db.get_system_config()
        return jsonify({
            'scrape_threads': config.get('scrape_threads', 2)
        })

    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400

        scrape_threads = int(data.get('scrape_threads', 2))

        # ç¡®ä¿çº¿ç¨‹æ•°åœ¨åˆç†èŒƒå›´å†…
        if scrape_threads < 1 or scrape_threads > 10:
            return jsonify({'error': 'æŠ“å–çº¿ç¨‹æ•°å¿…é¡»æ˜¯1-10ä¹‹é—´çš„æ•´æ•°'}), 400

        # ä¿å­˜åˆ°æ•°æ®åº“
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute('UPDATE system_config SET scrape_threads = ?, updated_at = CURRENT_TIMESTAMP WHERE id = 1',
                          (scrape_threads,))
            conn.commit()

        return jsonify({
            'message': f'æŠ“å–çº¿ç¨‹æ•°å·²è®¾ç½®ä¸º {scrape_threads}',
            'scrape_threads': scrape_threads
        })

    except ValueError as e:
        return jsonify({'error': 'çº¿ç¨‹æ•°å¿…é¡»æ˜¯æ•´æ•°'}), 400
    except Exception as e:
        logger.error(f"æ›´æ–°æŠ“å–çº¿ç¨‹é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': 'æ›´æ–°é…ç½®å¤±è´¥'}), 500

@app.route('/api/config/global-reply-delay', methods=['GET'])
def get_global_reply_delay():
    """è·å–å…¨å±€å›å¤å»¶è¿Ÿé…ç½®"""
    try:
        delay_config = db.get_global_reply_config()
        return jsonify({
            'min_delay': delay_config['min_delay'],
            'max_delay': delay_config['max_delay'],
            'description': f'{delay_config["min_delay"]}-{delay_config["max_delay"]}ç§’éšæœºå»¶è¿Ÿ'
        })
    except Exception as e:
        logger.error(f"è·å–å…¨å±€å›å¤å»¶è¿Ÿå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/faiss_status', methods=['GET'])
def get_faiss_status():
    """è·å–FAISSå‘é‡æ•°æ®åº“çŠ¶æ€"""
    try:
        try:
            from vector_engine import get_vector_engine
        except ImportError:
            from .vector_engine import get_vector_engine
        engine = get_vector_engine()
        stats = engine.get_stats()

        # å°è¯•æœç´¢ä¸€ä¸ªæµ‹è¯•å‘é‡
        test_vector = np.zeros(config.VECTOR_DIMENSION, dtype='float32')
        test_results = engine.search(test_vector, top_k=1)

        return jsonify({
            'index_exists': True,
            'entity_count': stats['total_vectors'],
            'test_search_works': len(test_results) > 0,
            'vector_dimension': config.VECTOR_DIMENSION,
            'index_type': stats['index_type'],
            'metric_type': stats['metric_type'],
            'memory_usage_mb': stats['memory_usage_mb'],
            'ef_construction': stats['ef_construction'],
            'ef_search': stats['ef_search']
        })
    except Exception as e:
        logger.error(f"è·å–FAISSçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            'error': str(e),
            'index_exists': False,
            'entity_count': 0
        }), 500


@app.route('/api/config/global-reply-delay', methods=['POST'])
def update_global_reply_delay():
    """æ›´æ–°å…¨å±€å›å¤å»¶è¿Ÿé…ç½®"""
    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400
        min_delay = float(data.get('min_delay', 3))
        max_delay = float(data.get('max_delay', 8))

        # éªŒè¯èŒƒå›´
        if min_delay < 0 or max_delay < 0:
            return jsonify({'error': 'å»¶è¿Ÿæ—¶é—´ä¸èƒ½ä¸ºè´Ÿæ•°'}), 400
        if min_delay > max_delay:
            return jsonify({'error': 'æœ€å°å»¶è¿Ÿä¸èƒ½å¤§äºæœ€å¤§å»¶è¿Ÿ'}), 400
        if max_delay > 300:
            return jsonify({'error': 'æœ€å¤§å»¶è¿Ÿä¸èƒ½è¶…è¿‡300ç§’'}), 400

        # ä¿å­˜åˆ°æ•°æ®åº“
        if db.update_global_reply_config(min_delay, max_delay):
            # åŒæ—¶æ›´æ–°å†…å­˜ä¸­çš„é…ç½®
            config.GLOBAL_REPLY_MIN_DELAY = min_delay
            config.GLOBAL_REPLY_MAX_DELAY = max_delay

            logger.info(f"å…¨å±€å›å¤å»¶è¿Ÿè®¾ç½®ä¸º: {min_delay}-{max_delay}ç§’")

            return jsonify({
                'success': True,
                'min_delay': min_delay,
                'max_delay': max_delay,
                'description': f'{min_delay}-{max_delay}ç§’éšæœºå»¶è¿Ÿ',
                'message': 'å…¨å±€å›å¤å»¶è¿Ÿè®¾ç½®å·²æ›´æ–°ï¼Œæ‰€æœ‰è‡ªåŠ¨å›å¤å°†ä½¿ç”¨æ­¤è®¾ç½®'
            })
        else:
            return jsonify({'error': 'ä¿å­˜å¤±è´¥'}), 500

    except Exception as e:
        logger.error(f"æ›´æ–°å…¨å±€å›å¤å»¶è¿Ÿå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/config/discord-channel', methods=['GET'])
def get_discord_channel():
    """è·å–Discordé¢‘é“é…ç½®"""
    try:
        sys_config = db.get_system_config()
        return jsonify({
            'channel_id': sys_config['discord_channel_id'],
            'cnfans_channel_id': sys_config['cnfans_channel_id'],
            'acbuy_channel_id': sys_config['acbuy_channel_id']
        })
    except Exception as e:
        logger.error(f"è·å–Discordé¢‘é“é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/config/discord-channel', methods=['POST'])
def update_discord_channel():
    """æ›´æ–°Discordé¢‘é“é…ç½®"""
    try:
        data = request.get_json()
        if data is None:
            return jsonify({'error': 'Invalid request body'}), 400

        channel_id = data.get('channel_id', '').strip()
        cnfans_channel_id = data.get('cnfans_channel_id', '').strip()
        acbuy_channel_id = data.get('acbuy_channel_id', '').strip()

        # éªŒè¯é¢‘é“IDæ ¼å¼ï¼ˆåº”è¯¥æ˜¯æ•°å­—ï¼‰
        for cid_name, cid_value in [('channel_id', channel_id), ('cnfans_channel_id', cnfans_channel_id), ('acbuy_channel_id', acbuy_channel_id)]:
            if cid_value and not cid_value.isdigit():
                return jsonify({'error': f'{cid_name} å¿…é¡»æ˜¯æ•°å­—'}), 400

        # ä¿å­˜åˆ°æ•°æ®åº“
        if db.update_system_config(
            discord_channel_id=channel_id,
            cnfans_channel_id=cnfans_channel_id,
            acbuy_channel_id=acbuy_channel_id
        ):
            # åŒæ—¶æ›´æ–°ç¯å¢ƒå˜é‡å’Œbot_config
            if channel_id:
                os.environ['DISCORD_CHANNEL_ID'] = channel_id
                import bot_config
                bot_config.config.DISCORD_CHANNEL_ID = int(channel_id)
                logger.info(f"Discordé¢‘é“IDè®¾ç½®ä¸º: {channel_id}")
            else:
                os.environ.pop('DISCORD_CHANNEL_ID', None)
                import bot_config
                bot_config.config.DISCORD_CHANNEL_ID = 0
                logger.info("Discordé¢‘é“IDå·²æ¸…é™¤")

            return jsonify({
                'success': True,
                'channel_id': channel_id,
                'message': f'Discordé¢‘é“IDå·²è®¾ç½®ä¸º: {channel_id or "æ— (ç›‘å¬æ‰€æœ‰é¢‘é“)"}'
            })
        else:
            return jsonify({'error': 'ä¿å­˜å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ›´æ–°Discordé¢‘é“é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/search_history', methods=['GET'])
def get_search_history():
    """è·å–æœç´¢å†å²è®°å½•ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
    try:
        limit = min(int(request.args.get('limit', 20)), 100)  # æœ€å¤š100æ¡
        offset = max(int(request.args.get('offset', 0)), 0)
        page = max(int(request.args.get('page', 1)), 1)

        # å¦‚æœæä¾›äº†pageå‚æ•°ï¼Œè®¡ç®—offset
        if 'page' in request.args and 'offset' not in request.args:
            offset = (page - 1) * limit

        result = db.get_search_history(limit, offset)
        return jsonify(result)
    except Exception as e:
        logger.error(f"è·å–æœç´¢å†å²å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/search_similar_text', methods=['POST'])
def search_similar_text():
    """æ ¹æ®æ–‡å­—å…³é”®è¯æœç´¢ç›¸ä¼¼å•†å“"""
    try:
        import re
        data = request.get_json()
        if not data:
            return jsonify({'error': 'Invalid request body'}), 400

        query = data.get('query', '').strip()
        limit = min(int(data.get('limit', 5)), 20)

        if not query:
            return jsonify({'error': 'Query is required'}), 400

        logger.info(f'æ–‡å­—æœç´¢è¯·æ±‚: "{query}", é™åˆ¶: {limit}')

        with db.get_connection() as conn:
            cursor = conn.cursor()
            query_lower = query.lower()
            query_normalized = re.sub(r'\s+', ' ', query_lower).strip()

            def fetch_by_terms(terms, remaining_limit, exclude_ids=None):
                if not terms or remaining_limit <= 0:
                    return []
                conditions = []
                params = []
                for term in terms:
                    like = f"%{term}%"
                    conditions.append("(LOWER(english_title) LIKE ? OR LOWER(title) LIKE ?)")
                    params.extend([like, like])
                where_clause = " OR ".join(conditions)
                exclude_clause = ""
                exclude_params = []
                if exclude_ids:
                    placeholders = ",".join("?" for _ in exclude_ids)
                    exclude_clause = f" AND id NOT IN ({placeholders})"
                    exclude_params = list(exclude_ids)
                cursor.execute(f"""
                    SELECT id, product_url, title, english_title, description,
                           ruleEnabled, min_delay, max_delay, created_at,
                           cnfans_url, shop_name, custom_reply_text,
                           custom_reply_images, custom_image_urls, image_source,
                           reply_scope,
                           uploaded_reply_images
                    FROM products
                    WHERE ({where_clause}){exclude_clause}
                    ORDER BY created_at DESC
                    LIMIT ?
                """, (*params, *exclude_params, remaining_limit))
                return cursor.fetchall()

            rows = fetch_by_terms([query_normalized], limit)
            found_ids = {row['id'] for row in rows}

            tokens = [kw for kw in re.findall(r'\w+', query_normalized) if len(kw) >= 2]
            extra_terms = []
            if len(tokens) >= 2:
                for i in range(len(tokens) - 1):
                    term = f"{tokens[i]} {tokens[i + 1]}"
                    if term not in extra_terms:
                        extra_terms.append(term)
            for token in tokens:
                if any(ch.isdigit() for ch in token) and token not in extra_terms:
                    extra_terms.append(token)

            if extra_terms and len(rows) < limit:
                remaining = limit - len(rows)
                extra_rows = fetch_by_terms(extra_terms, remaining, found_ids)
                rows.extend(extra_rows)
                found_ids.update({row['id'] for row in extra_rows})

            if not rows:
                tokens = tokens[:6]
                if tokens:
                    rows = fetch_by_terms(tokens, limit)

            if not rows:
                cursor.execute("""
                    SELECT id, product_url, title, english_title, description,
                           ruleEnabled, min_delay, max_delay, created_at,
                           cnfans_url, shop_name, custom_reply_text,
                           custom_reply_images, custom_image_urls, image_source,
                           reply_scope,
                           uploaded_reply_images
                    FROM products
                    WHERE (
                        english_title IS NOT NULL
                        AND LENGTH(TRIM(english_title)) >= 2
                        AND INSTR(?, LOWER(english_title)) > 0
                    )
                    OR (
                        title IS NOT NULL
                        AND LENGTH(TRIM(title)) >= 2
                        AND INSTR(?, LOWER(title)) > 0
                    )
                    ORDER BY created_at DESC
                    LIMIT ?
                """, (query_normalized, query_normalized, limit))
                rows = cursor.fetchall()

            products = []
            for row in rows:
                prod = dict(row)
                # ç®€å•è·å–ç¬¬ä¸€å¼ å›¾ä½œä¸ºé¢„è§ˆ
                cursor.execute("SELECT image_index FROM product_images WHERE product_id = ? ORDER BY image_index LIMIT 1", (prod['id'],))
                img_row = cursor.fetchone()
                # æ„é€ ç¬¦åˆ Bot é€»è¾‘çš„ image è·¯å¾„
                if img_row:
                    prod['images'] = [f"/api/image/{prod['id']}/{img_row[0]}"]
                else:
                    prod['images'] = []

                # è¡¥å…… Bot éœ€è¦çš„å­—æ®µ
                prod['weidianUrl'] = prod.get('product_url')
                prod['autoReplyEnabled'] = bool(prod.get('ruleEnabled', True))
                prod['replyScope'] = prod.get('reply_scope') or 'all'

                # è§£æ JSON å­—æ®µä¾› Bot ä½¿ç”¨
                try:
                    if prod.get('custom_reply_images'):
                        prod['selectedImageIndexes'] = json.loads(prod['custom_reply_images'])
                    if prod.get('custom_image_urls'):
                        prod['customImageUrls'] = json.loads(prod['custom_image_urls'])
                    # è§£æä¸Šä¼ çš„è‡ªå®šä¹‰å›å¤å›¾ç‰‡
                    if prod.get('uploaded_reply_images'):
                        prod['uploaded_reply_images'] = json.loads(prod['uploaded_reply_images'])
                except:
                    pass

                products.append(prod)

        logger.info(f'æ–‡å­—æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(products)} ä¸ªå•†å“')

        return jsonify({
            'success': True,
            'query': query,
            'products': products,
            'total': len(products)
        })

    except Exception as e:
        logger.error(f"æ–‡å­—æœç´¢å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/search_history/<int:history_id>', methods=['DELETE'])
def delete_search_history(history_id):
    """åˆ é™¤æœç´¢å†å²è®°å½•"""
    try:
        if db.delete_search_history(history_id):
            return jsonify({'success': True})
        else:
            return jsonify({'error': 'è®°å½•ä¸å­˜åœ¨'}), 404
    except Exception as e:
        logger.error(f"åˆ é™¤æœç´¢å†å²å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/search_history', methods=['DELETE'])
def clear_search_history():
    """æ¸…ç©ºæ‰€æœ‰æœç´¢å†å²"""
    try:
        if db.clear_search_history():
            return jsonify({'success': True})
        else:
            return jsonify({'error': 'æ¸…ç©ºå¤±è´¥'}), 500
    except Exception as e:
        logger.error(f"æ¸…ç©ºæœç´¢å†å²å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/logs/stream')
def log_stream():
    """Server-Sent Events æ—¥å¿—æµ"""
    import json

    def generate():
        # ä¸ºè¿™ä¸ªå®¢æˆ·ç«¯åˆ›å»ºé˜Ÿåˆ—
        client_queue = queue.Queue(maxsize=100)  # é™åˆ¶é˜Ÿåˆ—å¤§å°
        log_clients.append(client_queue)

        try:
            # å‘é€æœ€è¿‘çš„æ—¥å¿—å†å²
            for log_entry in all_logs[-50:]:  # å‘é€æœ€è¿‘50æ¡å†å²æ—¥å¿—
                yield f"data: {json.dumps(log_entry)}\n\n"

            # æŒç»­ç›‘å¬æ–°æ—¥å¿—
            while True:
                try:
                    # ç­‰å¾…æ–°æ—¥å¿—ï¼Œè¶…æ—¶æ—¶é—´è®¾ä¸º30ç§’
                    log_entry = client_queue.get(timeout=30)
                    yield f"data: {json.dumps(log_entry)}\n\n"
                except queue.Empty:
                    # å‘é€å¿ƒè·³åŒ…ä¿æŒè¿æ¥
                    yield f"data: {json.dumps({'type': 'heartbeat', 'timestamp': datetime.now().isoformat()})}\n\n"

        except GeneratorExit:
            # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥
            pass
        finally:
            # æ¸…ç†å®¢æˆ·ç«¯é˜Ÿåˆ—
            if client_queue in log_clients:
                log_clients.remove(client_queue)

    return Response(generate(), mimetype='text/event-stream',
                   headers={'Cache-Control': 'no-cache',
                           'Access-Control-Allow-Origin': '*',
                           'Access-Control-Allow-Headers': 'Cache-Control'})

@app.route('/api/logs/recent')
def get_recent_logs():
    """è·å–æœ€è¿‘çš„æ—¥å¿—è®°å½•"""
    try:
        # ä»æ—¥å¿—åˆ—è¡¨ä¸­è¿”å›æœ€è¿‘500æ¡æ—¥å¿—
        return jsonify({
            'logs': all_logs[-500:],
            'total': len(all_logs)
        })
    except Exception as e:
        logger.error(f"è·å–æœ€è¿‘æ—¥å¿—å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/logs/add', methods=['POST'])
def add_external_log():
    """æ¥æ”¶å¤–éƒ¨è¿›ç¨‹å‘é€çš„æ—¥å¿—"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'error': 'Invalid log data'}), 400

        # åˆ›å»ºæ—¥å¿—æ¡ç›®
        log_entry = {
            'timestamp': data.get('timestamp', datetime.now().isoformat()),
            'level': data.get('level', 'INFO'),
            'message': data.get('message', ''),
            'module': data.get('module', 'external'),
            'func': data.get('func', '')
        }

        # æ·»åŠ åˆ°æ—¥å¿—åˆ—è¡¨
        all_logs.append(log_entry)
        if len(all_logs) > 500:
            all_logs.pop(0)

        # æ·»åŠ åˆ°é˜Ÿåˆ—
        log_queue.put(log_entry)

        return jsonify({'success': True})
    except Exception as e:
        print(f"æ·»åŠ å¤–éƒ¨æ—¥å¿—å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

def start_discord_bot(user_id=None):
    """å¯åŠ¨Discordæœºå™¨äºº - æ”¯æŒå¤šè´¦å·"""
    global bot_running

    if bot_running:
        logger.warning("æœºå™¨äººå·²ç»åœ¨è¿è¡Œä¸­")
        return

    try:
        import asyncio
        from bot import DiscordBotClient

        logger.info(f"æ­£åœ¨å¯åŠ¨Discordæœºå™¨äºº... (ç”¨æˆ·ID: {user_id})")

        # è·å–è´¦å· - å¦‚æœæŒ‡å®šäº†ç”¨æˆ·IDï¼Œåªè·å–è¯¥ç”¨æˆ·çš„è´¦å·
        if user_id:
            accounts = db.get_discord_accounts_by_user(user_id)
        else:
            # è·å–æ‰€æœ‰è´¦å·
            accounts = db.get_discord_accounts_by_user(None)

        if not accounts:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„Discordè´¦å·")
            return

        logger.info(f"æ‰¾åˆ° {len(accounts)} ä¸ªDiscordè´¦å·ï¼Œå¼€å§‹å¯åŠ¨...")

        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œæœºå™¨äºº
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # ä¸ºæ¯ä¸ªè´¦å·åˆ›å»ºæœºå™¨äººå®ä¾‹
        for account in accounts:
            account_id = account['id']
            token = account['token']
            username = account.get('username', f'account_{account_id}')
            user_id = account.get('user_id')

            # è·å–ç”¨æˆ·ç®¡ç†çš„åº—é“º
            user_shops = None
            if user_id:
                user = db.get_user_by_id(user_id)
                if user:
                    user_shops = user.get('shops', [])

            # ç¡®å®šè´¦å·è§’è‰²ï¼šæ£€æŸ¥æ˜¯å¦ç»‘å®šäº†ä»»ä½•ç½‘ç«™é…ç½®
            account_bindings = db.get_account_website_bindings(account_id)
            if account_bindings:
                # æ£€æŸ¥è´¦å·æ˜¯å¦æœ‰å‘é€æˆ–ç›‘å¬è§’è‰²
                has_sender = any(b['role'] in ['sender', 'both'] for b in account_bindings)
                has_listener = any(b['role'] in ['listener', 'both'] for b in account_bindings)

                if has_sender and has_listener:
                    role = 'both'
                elif has_sender:
                    role = 'sender'
                elif has_listener:
                    role = 'listener'
                else:
                    role = 'both'  # é»˜è®¤
            else:
                role = 'both'  # æœªç»‘å®šçš„è´¦å·é»˜è®¤ä¸ºboth

            logger.info(f"æ­£åœ¨å¯åŠ¨æœºå™¨äººè´¦å·: {username} (ç”¨æˆ·ID: {user_id}, ç®¡ç†åº—é“º: {user_shops}, è§’è‰²: {role})")

            # åˆ›å»ºæœºå™¨äººå®ä¾‹ï¼Œä¼ å…¥è§’è‰²å‚æ•°
            client = DiscordBotClient(account_id=account_id, user_id=user_id, user_shops=user_shops, role=role)

            # å¯åŠ¨æœºå™¨äºº
            try:
                task = loop.create_task(client.start(token, reconnect=True))
                bot_clients.append(client)
                bot_tasks.append(task)
                logger.info(f"Discordæœºå™¨äººå¯åŠ¨æˆåŠŸ: {username}")
            except Exception as e:
                logger.error(f"å¯åŠ¨æœºå™¨äººå¤±è´¥ {username}: {e}")

        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œäº‹ä»¶å¾ªç¯
        import threading
        bot_thread = threading.Thread(target=loop.run_forever, daemon=True)
        bot_thread.start()

        if bot_clients:
            bot_running = True
            logger.info(f"å…±å¯åŠ¨äº† {len(bot_clients)} ä¸ªDiscordæœºå™¨äºº")
        else:
            logger.warning("æ²¡æœ‰æˆåŠŸå¯åŠ¨ä»»ä½•æœºå™¨äºº")

    except ImportError as e:
        logger.warning(f"Discordæœºå™¨äººæ¨¡å—ä¸å¯ç”¨: {e}")
        logger.info("Flaskåº”ç”¨å°†ç»§ç»­è¿è¡Œï¼Œä½†æœºå™¨äººåŠŸèƒ½ä¸å¯ç”¨")
    except Exception as e:
        logger.error(f"Discordæœºå™¨äººå¯åŠ¨å¤±è´¥: {e}")
        logger.info("Flaskåº”ç”¨å°†ç»§ç»­è¿è¡Œï¼Œä½†æœºå™¨äººåŠŸèƒ½ä¸å¯ç”¨")

def stop_discord_bot(user_id=None):
    """åœæ­¢Discordæœºå™¨äºº (æ”¯æŒæŒ‰ç”¨æˆ·åœæ­¢)"""
    global bot_running

    # å¦‚æœæ²¡æœ‰å®¢æˆ·ç«¯ï¼Œç›´æ¥è¿”å›
    if not bot_clients:
        logger.info("æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„æœºå™¨äºº")
        bot_running = False
        return

    logger.info(f"æ­£åœ¨åœæ­¢æœºå™¨äºº... {'(ç‰¹å®šç”¨æˆ·: ' + str(user_id) + ')' if user_id else '(æ‰€æœ‰ç”¨æˆ·)'}")

    try:
        import asyncio
        # è·å–å½“å‰çš„äº‹ä»¶å¾ªç¯ï¼Œå¦‚æœæ˜¯åœ¨ Flask çº¿ç¨‹ä¸­å¯èƒ½éœ€è¦å¤„ç†
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # ç­›é€‰éœ€è¦åœæ­¢çš„å®¢æˆ·ç«¯ç´¢å¼•
        indices_to_remove = []

        for i, client in enumerate(bot_clients):
            # å¦‚æœæŒ‡å®šäº† user_idï¼Œåªåœæ­¢è¯¥ç”¨æˆ·çš„æœºå™¨äºº
            # client.user_id æ˜¯æˆ‘ä»¬åœ¨ DiscordBotClient åˆå§‹åŒ–æ—¶ä¼ å…¥çš„
            if user_id is not None and getattr(client, 'user_id', None) != user_id:
                continue

            try:
                if client and not client.is_closed():
                    # æ›´æ–°è´¦å·çŠ¶æ€ä¸ºoffline
                    if hasattr(client, 'account_id') and client.account_id:
                        db.update_account_status(client.account_id, 'offline')
                        logger.info(f"è´¦å· {client.account_id} çŠ¶æ€å·²æ›´æ–°ä¸ºç¦»çº¿")

                    # åœæ­¢æœºå™¨äºº
                    asyncio.run_coroutine_threadsafe(client.close(), loop)
                    logger.info(f"Discordæœºå™¨äºº {i} (ç”¨æˆ· {getattr(client, 'user_id', 'unknown')}) å·²åœæ­¢ä¿¡å·å‘é€")
            except Exception as e:
                logger.error(f"åœæ­¢æœºå™¨äºº {i} æ—¶å‡ºé”™: {e}")

            indices_to_remove.append(i)

        # ä»åˆ—è¡¨ä¸­ç§»é™¤å·²åœæ­¢çš„æœºå™¨äººå’Œä»»åŠ¡
        # ä»åå¾€å‰åˆ ï¼Œé¿å…ç´¢å¼•åç§»
        for i in sorted(indices_to_remove, reverse=True):
            if i < len(bot_clients):
                bot_clients.pop(i)
            if i < len(bot_tasks):
                # å°è¯•å–æ¶ˆä»»åŠ¡
                task = bot_tasks[i]
                if task and not task.done():
                    task.cancel()
                bot_tasks.pop(i)

        if not bot_clients:
            bot_running = False
            logger.info("æ‰€æœ‰æœºå™¨äººå·²åœæ­¢")
        else:
            logger.info(f"å‰©ä½™ {len(bot_clients)} ä¸ªæœºå™¨äººä»åœ¨è¿è¡Œ")

    except Exception as e:
        logger.error(f"åœæ­¢æœºå™¨äººæµç¨‹å‡ºé”™: {e}")

# ===== æœºå™¨äººæ§åˆ¶API =====

@app.route('/api/bot/start', methods=['POST'])
def start_bot():
    """å¯åŠ¨Discordæœºå™¨äºº"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        data = request.get_json()
        user_id = data.get('userId')

        if not user_id:
            return jsonify({'error': 'éœ€è¦ç”¨æˆ·ID'}), 400

        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™çš„è´¦å·
        user_accounts = db.get_discord_accounts_by_user(user_id)

        if not user_accounts:
            return jsonify({'error': 'ç”¨æˆ·æ²¡æœ‰Discordè´¦å·ï¼Œè¯·å…ˆæ·»åŠ è´¦å·'}), 400

        # å¯åŠ¨æœºå™¨äººï¼ˆå¯åŠ¨æ‰€æœ‰è´¦å·ï¼Œä¸ç®¡æ˜¯å¦åœ¨çº¿ï¼‰
        start_discord_bot(user_id)

        logger.info(f"ç”¨æˆ· {user_id} å¯åŠ¨æœºå™¨äººæˆåŠŸï¼Œå…±æœ‰ {len(user_accounts)} ä¸ªè´¦å·")
        return jsonify({
            'message': 'è´¦å·å¯åŠ¨æˆåŠŸ',
            'totalAccounts': len(user_accounts)
        })

    except Exception as e:
        logger.error(f"å¯åŠ¨æœºå™¨äººå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/bot/stop', methods=['POST'])
def stop_bot():
    """åœæ­¢Discordæœºå™¨äºº"""
    if not require_login():
        return jsonify({'error': 'éœ€è¦ç™»å½•'}), 401

    try:
        current_user = get_current_user()

        # å¦‚æœæ˜¯ç®¡ç†å‘˜ï¼Œä¸”è¯·æ±‚ä¸­åŒ…å« targetUserIdï¼Œåˆ™åœæ­¢æŒ‡å®šç”¨æˆ·çš„
        # å¦åˆ™åœæ­¢å½“å‰ç”¨æˆ·çš„
        # è¿™é‡Œç®€åŒ–é€»è¾‘ï¼šç”¨æˆ·åªèƒ½åœæ­¢è‡ªå·±çš„
        user_id = current_user['id']

        stop_discord_bot(user_id)

        logger.info(f"ç”¨æˆ· {user_id} çš„æœºå™¨äººå·²åœæ­¢")
        return jsonify({'message': 'æœºå™¨äººåœæ­¢æˆåŠŸ'})

    except Exception as e:
        logger.error(f"åœæ­¢æœºå™¨äººå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/bot/status', methods=['GET'])
def get_bot_status():
    """è·å–Discordæœºå™¨äººå…¨å±€è¿è¡ŒçŠ¶æ€"""
    try:
        # é€šè¿‡æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰è´¦å·çŠ¶æ€ä¸ºonlineæ¥ç¡®å®šæœºå™¨äººæ˜¯å¦åœ¨è¿è¡Œ
        # è¿™æ ·å¯ä»¥é¿å…ä¾èµ–å†…å­˜ä¸­çš„å…¨å±€å˜é‡ï¼Œåœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ›´å¯é 
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM discord_accounts WHERE status = 'online'")
            online_count = cursor.fetchone()[0]
            is_running = online_count > 0
        return jsonify({'running': is_running})
    except Exception as e:
        logger.error(f"è·å–æœºå™¨äººçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'running': False}), 500

@app.route('/api/shop-info', methods=['GET'])
def get_shop_info():
    """è·å–åº—é“ºä¿¡æ¯"""
    try:
        shop_id = request.args.get('shopId')
        if not shop_id:
            return jsonify({'error': 'ç¼ºå°‘shopIdå‚æ•°'}), 400

        shop_id = shop_id.strip()
        if not shop_id.isdigit():
            return jsonify({'error': 'shopIdå¿…é¡»æ˜¯æ•°å­—'}), 400

        logger.info(f'è·å–åº—é“ºä¿¡æ¯: {shop_id}')

        # è°ƒç”¨å¾®åº—APIè·å–åº—é“ºä¿¡æ¯
        try:
            param = json.dumps({"shop_id": shop_id, "page_id": 0})
            encoded_param = quote(param)

            api_url = f"https://thor.weidian.com/decorate/customSharePage.getPageInfo/1.0?param={encoded_param}&wdtoken=8ea9315c&_={int(time.time() * 1000)}"

            response = requests.get(api_url, headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
                'Origin': 'https://weidian.com',
                'Referer': 'https://weidian.com/',
                'Sec-Ch-Ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"macOS"',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-site',
            }, cookies={
                'wdtoken': '8ea9315c',
                '__spider__visitorid': '0dcf6a5b878847ec',
                'visitor_id': '4d36e980-4128-451c-8178-a976b6303114',
                'v-components/cpn-coupon-dialog@nologinshop': '10',
                '__spider__sessionid': 'e55c6458ac1fdba4'
            }, timeout=10)

            if response.status_code == 200:
                data = response.json()
                if data.get('status', {}).get('code') == 0:
                    shop_name = data.get('result', {}).get('shareTitle', f'åº—é“º {shop_id}')
                    return jsonify({'shopName': shop_name})
                else:
                    logger.warning(f'APIè¿”å›é”™è¯¯çŠ¶æ€: {data}')
            else:
                logger.warning(f'APIè¯·æ±‚å¤±è´¥: {response.status_code}')

        except Exception as e:
            logger.error(f'è·å–åº—é“ºä¿¡æ¯å¤±è´¥: {e}')

        # å¦‚æœAPIå¤±è´¥ï¼Œè¿”å›é»˜è®¤åç§°
        return jsonify({'shopName': f'åº—é“º {shop_id}'})

    except Exception as e:
        logger.error(f'è·å–åº—é“ºä¿¡æ¯å‡ºé”™: {e}')
        return jsonify({'error': 'è·å–åº—é“ºä¿¡æ¯å¤±è´¥'}), 500

# ===== åº—é“ºç®¡ç†API =====

@app.route('/api/shops', methods=['GET'])
def get_shops():
    """è·å–åº—é“ºåˆ—è¡¨ï¼ˆæ ¹æ®ç”¨æˆ·æƒé™è¿‡æ»¤ï¼‰"""
    try:
        # è·å–æ‰€æœ‰åº—é“º
        all_shops = db.get_all_shops()

        current_user = get_current_user()
        if not current_user:
            # å¦‚æœæœªç™»å½•ï¼Œè¿”å›ç©º
            return jsonify({'shops': []})

        # å¦‚æœæ˜¯ç®¡ç†å‘˜ï¼Œè¿”å›æ‰€æœ‰
        if current_user['role'] == 'admin':
            return jsonify({'shops': all_shops})

        # å¦‚æœæ˜¯æ™®é€šç”¨æˆ·ï¼Œåªç­›é€‰å‡ºä»–æœ‰æƒé™çš„åº—é“º
        user_permitted_shop_ids = current_user.get('shops', [])
        filtered_shops = [s for s in all_shops if s['shop_id'] in user_permitted_shop_ids]

        return jsonify({'shops': filtered_shops})
    except Exception as e:
        logger.error(f'è·å–åº—é“ºåˆ—è¡¨å¤±è´¥: {e}')
        return jsonify({'error': 'è·å–åº—é“ºåˆ—è¡¨å¤±è´¥'}), 500

@app.route('/api/shops', methods=['POST'])
def add_shop():
    """æ·»åŠ æ–°åº—é“º"""
    if not can_manage_shops():
        return jsonify({'error': 'éœ€è¦ç®¡ç†åº—é“ºçš„æƒé™'}), 403

    try:
        data = request.get_json()
        if not data or not data.get('shopId') or not data.get('name'):
            return jsonify({'error': 'ç¼ºå°‘shopIdæˆ–nameå‚æ•°'}), 400

        shop_id = data['shopId'].strip()
        name = data['name'].strip()

        if not shop_id.isdigit():
            return jsonify({'error': 'shopIdå¿…é¡»æ˜¯æ•°å­—'}), 400

        # è·å–çœŸå®çš„åº—é“ºåç§°
        shop_info = get_shop_info_from_api(shop_id)
        if shop_info and shop_info.get('shopName'):
            name = shop_info['shopName']

        if db.add_shop(shop_id, name):
            return jsonify({'success': True, 'message': 'åº—é“ºæ·»åŠ æˆåŠŸ'})
        else:
            return jsonify({'error': 'åº—é“ºå·²å­˜åœ¨æˆ–æ·»åŠ å¤±è´¥'}), 400
    except Exception as e:
        logger.error(f'æ·»åŠ åº—é“ºå¤±è´¥: {e}')
        return jsonify({'error': 'æ·»åŠ åº—é“ºå¤±è´¥'}), 500

@app.route('/api/shops/<shop_id>', methods=['DELETE'])
def delete_shop(shop_id):
    """åˆ é™¤åº—é“º"""
    if not can_manage_shops():
        return jsonify({'error': 'éœ€è¦ç®¡ç†åº—é“ºçš„æƒé™'}), 403

    try:
        # è·å–åº—é“ºä¿¡æ¯ï¼Œæ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰æƒé™åˆ é™¤
        shop_info = db.get_shop_by_id(shop_id)
        if not shop_info:
            return jsonify({'error': 'åº—é“ºä¸å­˜åœ¨'}), 404

        current_user = get_current_user()
        # ç®¡ç†å‘˜å¯ä»¥åˆ é™¤ä»»ä½•åº—é“ºï¼Œæ™®é€šç”¨æˆ·åªèƒ½åˆ é™¤åˆ†é…ç»™ä»–ä»¬çš„åº—é“º
        if current_user['role'] != 'admin' and shop_info['shop_id'] not in current_user.get('shops', []):
            return jsonify({'error': 'æ— æƒé™åˆ é™¤æ­¤åº—é“º'}), 403

        if db.delete_shop(shop_id):
            return jsonify({'success': True, 'message': 'åº—é“ºåˆ é™¤æˆåŠŸ'})
        else:
            return jsonify({'error': 'åˆ é™¤å¤±è´¥'}), 500
    except Exception as e:
        logger.error(f'åˆ é™¤åº—é“ºå¤±è´¥: {e}')
        return jsonify({'error': 'åˆ é™¤åº—é“ºå¤±è´¥'}), 500

def get_shop_info_from_api(shop_id):
    """ä»APIè·å–åº—é“ºä¿¡æ¯"""
    try:
        import json
        from urllib.parse import quote
        import time

        param = json.dumps({"shop_id": shop_id, "page_id": 0})
        encoded_param = quote(param)

        api_url = f"https://thor.weidian.com/decorate/customSharePage.getPageInfo/1.0?param={encoded_param}&wdtoken=8ea9315c&_={int(time.time() * 1000)}"

        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en-US,en;q=0.9,zh-HK;q=0.8,zh-CN;q=0.7,zh;q=0.6',
            'Origin': 'https://weidian.com',
            'Referer': 'https://weidian.com/',
            'Sec-Ch-Ua': '"Google Chrome";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'empty',
            'Sec-Fetch-Mode': 'cors',
            'Sec-Fetch-Site': 'same-site',
        }
        cookies = {
            'wdtoken': '8ea9315c',
            '__spider__visitorid': '0dcf6a5b878847ec',
            'visitor_id': '4d36e980-4128-451c-8178-a976b6303114',
            'v-components/cpn-coupon-dialog@nologinshop': '2',
            '__spider__sessionid': 'c7da7d6e06b1f1ac'
        }

        for attempt in range(1, 5):
            try:
                response = requests.get(
                    api_url,
                    headers=headers,
                    cookies=cookies,
                    timeout=10,
                    proxies={'http': None, 'https': None}
                )
                response.raise_for_status()
                data = response.json()
                if data.get('status', {}).get('code') == 0:
                    result = data.get('result', {})
                    shop_name = result.get('shareTitle', '')
                    if shop_name:
                        return {'shopName': shop_name}
            except Exception as e:
                if attempt < 4:
                    time.sleep(0.4 * attempt)
                    continue
                raise e

    except Exception as e:
        logger.warning(f'è·å–åº—é“ºä¿¡æ¯å¤±è´¥: {e}')

    return None

@app.route('/api/scrape/shop', methods=['POST'])
def scrape_shop():
    """æŠ“å–æ•´ä¸ªåº—é“ºçš„æ‰€æœ‰å•†å“"""
    if not can_manage_shops():
        return jsonify({'error': 'éœ€è¦ç®¡ç†åº—é“ºçš„æƒé™'}), 403

    try:
        data = request.get_json()
        if not data or not data.get('shopId'):
            return jsonify({'error': 'ç¼ºå°‘shopIdå‚æ•°'}), 400

        shop_id = data['shopId'].strip()
        if not shop_id.isdigit():
            return jsonify({'error': 'shopIdå¿…é¡»æ˜¯æ•°å­—'}), 400

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æŠ“å–ä»»åŠ¡åœ¨è¿è¡Œ
        current_status = db.get_scrape_status()
        if current_status.get('is_scraping', False):
            return jsonify({'error': 'å·²æœ‰æŠ“å–ä»»åŠ¡åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†è¯•'}), 409

        logger.info(f'å¼€å§‹æŠ“å–åº—é“º: {shop_id}')

        # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒæŠ“å–ä»»åŠ¡ï¼Œé¿å…é˜»å¡å…¶ä»–æ“ä½œ
        import threading

        def run_scrape_task():
            """åå°æŠ“å–ä»»åŠ¡"""
            try:
                scrape_shop_products(shop_id)
            except Exception as e:
                logger.error(f'æŠ“å–ä»»åŠ¡å¼‚å¸¸: {e}')
            finally:
                # ç¡®ä¿çŠ¶æ€æ­£ç¡®é‡ç½®
                error_msg = f'æŠ“å–å¼‚å¸¸ç»“æŸ: {str(e)}' if 'e' in locals() else 'æŠ“å–å·²å®Œæˆ'
                db.update_scrape_status(is_scraping=False, message=error_msg)

        # åˆ›å»ºå®ˆæŠ¤çº¿ç¨‹ï¼Œç¡®ä¿ä¸ä¼šé˜»å¡åº”ç”¨é€€å‡º
        scrape_thread = threading.Thread(target=run_scrape_task, daemon=True, name=f'scrape-{shop_id}')
        scrape_thread.start()

        logger.info(f'å·²å¯åŠ¨åå°æŠ“å–çº¿ç¨‹å¤„ç†åº—é“º {shop_id}')

        return jsonify({
            'success': True,
            'message': 'æŠ“å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œè¯·æŸ¥çœ‹è¿›åº¦'
        })

    except Exception as e:
        logger.error(f'åº—é“ºæŠ“å–å¤±è´¥: {e}')
        return jsonify({'error': str(e)        }), 500


@app.route('/api/scrape/shop/control', methods=['POST'])
def control_shop_scrape():
    """æ§åˆ¶æŠ“å–ä»»åŠ¡: start, stop"""
    data = request.get_json(silent=True) or {}
    action = data.get('action')
    shop_id = data.get('shopId')  # å¯é€‰å‚æ•°

    global current_scrape_thread, scrape_thread_lock, scrape_stop_event

    # è·å–å½“å‰çŠ¶æ€
    current_status = db.get_scrape_status()
    logger.info(f"æ”¶åˆ°æŠ“å–æ§åˆ¶è¯·æ±‚: action={action}, shop_id={shop_id}, å½“å‰çŠ¶æ€: is_scraping={current_status.get('is_scraping')}, stop_signal={current_status.get('stop_signal')}")

    if action == 'stop':
        # ç«‹å³åœæ­¢ - è®¾ç½®åœæ­¢äº‹ä»¶å’Œæ•°æ®åº“çŠ¶æ€
        scrape_stop_event.set()  # è®¾ç½®åœæ­¢äº‹ä»¶ï¼Œé€šçŸ¥çº¿ç¨‹åœæ­¢

        success = db.update_scrape_status(
            is_scraping=False,
            stop_signal=True,
            completed=False,
            message='æ­£åœ¨åœæ­¢æŠ“å–...',
            progress=100
        )

        if success:
            logger.info("âœ… æŠ“å–ä»»åŠ¡å·²å¼ºåˆ¶åœæ­¢")

            # ç­‰å¾…çº¿ç¨‹ç»ˆæ­¢ï¼ˆæœ€å¤šç­‰å¾…10ç§’ï¼‰
            with scrape_thread_lock:
                if current_scrape_thread and current_scrape_thread.is_alive():
                    logger.info("ç­‰å¾…æŠ“å–çº¿ç¨‹ç»ˆæ­¢...")
                    current_scrape_thread.join(timeout=10.0)
                    if current_scrape_thread.is_alive():
                        logger.warning("æŠ“å–çº¿ç¨‹æœªèƒ½åœ¨10ç§’å†…ç»ˆæ­¢")
                    current_scrape_thread = None

            updated_status = db.get_scrape_status()
            return jsonify(updated_status)
        else:
            return jsonify({'error': 'åœæ­¢æŠ“å–å¤±è´¥'}), 500

    if action == 'start':
        if current_status.get('is_scraping', False):
            return jsonify({'error': 'å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œ'}), 400

        # æ£€æŸ¥æ˜¯å¦æœ‰çº¿ç¨‹åœ¨è¿è¡Œ
        with scrape_thread_lock:
            if current_scrape_thread and current_scrape_thread.is_alive():
                return jsonify({'error': 'å·²æœ‰çº¿ç¨‹åœ¨è¿è¡Œ'}), 400

        # æ¸…é™¤åœæ­¢äº‹ä»¶ï¼Œä¸ºæ–°ä»»åŠ¡åšå‡†å¤‡
        scrape_stop_event.clear()

        # é‡ç½®çŠ¶æ€
        success = db.update_scrape_status(
            is_scraping=True,
            stop_signal=False,
            current_shop_id=shop_id,
            total=0,
            processed=0,
            success=0,
            progress=0,
            message='åˆå§‹åŒ–æŠ“å–...',
            completed=False,
            thread_id=None,
            failed_items=[]
        )

        if not success:
            return jsonify({'error': 'é‡ç½®çŠ¶æ€å¤±è´¥'}), 500

        # å¼‚æ­¥å¯åŠ¨
        with scrape_thread_lock:
            current_scrape_thread = threading.Thread(
                target=run_shop_scrape_task,
                args=(shop_id,),
                daemon=True,
                name=f'scrape-{shop_id}'
            )
            current_scrape_thread.start()

            # æ›´æ–°çº¿ç¨‹IDåˆ°æ•°æ®åº“
            db.update_scrape_status(thread_id=current_scrape_thread.ident)

        updated_status = db.get_scrape_status()
        return jsonify(updated_status)

    return jsonify({'error': 'Invalid action'}), 400

@app.route('/api/scrape/batch', methods=['POST'])
def batch_scrape_products():
    """æ‰¹é‡æŠ“å–å¤šä¸ªå•†å“ï¼ˆé«˜æ€§èƒ½å¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
    # åœ¨å‡½æ•°å¼€å§‹å°±å¯¼å…¥æ‰€æœ‰éœ€è¦çš„æ¨¡å—ï¼Œé¿å…å˜é‡ä½œç”¨åŸŸé—®é¢˜
    import concurrent.futures
    import time
    import threading

    # ç¡®ä¿threadingå˜é‡åœ¨å‡½æ•°ä½œç”¨åŸŸå†…å¯ç”¨
    threading = threading

    try:
        data = request.get_json()
        if not data or not data.get('productIds'):
            return jsonify({'error': 'ç¼ºå°‘productIdså‚æ•°'}), 400

        product_ids = data.get('productIds', [])
        if not isinstance(product_ids, list) or len(product_ids) == 0:
            return jsonify({'error': 'productIdså¿…é¡»æ˜¯éç©ºæ•°ç»„'}), 400

        # ====================================================
        # ä¿®å¤ï¼šç¡®ä¿SCRAPE_THREADSä»configæ­£ç¡®è·å–
        # ====================================================
        max_threads = getattr(config, 'SCRAPE_THREADS', 10)

        # åˆ›å»ºåœæ­¢äº‹ä»¶ç”¨äºä¼˜é›…å…³é—­
        shutdown_event = threading.Event()

        logger.info(f"âœ… å¼€å§‹æ‰¹é‡æŠ“å– {len(product_ids)} ä¸ªå•†å“ï¼Œä½¿ç”¨ {max_threads} ä¸ªçº¿ç¨‹")

        results = {
            'total': len(product_ids),
            'processed': 0,
            'success': 0,
            'skipped': 0,
            'cancelled': 0,
            'partial': 0,
            'errors': 0,
            'start_time': time.time()
        }
        details = []

        def process_single_product_batch(product_id):
            """å¤„ç†å•ä¸ªå•†å“ï¼ˆç”¨äºçº¿ç¨‹æ± ï¼‰"""
            try:
                current_status = db.get_scrape_status()
                if current_status.get('stop_signal', False):
                    logger.info(f"ğŸ”´ å¤„ç†å•†å“å‰æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆå¤„ç†å•†å“ {product_id}")
                    return {'status': 'cancelled', 'product_id': product_id, 'message': 'ä»»åŠ¡å·²å–æ¶ˆ'}

                product_info = {
                    'item_id': str(product_id),
                    'item_url': f'https://weidian.com/item.html?itemID={product_id}',
                    'shop_name': 'æ‰¹é‡ä¸Šä¼ '
                }

                result = process_and_save_single_product_sync(product_info) or {}
                status = result.get('status', 'failed')

                return {
                    'status': status,
                    'product_id': product_id,
                    'message': result.get('message', ''),
                    'failed_details': result.get('failed_details', [])
                }

            except Exception as e:
                logger.error(f"å¤„ç†å•†å“ {product_id} å¤±è´¥: {e}")
                return {'status': 'error', 'product_id': product_id, 'message': str(e)}

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†å•†å“
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_threads) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_product = {
                executor.submit(process_single_product_batch, pid): pid
                for pid in product_ids
            }

            # æ”¶é›†ç»“æœ - æ”¯æŒä¼˜é›…åœæ­¢
            pending_futures = set(future_to_product.keys())
            stop_detected = False

            try:
                while pending_futures:
                    # æ£€æŸ¥æ˜¯å¦æœ‰åœæ­¢ä¿¡å·æˆ–å…³é—­äº‹ä»¶
                    current_status = db.get_scrape_status()
                    should_stop = (current_status.get('stop_signal', False) or
                                 (shutdown_event and shutdown_event.is_set()))

                    if should_stop and not stop_detected:
                        logger.info("ğŸ”´ æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ç­‰å¾…å·²æäº¤çš„ä»»åŠ¡å®Œæˆ...")
                        db.update_scrape_status(message='æ­£åœ¨ç­‰å¾…å½“å‰å•†å“å®Œæˆ...')
                        stop_detected = True
                        # ä¸å…³é—­çº¿ç¨‹æ± ï¼Œè®©å·²æäº¤çš„ä»»åŠ¡ç»§ç»­å®Œæˆ

                    # ç­‰å¾…ä»»æ„ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                    done, pending_futures = concurrent.futures.wait(
                        pending_futures,
                        timeout=1.0,
                        return_when=concurrent.futures.FIRST_COMPLETED
                    )

                    # å¤„ç†å·²å®Œæˆçš„ä»»åŠ¡
                    for future in done:
                        product_id = future_to_product[future]
                        try:
                            result = future.result()
                            results['processed'] += 1

                            details.append({
                                'id': product_id,
                                'status': result.get('status'),
                                'message': result.get('message', ''),
                                'failed_details': result.get('failed_details', [])
                            })

                            if result['status'] == 'success':
                                results['success'] += 1
                                logger.info(f"å•†å“ {product_id} å¤„ç†æˆåŠŸ")
                            elif result['status'] == 'skipped':
                                results['skipped'] += 1
                                logger.debug(f"å•†å“ {product_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                            elif result['status'] == 'cancelled':
                                results['cancelled'] += 1
                                logger.info(f"å•†å“ {product_id} å¤„ç†è¢«å–æ¶ˆ")
                            elif result['status'] == 'partial':
                                results['partial'] += 1
                                logger.info(f"å•†å“ {product_id} éƒ¨åˆ†å®Œæˆï¼ˆå·²å…¥åº“ï¼Œå›¾ç‰‡å¤„ç†è¢«å–æ¶ˆï¼‰")
                            else:
                                results['errors'] += 1
                                logger.error(f"å•†å“ {product_id} å¤„ç†å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")

                        except Exception as e:
                            results['processed'] += 1
                            results['errors'] += 1
                            details.append({
                                'id': product_id,
                                'status': 'error',
                                'message': str(e)
                            })
                            logger.error(f"å¤„ç†å•†å“ {product_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")

                    # å¦‚æœæ£€æµ‹åˆ°åœæ­¢ä¿¡å·ä¸”æ²¡æœ‰å¾…å¤„ç†çš„ä»»åŠ¡ï¼Œé€€å‡ºå¾ªç¯
                    if stop_detected and len(pending_futures) == 0:
                        logger.info("âœ… æ‰€æœ‰å·²æäº¤çš„ä»»åŠ¡å·²å®Œæˆï¼Œé€€å‡ºæ‰¹é‡å¤„ç†")
                        break

            except KeyboardInterrupt:
                logger.warning("æ”¶åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
                executor.shutdown(wait=True, timeout=10.0)
                raise
            finally:
                # ç¡®ä¿çº¿ç¨‹æ± è¢«æ­£ç¡®å…³é—­
                if not executor._shutdown:
                    executor.shutdown(wait=False)

        # è®¡ç®—å¤„ç†æ—¶é—´
        results['end_time'] = time.time()
        results['duration'] = results['end_time'] - results['start_time']

        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {results}")

        # æ³¨æ„ï¼šæ‰¹é‡æŠ“å–ä¸åº”è¯¥é‡ç½®åº—é“ºæŠ“å–çš„çŠ¶æ€
        # æ‰¹é‡æŠ“å–æœ‰è‡ªå·±çš„çŠ¶æ€ç®¡ç†ï¼Œä¸å½±å“åº—é“ºæŠ“å–çš„çŠ¶æ€æ˜¾ç¤º

        return jsonify({
            'message': f'æ‰¹é‡å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {results["total"]} ä¸ªå•†å“ï¼ŒæˆåŠŸ {results["success"]} ä¸ªï¼Œè·³è¿‡ {results["skipped"]} ä¸ªï¼Œå–æ¶ˆ {results["cancelled"]} ä¸ªï¼Œéƒ¨åˆ†å®Œæˆ {results["partial"]} ä¸ªï¼Œå¤±è´¥ {results["errors"]} ä¸ª',
            'results': results,
            'details': details
        })

    except Exception as e:
        logger.error(f"æ‰¹é‡æŠ“å–å¤±è´¥: {e}")
        logger.error(f"é”™è¯¯å‘ç”Ÿåœ¨: {e.__class__.__name__}")
        import traceback
        logger.error(f"å®Œæ•´å †æ ˆ:\n{traceback.format_exc()}")
        return jsonify({'error': f'æ‰¹é‡æŠ“å–å¤±è´¥: {str(e)}'}), 500

@app.route('/api/scrape/shop/status', methods=['GET'])
def get_scrape_status():
    """è·å–æŠ“å–çŠ¶æ€"""
    try:
        status = db.get_scrape_status()

        # ç¡®ä¿è¿”å›å¿…è¦çš„å­—æ®µï¼ˆå…¼å®¹å‰ç«¯æœŸæœ›çš„å­—æ®µåï¼‰
        result = {
            'is_scraping': status.get('is_scraping', False),
            'stop_signal': status.get('stop_signal', False),
            'progress': status.get('progress', 0),
            'total': status.get('total', 0),
            'current': status.get('processed', 0),  # å‰ç«¯æœŸæœ›currentå­—æ®µ
            'processed': status.get('processed', 0),
            'success': status.get('success', 0),
            'failed_items': status.get('failed_items', []),
            'message': status.get('message', ''),
            'completed': status.get('completed', False),
            'current_shop_id': status.get('current_shop_id'),
            'thread_id': status.get('thread_id')
        }

        # è°ƒè¯•æ—¥å¿—
        logger.debug(f"DEBUG: Scrape status - is_scraping: {result.get('is_scraping')}, message: {result.get('message')}")

        return jsonify(result)
    except Exception as e:
        logger.error(f'è·å–æŠ“å–çŠ¶æ€å¤±è´¥: {e}')
        return jsonify({
            'is_scraping': False,
            'stop_signal': False,
            'progress': 0,
            'total': 0,
            'current': 0,
            'processed': 0,
            'success': 0,
            'message': 'è·å–çŠ¶æ€å¤±è´¥',
            'failed_items': [],
            'completed': False,
            'current_shop_id': None,
            'thread_id': None
        })

@app.route('/api/products/count', methods=['GET'])
def get_products_count():
    """è·å–å•†å“æ€»æ•°"""
    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM products")
            count = cursor.fetchone()[0]
            return jsonify({'count': count})
    except Exception as e:
        logger.error(f"è·å–å•†å“æ•°é‡å¤±è´¥: {e}")
        return jsonify({'count': 0}), 500

@app.route('/api/debug/user_permissions', methods=['GET'])
def debug_user_permissions():
    """è°ƒè¯•ç”¨æˆ·æƒé™å’Œå•†å“åˆ†é…ï¼ˆç®¡ç†å‘˜æƒé™ï¼‰"""
    if not require_admin():
        return jsonify({'error': 'éœ€è¦ç®¡ç†å‘˜æƒé™'}), 403

    try:
        with db.get_connection() as conn:
            cursor = conn.cursor()

            # è·å–æ‰€æœ‰ç”¨æˆ·
            cursor.execute('SELECT id, username, role FROM users')
            users = []
            for row in cursor.fetchall():
                user_dict = dict(row)
                user_dict['shops'] = db.get_user_shops(user_dict['id'])
                users.append(user_dict)

            # è·å–æ‰€æœ‰åº—é“º
            cursor.execute('SELECT id, name FROM shops')
            shops = [dict(row) for row in cursor.fetchall()]

            # è·å–å•†å“ç»Ÿè®¡
            cursor.execute('SELECT shop_name, COUNT(*) as count FROM products GROUP BY shop_name')
            product_stats = [dict(row) for row in cursor.fetchall()]

            # è·å–ç”¨æˆ·åº—é“ºæƒé™ç»Ÿè®¡
            cursor.execute('SELECT user_id, COUNT(*) as shop_count FROM user_shop_permissions GROUP BY user_id')
            permission_stats = []
            for row in cursor.fetchall():
                user_id, shop_count = row
                user = next((u for u in users if u['id'] == user_id), None)
                if user:
                    permission_stats.append({
                        'username': user['username'],
                        'shop_count': shop_count,
                        'shops': user['shops']
                    })

            return jsonify({
                'users': users,
                'shops': shops,
                'product_stats': product_stats,
                'permission_stats': permission_stats
            })
    except Exception as e:
        logger.error(f"è°ƒè¯•ç”¨æˆ·æƒé™å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500

def run_shop_scrape_task(shop_id):
    """åå°ä»»åŠ¡åŒ…è£…å™¨ - è°ƒç”¨çœŸæ­£çš„æŠ“å–é€»è¾‘"""
    try:
        logger.info(f"ğŸ§µ åå°æŠ“å–çº¿ç¨‹å¯åŠ¨: {shop_id}")
        scrape_shop_products(shop_id)
    except Exception as e:
        logger.error(f"âŒ åå°æŠ“å–çº¿ç¨‹å´©æºƒ: {e}")
        db.update_scrape_status(message=f"ç³»ç»Ÿé”™è¯¯: {str(e)}")
    finally:
        # ç¡®ä¿çŠ¶æ€æ­£ç¡®é‡ç½®
        final_status = db.get_scrape_status()
        db.update_scrape_status(
            is_scraping=False,
            completed=True
        )
        if not final_status.get('stop_signal', False):
            db.update_scrape_status(message='ä»»åŠ¡ç»“æŸ')
        logger.info("ğŸ§µ åå°æŠ“å–çº¿ç¨‹ç»“æŸ")

def get_all_category_ids(shop_id, session):
    """
    è·å–åº—é“ºæ‰€æœ‰åˆ†ç±»ID (åŒ…æ‹¬å­åˆ†ç±»)
    API: decorate/itemCate.getCateTree
    """
    try:
        import time
        from urllib.parse import quote

        url = "https://thor.weidian.com/decorate/itemCate.getCateTree/1.0"
        param = json.dumps({
            "shopId": str(shop_id),
            "attrQuery": [],
            "from": "h5"
        })
        full_url = f"{url}?param={quote(param)}&wdtoken=8ea9315c&_={int(time.time()*1000)}"

        logger.info(f"æ­£åœ¨è·å–åº—é“ºåˆ†ç±»æ ‘: {shop_id}")
        response = session.get(full_url, timeout=10)
        data = response.json()

        cate_ids = []

        if data.get('status', {}).get('code') == 0:
            cate_list = data.get('result', {}).get('cateList', [])

            def extract_ids(nodes):
                for node in nodes:
                    cid = node.get('cateId')
                    cname = node.get('cateName')
                    count = node.get('speCateItemNum', 0)

                    if cid:
                        cate_ids.append({'id': cid, 'name': cname, 'count': count})

                    children = node.get('childCateList', [])
                    if children:
                        extract_ids(children)

            extract_ids(cate_list)
            logger.info(f"âœ… æˆåŠŸè·å– {len(cate_ids)} ä¸ªåˆ†ç±»: {[c['name'] for c in cate_ids]}")
        else:
            logger.warning(f"è·å–åˆ†ç±»æ ‘å¤±è´¥: {data}")

        return cate_ids

    except Exception as e:
        logger.error(f"è·å–åˆ†ç±»æ ‘å¼‚å¸¸: {e}")
        return []

def fetch_category_items(shop_id, cate_id, cate_name, session, limit=20):
    """
    ç”Ÿæˆå™¨ï¼šæŠ“å–æŒ‡å®šåˆ†ç±»ä¸‹çš„æ‰€æœ‰å•†å“
    API: decorate/itemCate.getCateItemList
    """
    import time
    from urllib.parse import quote

    offset = 0

    while True:
        try:
            url = "https://thor.weidian.com/decorate/itemCate.getCateItemList/1.0"
            param = json.dumps({
                "cateId": str(cate_id),
                "shopId": str(shop_id),
                "offset": offset,
                "limit": limit,
                "sortField": "all",
                "sortType": "desc",
                "isQdFx": False,
                "isHideSold": False,
                "hideItemRealAmount": False,
                "from": "h5"
            })
            full_url = f"{url}?param={quote(param)}&wdtoken=8ea9315c&_={int(time.time()*1000)}"

            response = session.get(full_url, timeout=10)
            data = response.json()

            if data.get('status', {}).get('code') != 0:
                logger.warning(f"åˆ†ç±»[{cate_name}] Offset {offset} APIé”™è¯¯: {data.get('status')}")
                break

            result = data.get('result', {})
            items = result.get('itemList', [])

            if not items:
                break

            for item in items:
                yield item

            if len(items) < limit:
                break

            offset += limit
            time.sleep(0.3)

        except Exception as e:
            logger.error(f"æŠ“å–åˆ†ç±»[{cate_name}]å¼‚å¸¸: {e}")
            break

def scrape_shop_products(shop_id):
    """æŠ“å–åº—é“ºæ‰€æœ‰å•†å“çš„å®ç° (åˆ†ç±»æ ‘æ–¹æ¡ˆ - çªç ´2000æ¡é™åˆ¶)"""
    import requests
    import time
    from weidian_scraper import get_weidian_scraper
    import concurrent.futures

    # è·å–é…ç½®çš„çº¿ç¨‹æ•°
    try:
        from config import config
        max_threads = config.SCRAPE_THREADS
    except:
        max_threads = 2

    # å¯¼å…¥å…¨å±€åœæ­¢äº‹ä»¶
    global scrape_stop_event

    scraper = get_weidian_scraper()
    unique_product_tasks = {}  # ä½¿ç”¨å­—å…¸å»é‡ï¼šitem_id -> product_info
    failed_items = []

    # åˆå§‹åŒ–çŠ¶æ€
    db.update_scrape_status(
        is_scraping=True,
        paused=False,
        stop_signal=False,
        progress=0,
        total=0,
        processed=0,
        success=0,
        failed=0,
        image_failed=0,
        index_failed=0,
        failed_items=[],
        message='æ­£åœ¨åˆå§‹åŒ–...'
    )

    # è·å–åº—é“ºåç§°
    shop_info = get_shop_info_from_api(shop_id)
    shop_name = shop_info.get('shopName', f'åº—é“º {shop_id}') if shop_info else f'åº—é“º {shop_id}'

    db.update_scrape_status(message=f'æ­£åœ¨æŠ“å–åº—é“º: {shop_name}')
    logger.info(f"å¼€å§‹æ”¶é›†å•†å“åˆ—è¡¨ï¼Œåº—é“º: {shop_name}")

    # ã€æ€§èƒ½ä¼˜åŒ–ã€‘ä¸€æ¬¡æ€§è·å–æ‰€æœ‰å·²å­˜åœ¨çš„å•†å“IDï¼Œé¿å…é€ä¸ªæŸ¥è¯¢æ•°æ®åº“
    logger.info("æ­£åœ¨åŠ è½½å·²å­˜åœ¨çš„å•†å“ID...")
    existing_item_ids = db.get_all_existing_item_ids()
    logger.info(f"å·²åŠ è½½ {len(existing_item_ids)} ä¸ªå·²å­˜åœ¨çš„å•†å“IDï¼Œå°†å¿«é€Ÿè·³è¿‡")

    # =========================================================================
    # é˜¶æ®µ 1: é€šè¿‡åˆ†ç±»æ ‘æŠ“å–æ‰€æœ‰å•†å“ (å¤šçº¿ç¨‹ä¼˜åŒ–ç‰ˆ)
    # =========================================================================
    logger.info("=== é˜¶æ®µ 1: é€šè¿‡åˆ†ç±»æ ‘å¹¶å‘æŠ“å–å•†å“ ===")

    db.update_scrape_status(message='æ­£åœ¨è·å–åº—é“ºåˆ†ç±»æ ‘...')

    if not (scrape_stop_event.is_set() or db.get_scrape_status().get('stop_signal', False)):
        try:
            # 1. è·å–æ‰€æœ‰åˆ†ç±» ID
            categories = get_all_category_ids(shop_id, scraper.session)

            if not categories:
                logger.warning("æœªè·å–åˆ°ä»»ä½•åˆ†ç±»ï¼Œå°è¯•ä½¿ç”¨Tab 0å¤‡ç”¨æ–¹æ¡ˆ...")
                db.update_scrape_status(message='æœªæ‰¾åˆ°åˆ†ç±»ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...')
            else:
                logger.info(f"è·å–åˆ° {len(categories)} ä¸ªåˆ†ç±»ï¼Œå‡†å¤‡å¹¶å‘æ‰«æ...")

                # å®šä¹‰å•ä¸ªåˆ†ç±»çš„å¤„ç†å‡½æ•°
                def process_category(cate):
                    if scrape_stop_event.is_set() or db.get_scrape_status().get('stop_signal', False):
                        return 0

                    cate_id = cate['id']
                    cate_name = cate['name']
                    # è·³è¿‡ç©ºåˆ†ç±»
                    if cate['count'] == 0:
                        return 0

                    local_new_count = 0
                    # æ³¨æ„ï¼šfetch_category_items å†…éƒ¨ä¼šæœ‰åˆ†é¡µè¯·æ±‚ï¼Œè¿™é‡Œæ˜¯ IO å¯†é›†å‹
                    for item in fetch_category_items(shop_id, cate_id, cate_name, scraper.session):
                        item_id = str(item.get('itemId', ''))

                        # æ£€æŸ¥åœæ­¢ä¿¡å·
                        if scrape_stop_event.is_set():
                            break

                        if item_id:
                            # å­—å…¸æ“ä½œçš„çº¿ç¨‹å®‰å…¨æ€§ï¼šPythonå­—å…¸çš„keyå”¯ä¸€æ€§å¤©ç„¶å»é‡
                            if item_id not in unique_product_tasks:
                                # æ£€æŸ¥æ•°æ®åº“å»é‡
                                if item_id in existing_item_ids:
                                    continue

                                unique_product_tasks[item_id] = {
                                    'item_id': item_id,
                                    'item_url': item.get('itemUrl', f"https://weidian.com/item.html?itemID={item_id}"),
                                    'shop_name': shop_name
                                }
                                local_new_count += 1
                    return local_new_count

                # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰«æåˆ†ç±»
                # åˆ†ç±»æ‰«æä¸»è¦æ˜¯ç½‘ç»œè¯·æ±‚ï¼Œå¯ä»¥å¼€è¾ƒé«˜çš„å¹¶å‘
                cate_workers = min(10, len(categories))
                cate_executor = concurrent.futures.ThreadPoolExecutor(max_workers=cate_workers)
                cate_stop_requested = False
                try:
                    # æäº¤ä»»åŠ¡
                    future_to_cate = {cate_executor.submit(process_category, cate): cate for cate in categories}
                    pending_futures = set(future_to_cate.keys())

                    completed_cates = 0
                    while pending_futures:
                        if scrape_stop_event.is_set() or db.get_scrape_status().get('stop_signal', False):
                            cate_stop_requested = True
                            for future in pending_futures:
                                future.cancel()
                            db.update_scrape_status(message="æ­£åœ¨åœæ­¢ä»»åŠ¡...")
                            cate_executor.shutdown(wait=False, cancel_futures=True)
                            pending_futures.clear()
                            break

                        done, pending_futures = concurrent.futures.wait(
                            pending_futures,
                            timeout=1.0,
                            return_when=concurrent.futures.FIRST_COMPLETED
                        )

                        for future in done:
                            cate = future_to_cate[future]
                            try:
                                count = future.result()
                                completed_cates += 1
                                logger.debug(f"[{completed_cates}/{len(categories)}] åˆ†ç±» '{cate['name']}' æ‰«æå®Œæˆï¼Œæ–°å¢ {count} ä¸ªå•†å“")
                                # å®æ—¶æ›´æ–°å‰ç«¯æ˜¾ç¤ºçš„æ€»æ•°
                                db.update_scrape_status(
                                    total=len(unique_product_tasks),
                                    message=f"æ­£åœ¨å¹¶å‘æ‰«æåˆ†ç±» ({completed_cates}/{len(categories)})..."
                                )
                            except Exception as e:
                                logger.error(f"æ‰«æåˆ†ç±» '{cate['name']}' å¤±è´¥: {e}")
                finally:
                    if not cate_stop_requested:
                        cate_executor.shutdown(wait=True)

        except Exception as e:
            logger.error(f"åˆ†ç±»éå†è¿‡ç¨‹å¼‚å¸¸: {e}")

    logger.info(f"âœ… åˆ†ç±»æ ‘æŠ“å–å®Œæˆï¼Œå…±æ”¶é›† {len(unique_product_tasks)} ä¸ªå•†å“")

    # =========================================================================
    # é˜¶æ®µ 2: å¹¶å‘å¤„ç†
    # =========================================================================

    # è½¬å›åˆ—è¡¨ç”¨äºå¤„ç†
    all_product_tasks = list(unique_product_tasks.values())
    total_products = len(all_product_tasks)
    logger.info(f"âœ… å•†å“æ”¶é›†é˜¶æ®µç»“æŸï¼Œå»é‡åæœ€ç»ˆå¾…å¤„ç†: {total_products} ä¸ªå•†å“")

    # æ›´æ–°çŠ¶æ€ï¼šå¼€å§‹å¤„ç†
    db.update_scrape_status(
        total=total_products,
        progress=0, # é‡ç½®è¿›åº¦æ¡ä¸º0ï¼Œå¼€å§‹ç¬¬äºŒé˜¶æ®µ
        message=f'æ”¶é›†å®Œæˆï¼Œå‡†å¤‡å¹¶å‘å¤„ç† {total_products} ä¸ªå•†å“...'
    )

    # ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨å…¨å±€çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰€æœ‰å•†å“
    processed_count = 0
    success_count = 0
    failed_count = 0
    image_failed_count = 0
    index_failed_count = 0

    stop_requested = False
    if all_product_tasks:
        executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_threads)
        try:
            # æäº¤æ‰€æœ‰å•†å“ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
            future_to_product = {
                executor.submit(process_and_save_single_product_sync, product_info): product_info
                for product_info in all_product_tasks
            }

            pending_futures = set(future_to_product.keys())

            # è½®è¯¢ç­‰å¾…ï¼Œç¡®ä¿å¯ä¸­æ–­
            while pending_futures:
                # æ£€æŸ¥åœæ­¢äº‹ä»¶æˆ–åœæ­¢ä¿¡å·
                if scrape_stop_event.is_set() or db.get_scrape_status().get('stop_signal', False):
                    logger.info("ğŸ”´ æ£€æµ‹åˆ°åœæ­¢äº‹ä»¶/ä¿¡å·ï¼Œæ­£åœ¨å–æ¶ˆå‰©ä½™ä»»åŠ¡...")
                    stop_requested = True
                    for future in pending_futures:
                        future.cancel()
                    db.update_scrape_status(message="æ­£åœ¨åœæ­¢ä»»åŠ¡...")
                    executor.shutdown(wait=False, cancel_futures=True)
                    pending_futures.clear()
                    break

                done, pending_futures = concurrent.futures.wait(
                    pending_futures,
                    timeout=1.0,
                    return_when=concurrent.futures.FIRST_COMPLETED
                )

                for future in done:
                    product_info = future_to_product.get(future, {})
                    try:
                        result = future.result() or {}
                        processed_count += 1

                        if not result:
                            failed_count += 1
                            failed_items.append({
                                'id': str(product_info.get('item_id', '')),
                                'reason': 'æœªçŸ¥é”™è¯¯'
                            })
                        elif result.get('failed'):
                            failed_count += 1
                            if result.get('image_failed'):
                                image_failed_count += 1
                            if result.get('index_failed'):
                                index_failed_count += 1
                            failed_items.append({
                                'id': str(result.get('item_id') or product_info.get('item_id') or ''),
                                'reason': result.get('message') or 'æœªçŸ¥é”™è¯¯',
                                'details': result.get('failed_details', [])
                            })
                        else:
                            success_count += 1

                        # æ”¹ä¸ºæ¯5ä¸ªæ›´æ–°ä¸€æ¬¡ï¼Œåé¦ˆæ›´åŠæ—¶
                        if processed_count % 5 == 0 or processed_count == total_products:
                            # è®¡ç®—è¿›åº¦ (é¿å…é™¤ä»¥0)
                            progress = int((processed_count / total_products) * 100) if total_products > 0 else 100
                            db.update_scrape_status(
                                processed=processed_count,
                                success=success_count,
                                failed=failed_count,
                                image_failed=image_failed_count,
                                index_failed=index_failed_count,
                                progress=progress,
                                message=f'æ­£åœ¨æŠ“å–è¯¦æƒ…ä¸å›¾ç‰‡... ({processed_count}/{total_products})'
                            )
                    except Exception as e:
                        logger.error(f"å•†å“å¤„ç†å¼‚å¸¸: {e}")
                        processed_count += 1
                        failed_count += 1
                        failed_items.append({
                            'id': str(product_info.get('item_id', '')),
                            'reason': str(e)
                        })
        finally:
            if not stop_requested:
                executor.shutdown(wait=True)

    # ç»“æŸ
    if stop_requested or scrape_stop_event.is_set() or db.get_scrape_status().get('stop_signal', False):
        final_message = f'æŠ“å–å·²åœæ­¢ï¼Œå·²å¤„ç† {processed_count} ä¸ªå•†å“'
    else:
        if failed_count > 0 or image_failed_count > 0 or index_failed_count > 0:
            final_message = (
                f'æŠ“å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªå•†å“ï¼ŒæˆåŠŸ {success_count} ä¸ªï¼Œ'
                f'å¤±è´¥ {failed_count} ä¸ª (å›¾ç‰‡å¤±è´¥ {image_failed_count} / ç´¢å¼•å¤±è´¥ {index_failed_count})'
            )
        else:
            final_message = f'æŠ“å–å®Œæˆï¼Œå…±å¤„ç† {processed_count} ä¸ªå•†å“ï¼ŒæˆåŠŸ {success_count} ä¸ª'

    db.update_scrape_status(
        is_scraping=False,
        completed=True,
        progress=100,
        processed=processed_count,
        success=success_count,
        failed=failed_count,
        image_failed=image_failed_count,
        index_failed=index_failed_count,
        failed_items=failed_items,
        message=final_message
    )
    if failed_count > 0 or image_failed_count > 0 or index_failed_count > 0:
        logger.info(
            f"âœ… åº—é“º {shop_id} æŠ“å–ä»»åŠ¡å®Œæˆ: æˆåŠŸ {success_count} / å¤±è´¥ {failed_count} / æ€»è®¡ {processed_count}"
        )
    else:
        logger.info(
            f"âœ… åº—é“º {shop_id} æŠ“å–ä»»åŠ¡å®Œæˆ: æˆåŠŸ {success_count} / æ€»è®¡ {processed_count}"
        )

    return {
        "total_products": processed_count,
        "success_count": success_count,
        "failed_count": failed_count,
        "image_failed_count": image_failed_count,
        "index_failed_count": index_failed_count
    }

def process_and_save_single_product_sync(product_info):
    """åŒæ­¥å¤„ç†å•ä¸ªå•†å“ï¼Œé¿å…é‡å¤å¤„ç†"""
    try:
        item_id = product_info.get('item_id', '')

        # === æ£€æŸ¥åœæ­¢äº‹ä»¶æˆ–åœæ­¢ä¿¡å· ===
        global scrape_stop_event
        if scrape_stop_event.is_set():
            logger.debug(f"ğŸ”´ å¤„ç†å•†å“å‰æ£€æµ‹åˆ°åœæ­¢äº‹ä»¶ï¼Œå–æ¶ˆå¤„ç†å•†å“ {item_id}")
            return {
                'status': 'cancelled',
                'item_id': item_id,
                'failed': False,
                'image_failed': False,
                'index_failed': False,
                'message': 'ä»»åŠ¡å·²å–æ¶ˆ'
            }

        current_status = db.get_scrape_status()
        if current_status.get('stop_signal', False):
            logger.debug(f"ğŸ”´ å¤„ç†å•†å“å‰æ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆå¤„ç†å•†å“ {item_id}")
            return {
                'status': 'cancelled',
                'item_id': item_id,
                'failed': False,
                'image_failed': False,
                'index_failed': False,
                'message': 'ä»»åŠ¡å·²å–æ¶ˆ'
            }

        # === 0. åŸºäºitem_idçš„å¼ºåŠ›å»é‡ ===
        if db.get_product_by_item_id(item_id):
            logger.debug(f"â­ï¸ å•†å“ {item_id} å·²å­˜åœ¨ï¼Œè·³è¿‡é‡å¤å¤„ç†")
            return {
                'status': 'skipped',
                'item_id': item_id,
                'failed': False,
                'image_failed': False,
                'index_failed': False,
                'message': 'å•†å“å·²å­˜åœ¨'
            }

        # 1. æŠ“å–è¯¦æƒ…
        from app import process_single_product  # å¼•ç”¨ app.py ä¸­çš„é€»è¾‘
        product_data = process_single_product(product_info)

        if not product_data:
            logger.warning(f"âŒ å•†å“ {item_id} æŠ“å–å¤±è´¥ï¼šæœªè·å–åˆ°å•†å“è¯¦æƒ…")
            return {
                'status': 'failed',
                'item_id': item_id,
                'failed': True,
                'image_failed': False,
                'index_failed': False,
                'message': 'æœªè·å–åˆ°å•†å“è¯¦æƒ…'
            }

        product_title = product_data.get('title', '')
        # === å†æ¬¡æ£€æŸ¥åœæ­¢çŠ¶æ€ ===
        current_status = db.get_scrape_status()
        if current_status.get('stop_signal', False):
            logger.debug(f"ğŸ”´ æŠ“å–è¯¦æƒ…åæ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå–æ¶ˆå¤„ç†å•†å“ {item_id}")
            return {
                'status': 'cancelled',
                'item_id': item_id,
                'failed': False,
                'image_failed': False,
                'index_failed': False,
                'message': 'ä»»åŠ¡å·²å–æ¶ˆ'
            }

        # 2. å†æ¬¡æŸ¥é‡ (åŒé‡ä¿é™©)
        if db.get_product_by_url(product_data['product_url']):
            logger.debug(f"â­ï¸ å•†å“URLå·²å­˜åœ¨: {product_data['product_url']}")
            return {
                'status': 'skipped',
                'item_id': item_id,
                'failed': False,
                'image_failed': False,
                'index_failed': False,
                'message': 'å•†å“å·²å­˜åœ¨'
            }

        # 3. å…¥åº“ (æ·»åŠ item_idå­—æ®µ)
        product_data['item_id'] = item_id  # ç¡®ä¿item_idè¢«ä¿å­˜
        product_id = db.insert_product(product_data)

        logger.debug(f"å•†å“ {item_id} å…¥åº“å®Œæˆï¼Œæ•°æ®åº“ID: {product_id}")

        # === å†æ¬¡æ£€æŸ¥åœæ­¢çŠ¶æ€ ===
        current_status = db.get_scrape_status()
        if current_status.get('stop_signal', False):
            logger.debug(f"ğŸ”´ å…¥åº“åæ£€æµ‹åˆ°åœæ­¢ä¿¡å·ï¼Œå•†å“ {item_id} å·²å…¥åº“ä½†è·³è¿‡å›¾ç‰‡å¤„ç†")
            return {
                'status': 'partial',
                'item_id': item_id,
                'failed': False,
                'image_failed': False,
                'index_failed': False,
                'message': 'å•†å“å·²å…¥åº“ï¼Œå›¾ç‰‡å¤„ç†è¢«å–æ¶ˆ'
            }

        # 4. å›¾ç‰‡å¤„ç†
        image_stats = {
            'total_urls': 0,
            'download_failed': 0,
            'processed': 0,
            'faiss_failed': False,
            'failed_details': []
        }
        processed_count = 0
        if product_data.get('images'):
            from app import save_product_images_unified
            processed_count, image_stats = save_product_images_unified(
                product_id,
                product_data['images'],
                shutdown_event=scrape_stop_event
            )

        image_total = len(product_data.get('images') or [])
        failed_details = image_stats.get('failed_details', [])
        download_failed = len(failed_details)

        if image_total == 0:
            logger.error(f"âŒ å•†å“ {item_id} {product_title} æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡ï¼Œå›æ»šåˆ é™¤")
            try:
                db.delete_product_images(product_id)
            except Exception as delete_error:
                logger.error(f"å›æ»šåˆ é™¤å•†å“å¤±è´¥: {delete_error}")
            return {
                'status': 'failed',
                'item_id': item_id,
                'title': product_title,
                'images_total': image_total,
                'images_processed': processed_count,
                'failed': True,
                'image_failed': True,
                'message': 'æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡'
            }

        if processed_count == 0:
            logger.error(f"âŒ å•†å“ {item_id} {product_title} æ‰€æœ‰å›¾ç‰‡è·å–å¤±è´¥ï¼Œå›æ»šåˆ é™¤")
            try:
                db.delete_product_images(product_id)
            except Exception as delete_error:
                logger.error(f"å›æ»šåˆ é™¤å•†å“å¤±è´¥: {delete_error}")
            return {
                'status': 'failed',
                'item_id': item_id,
                'title': product_title,
                'images_total': image_total,
                'images_processed': processed_count,
                'failed': True,
                'image_failed': True,
                'failed_details': failed_details,
                'message': f"30æ¬¡å°è¯•åæ‰€æœ‰ {image_total} å¼ å›¾ç‰‡å‡å¤±è´¥"
            }

        if processed_count < image_total:
            missing_indices = [str(detail.get('index')) for detail in failed_details if detail.get('index') is not None]
            warn_msg = f"éƒ¨åˆ†æˆåŠŸ: ç¼º {len(missing_indices)} å¼  (ç´¢å¼•: {','.join(missing_indices)})"
            logger.warning(f"âš ï¸ å•†å“ {item_id} {product_title} {warn_msg}ï¼Œä¿ç•™å·²è·å–æ•°æ®")
            return {
                'status': 'success',
                'item_id': item_id,
                'title': product_title,
                'images_total': image_total,
                'images_processed': processed_count,
                'download_failed': download_failed,
                'failed': False,
                'failed_details': failed_details,
                'message': warn_msg
            }

        stored_count = image_stats.get('stored', 0)
        duplicate_count = image_stats.get('duplicates', 0)
        existing_count = image_stats.get('existing', 0)

        if stored_count == image_total:
            logger.info(f"âœ… å•†å“ {item_id} {product_title} å®Œç¾æŠ“å– ({processed_count}/{image_total})")
        else:
            logger.info(
                f"âœ… å•†å“ {item_id} {product_title} æŠ“å–å®Œæˆ "
                f"(æ€» {image_total}, å†™å…¥ {stored_count}, é‡å¤ {duplicate_count}, å·²æœ‰ {existing_count})"
            )
        return {
            'status': 'success',
            'item_id': item_id,
            'title': product_title,
            'images_total': image_total,
            'images_processed': processed_count,
            'failed': False,
            'failed_details': failed_details
        }
    except Exception as e:
        logger.error(f"âŒ å¤„ç†å•†å“å‡ºé”™ {product_info.get('item_id')}: {e}")
        return {
            'status': 'failed',
            'item_id': product_info.get('item_id'),
            'failed': True,
            'image_failed': False,
            'index_failed': False,
            'message': str(e)
        }

def scrape_product_info(product_url):
    """æ ¹æ®å•†å“URLè·å–å•†å“è¯¦ç»†ä¿¡æ¯"""
    try:
        from weidian_scraper import get_weidian_scraper

        scraper = get_weidian_scraper()
        product_info = scraper.scrape_product_info(product_url)

        if product_info:
            # é‡æ–°æ ¼å¼åŒ–è¿”å›æ•°æ®
            return {
                'title': product_info.get('title', ''),
                'description': product_info.get('description', ''),
                # ä¿®å¤ï¼šç§»é™¤ [:5] é™åˆ¶ï¼Œè¿”å›æ‰€æœ‰æŠ“å–åˆ°çš„å›¾ç‰‡
                'images': product_info.get('images', []),
                'shop_name': product_info.get('shop_name', '')
            }

        return None

    except Exception as e:
        logger.error(f'è·å–å•†å“è¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}')
        return None

def generate_acbuy_url(weidian_url):
    """ç”ŸæˆAcBuyé“¾æ¥"""
    if not weidian_url:
        return ''

    try:
        import re
        item_id_match = re.search(r'itemID=(\d+)', weidian_url)
        if item_id_match:
            item_id = item_id_match.group(1)
            # æ„å»ºacbuyé“¾æ¥
            encoded_url = weidian_url.replace(':', '%3A').replace('/', '%2F').replace('?', '%3F').replace('=', '%3D').replace('&', '%26')
            return f'https://www.acbuy.com/product?url={encoded_url}&id={item_id}&source=WD'
    except Exception as e:
        logger.error(f'ç”ŸæˆAcBuyé“¾æ¥å¤±è´¥: {e}')

    return ''

def generate_cnfans_url(item_id):
    """ç”ŸæˆCNFansé“¾æ¥"""
    if not item_id:
        return ''
    return f"https://cnfans.com/product?id={item_id}&platform=WEIDIAN"

def generate_english_title(chinese_title):
    """å°†ä¸­æ–‡æ ‡é¢˜ç¿»è¯‘ä¸ºè‹±æ–‡æ ‡é¢˜"""
    if not chinese_title:
        return ''

    try:
        import re
        import requests

        # é¦–å…ˆå°è¯•æå–å·²æœ‰çš„è‹±æ–‡éƒ¨åˆ†
        english_parts = re.findall(r'[a-zA-Z\s]+', chinese_title)
        if english_parts and len(' '.join(english_parts).strip()) > 5:
            # å¦‚æœè‹±æ–‡éƒ¨åˆ†è¶³å¤Ÿé•¿ï¼Œç›´æ¥è¿”å›
            return ' '.join(english_parts).strip()

        # å“ç‰Œåç§°æ˜ å°„ï¼ˆæ‰©å±•ç‰ˆï¼‰
        brand_mappings = {
            'Nike': 'Nike', 'é˜¿è¿ª': 'Adidas', 'Adidas': 'Adidas', 'æå®': 'LiNing',
            'å®‰è¸': 'Anta', 'åŒ¹å…‹': 'Peak', 'ä¹”ä¸¹': 'Jordan', 'New Balance': 'New Balance',
            'Converse': 'Converse', 'Vans': 'Vans', 'Supreme': 'Supreme', 'BAPE': 'BAPE',
            'Palace': 'Palace', 'Stone Island': 'Stone Island', 'Off-White': 'Off-White',
            'Balenciaga': 'Balenciaga', 'Gucci': 'Gucci', 'Louis Vuitton': 'Louis Vuitton',
            'Chanel': 'Chanel', 'Dior': 'Dior', 'Yeezy': 'Yeezy', 'Puma': 'Puma',
            'Reebok': 'Reebok', 'Under Armour': 'Under Armour', 'Fila': 'Fila',
            'The North Face': 'The North Face', 'Columbia': 'Columbia', 'Patagonia': 'Patagonia',
            'Arc\'teryx': 'Arc\'teryx', 'Canada Goose': 'Canada Goose', 'Moncler': 'Moncler',
            'Burberry': 'Burberry', 'Prada': 'Prada', 'Versace': 'Versace', 'Fendi': 'Fendi',
            'Hermes': 'Hermes', 'Rolex': 'Rolex', 'Cartier': 'Cartier', 'Omega': 'Omega',
            'IWC': 'IWC', 'Jaeger-LeCoultre': 'Jaeger-LeCoultre', 'Patek Philippe': 'Patek Philippe'
        }

        # åº”ç”¨å“ç‰Œæ˜ å°„
        title = chinese_title
        for zh, en in brand_mappings.items():
            title = title.replace(zh, en)

        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸­æ–‡å­—ç¬¦
        has_chinese = any('\u4e00' <= char <= '\u9fff' for char in title)

        if has_chinese:
            # ä½¿ç”¨ç™¾åº¦ç¿»è¯‘APIæˆ–å…¶ä»–å…è´¹ç¿»è¯‘æœåŠ¡
            try:
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„ç¿»è¯‘APIç¤ºä¾‹
                # å®é™…éƒ¨ç½²æ—¶éœ€è¦æ›¿æ¢ä¸ºç¨³å®šçš„ç¿»è¯‘æœåŠ¡
                api_url = "https://api.mymemory.translated.net/get"
                params = {
                    'q': chinese_title,
                    'langpair': 'zh-CN|en-US',
                    'de': 'your-email@example.com'  # MyMemoryè¦æ±‚æä¾›é‚®ç®±
                }

                response = requests.get(api_url, params=params, timeout=5, proxies={'http': None, 'https': None})
                if response.status_code == 200:
                    data = response.json()
                    translated = data.get('responseData', {}).get('translatedText', '')
                    if translated and translated != chinese_title:
                        # æ¸…ç†ç¿»è¯‘ç»“æœ
                        translated = re.sub(r'[^\w\s\-]', '', translated)
                        return translated.strip()

            except Exception as e:
                logger.warning(f'åœ¨çº¿ç¿»è¯‘å¤±è´¥: {e}')

            # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œè¿”å›æå–çš„è‹±æ–‡éƒ¨åˆ†æˆ–åŸæ ‡é¢˜
            english_parts = re.findall(r'[a-zA-Z\s\-]+', title)
            if english_parts:
                result = ' '.join(english_parts).strip()
                if len(result) > 3:
                    return result

        # å¦‚æœæ²¡æœ‰ä¸­æ–‡æˆ–ç¿»è¯‘å¤±è´¥ï¼Œè¿”å›å¤„ç†åçš„æ ‡é¢˜
        return re.sub(r'[^\w\s\-]', '', title).strip()

    except Exception as e:
        logger.error(f'ç”Ÿæˆè‹±æ–‡æ ‡é¢˜å¤±è´¥: {e}')
        return chinese_title

def process_single_product(product_info):
    """å¤„ç†å•ä¸ªå•†å“çš„è¯¦æƒ…æŠ“å–"""
    try:
        item_id = product_info['item_id']
        item_url = product_info['item_url']
        shop_name = product_info['shop_name']

        # æ£€æŸ¥åœæ­¢äº‹ä»¶
        global scrape_stop_event
        if scrape_stop_event.is_set():
            logger.info(f"ğŸ”´ å¤„ç†å•†å“ {item_id} æ—¶æ£€æµ‹åˆ°åœæ­¢äº‹ä»¶ï¼Œä¸­æ­¢å¤„ç†")
            return None

        # è·å–å•†å“è¯¦ç»†ä¿¡æ¯
        product_details = scrape_product_info(item_url)

        if product_details:
            # ç”Ÿæˆè‹±æ–‡æ ‡é¢˜
            english_title = generate_english_title(product_details.get('title', ''))

            # ä¼˜å…ˆä½¿ç”¨ä»å•†å“è¯¦æƒ…ä¸­è·å–çš„åº—é“ºåç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼ å…¥çš„
            actual_shop_name = product_details.get('shop_name', '') or shop_name

            return {
                'product_url': item_url,
                'title': product_details.get('title', ''),
                'description': product_details.get('description', ''),
                'english_title': english_title,
                'cnfans_url': generate_cnfans_url(item_id),
                'acbuy_url': generate_acbuy_url(item_url),
                'shop_name': actual_shop_name,
                'images': product_details.get('images', []),
                'ruleEnabled': True
            }
        return None

    except Exception as e:
        logger.error(f'å¤„ç†å•†å“å¤±è´¥: {e}')
        return None

def process_products_multithreaded(products_list):
    """å¤šçº¿ç¨‹å¤„ç†å•†å“è¯¦æƒ…æŠ“å–"""
    import concurrent.futures

    processed_products = []

    # è·å–é…ç½®çš„çº¿ç¨‹æ•°
    max_workers = config.DOWNLOAD_THREADS

    logger.info(f'å¼€å§‹å¤šçº¿ç¨‹å¤„ç† {len(products_list)} ä¸ªå•†å“ï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹')

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_product = {
            executor.submit(process_single_product, product): product
            for product in products_list
        }

        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_product):
            try:
                result = future.result()
                if result:
                    processed_products.append(result)
            except Exception as e:
                logger.error(f'å•†å“å¤„ç†ä»»åŠ¡å¤±è´¥: {e}')

    logger.info(f'å¤šçº¿ç¨‹å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(processed_products)} ä¸ªå•†å“')
    return processed_products

def save_product_images_unified(product_id, image_urls, max_workers=None, shutdown_event=None):
    """
    ã€æœ€ç»ˆå¢å¼ºç‰ˆã€‘ç»Ÿä¸€æ‰¹é‡å›¾ç‰‡å¤„ç†
    ç‰¹æ€§ï¼š
    1. æ·±åº¦æ ¡éªŒå›¾ç‰‡æ–‡ä»¶å¤´ï¼ˆé˜²æ­¢ 0 å­—èŠ‚æ–‡ä»¶æˆ– HTML ä¼ªè£…æˆå›¾ç‰‡ï¼‰ã€‚
    2. ç‰¹å¾æå–å¤±è´¥ç›´æ¥è§†ä¸ºè‡´å‘½é”™è¯¯ï¼ˆé¿å…å¯¹åå›¾æ— æ•ˆé‡è¯•ï¼‰ã€‚
    3. HTTP 404/403 è§†ä¸ºè‡´å‘½é”™è¯¯ã€‚
    3. è¿”å›è¯¦ç»†çš„å¤±è´¥åŸå› æŠ¥å‘Šã€‚
    """
    import time
    import concurrent.futures
    import requests
    import os
    import random
    from PIL import Image, UnidentifiedImageError

    stats = {
        'total_urls': len(image_urls),
        'processed': 0,
        'stored': 0,
        'duplicates': 0,
        'existing': 0,
        'fatal': 0,
        'retry_failed': 0,
        'failed_details': [],
        'download_failed': 0,
        'faiss_failed': False
    }

    if not image_urls:
        return 0, stats

    try:
        existing_images = db.get_product_images(product_id)
        existing_indices = {img['image_index'] for img in existing_images}
        existing_feats = [img['features'] for img in existing_images if img.get('features') is not None]
    except Exception:
        existing_indices = set()
        existing_feats = []

    pending_items = []
    for idx, url in enumerate(image_urls):
        if idx not in existing_indices:
            pending_items.append((idx, url))
        else:
            stats['processed'] += 1
            stats['existing'] += 1

    if not pending_items:
        return stats['processed'], stats

    max_retries = 30

    def verify_image_file(file_path):
        try:
            if os.path.getsize(file_path) < 100:
                return False, "æ–‡ä»¶è¿‡å°"
            with Image.open(file_path) as img:
                img.verify()
            return True, None
        except UnidentifiedImageError:
            return False, "æ— æ³•è¯†åˆ«çš„å›¾ç‰‡æ ¼å¼"
        except Exception as e:
            return False, f"å›¾ç‰‡æ–‡ä»¶æŸå: {str(e)}"

    def process_batch(items_to_process):
        current_downloaded = []
        retry_list = []
        fatal_list = []

        workers = min(getattr(config, 'DOWNLOAD_THREADS', 8), len(items_to_process))

        def download_task(item):
            idx, url = item
            if shutdown_event and shutdown_event.is_set():
                return ('RETRY', item, 'Cancelled')

            save_dir = os.path.join(config.IMAGE_SAVE_DIR, str(product_id))
            os.makedirs(save_dir, exist_ok=True)
            timestamp = int(time.time() * 1000000)
            filename = f"{product_id}_{idx}_{timestamp}.jpg"
            save_path = os.path.join(save_dir, filename)

            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://weidian.com/'
                }

                resp = requests.get(
                    url,
                    timeout=(10, 20),
                    proxies={'http': None, 'https': None},
                    headers=headers
                )

                if resp.status_code in [403, 404]:
                    return ('FATAL', item, f"HTTP {resp.status_code} (æ­»é“¾æ¥)")

                if resp.status_code != 200:
                    return ('RETRY', item, f"HTTP {resp.status_code}")

                with open(save_path, 'wb') as f:
                    f.write(resp.content)

                is_valid, reason = verify_image_file(save_path)
                if not is_valid:
                    try:
                        os.remove(save_path)
                    except Exception:
                        pass
                    if reason and "æ— æ³•è¯†åˆ«" in reason:
                        return ('FATAL', item, f"æ— æ•ˆå›¾ç‰‡: {reason}")
                    return ('RETRY', item, f"æ–‡ä»¶æŸå: {reason}")

                return ('SUCCESS', (idx, save_path), None)
            except Exception as e:
                return ('RETRY', item, str(e))

        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            futures = [executor.submit(download_task, item) for item in items_to_process]
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if not result:
                    continue
                status, data, error = result
                if status == 'SUCCESS':
                    current_downloaded.append(data)
                elif status == 'FATAL':
                    fatal_list.append((data, error))
                else:
                    retry_list.append(data)

        if not current_downloaded:
            return [], retry_list, fatal_list

        extractor = get_global_feature_extractor()
        if extractor is None:
            for _, path in current_downloaded:
                try:
                    os.remove(path)
                except Exception:
                    pass
            retry_list.extend([(idx, image_urls[idx]) for idx, _ in current_downloaded])
            return [], retry_list, fatal_list

        processed_indices = []
        vectors_to_add = []

        for index, save_path in current_downloaded:
            if shutdown_event and shutdown_event.is_set():
                retry_list.append((index, image_urls[index]))
                continue

            try:
                features = None
                with GLOBAL_AI_SEMAPHORE:
                    try:
                        features = extractor.extract_feature(save_path)
                    except Exception as e:
                        logger.error(f"ç‰¹å¾æå–åº•å±‚é”™è¯¯: {e}")
                        features = None

                if features is None:
                    try:
                        os.remove(save_path)
                    except Exception:
                        pass
                    fatal_list.append(((index, image_urls[index]), "AIæ— æ³•æå–ç‰¹å¾(å›¾ç‰‡å¯èƒ½æŸå)"))
                    continue

                is_dup, score = check_duplicate_image(features, existing_feats, threshold=0.995)
                if is_dup:
                    try:
                        os.remove(save_path)
                    except Exception:
                        pass
                    stats['duplicates'] += 1
                    processed_indices.append(index)
                    continue

                existing_feats.append(features)
                img_db_id = db.insert_image_record(product_id, save_path, index, features)

                if img_db_id:
                    vectors_to_add.append((img_db_id, features))
                    processed_indices.append(index)
                    stats['stored'] += 1
                else:
                    retry_list.append((index, image_urls[index]))

            except Exception as e:
                logger.error(f"å¤„ç†å›¾ç‰‡ {index} å¼‚å¸¸: {e}")
                retry_list.append((index, image_urls[index]))

        if vectors_to_add:
            try:
                from vector_engine import get_vector_engine
                engine = get_vector_engine()
                with faiss_lock:
                    for img_id, feats in vectors_to_add:
                        engine.add_vector(img_id, feats)
                    engine.save()
            except Exception as e:
                logger.error(f"FAISS å†™å…¥å¤±è´¥: {e}")
                stats['faiss_failed'] = True

        return processed_indices, retry_list, fatal_list

    logger.info(f"ğŸš€ [å•†å“ {product_id}] å¼€å§‹å¤„ç†ï¼Œå¾…å¤„ç† {len(pending_items)} å¼ ")

    for attempt in range(1, max_retries + 1):
        if not pending_items:
            break
        if shutdown_event and shutdown_event.is_set():
            break

        success_indices, retry_items, fatal_items = process_batch(pending_items)

        stats['processed'] += len(success_indices)

        for item, reason in fatal_items:
            idx, url = item
            logger.warning(f"âŒ [å•†å“ {product_id}] å›¾ç‰‡ {idx} å‘ç”Ÿè‡´å‘½é”™è¯¯ï¼Œæ”¾å¼ƒ: {reason}")
            stats['fatal'] += 1
            stats['failed_details'].append({
                'index': idx,
                'url': url,
                'reason': reason
            })

        pending_items = retry_items

        if not pending_items:
            break

        if attempt <= 5:
            sleep_time = random.uniform(1, 2)
        elif attempt <= 15:
            sleep_time = random.uniform(3, 5)
        else:
            sleep_time = random.uniform(5, 10)

        logger.warning(
            f"âš ï¸ [å•†å“ {product_id}] ç¬¬ {attempt} è½®ç»“æŸï¼Œ{len(pending_items)} å¼ éœ€é‡è¯•ï¼Œ{len(fatal_items)} å¼ å·²æ”¾å¼ƒã€‚ç­‰å¾… {sleep_time:.1f}s..."
        )
        time.sleep(sleep_time)

    for idx, url in pending_items:
        stats['failed_details'].append({
            'index': idx,
            'url': url,
            'reason': 'è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° (å¯èƒ½ç½‘ç»œè¶…æ—¶)'
        })

    stats['download_failed'] = len(stats['failed_details'])
    stats['retry_failed'] = len(pending_items)

    if stats['download_failed'] > 0:
        logger.error(f"âŒ [å•†å“ {product_id}] æœ€ç»ˆç»“æœ: {stats['processed']} æˆåŠŸ, {stats['download_failed']} å¤±è´¥")
    else:
        logger.info(f"âœ… [å•†å“ {product_id}] å…¨éƒ¨å¤„ç†æˆåŠŸ")

    logger.info(
        f"ğŸ§¾ [å•†å“ {product_id}] å›¾ç‰‡ç»Ÿè®¡: total={stats['total_urls']}, stored={stats['stored']}, "
        f"existing={stats['existing']}, duplicate={stats['duplicates']}, fatal={stats['fatal']}, "
        f"retry_failed={stats['retry_failed']}"
    )

    return stats['processed'], stats

def run_cleanup_task():
    """åå°æ¸…ç†ä»»åŠ¡ï¼Œå®šæœŸæ¸…ç†æ•°æ®åº“å’Œå†…å­˜ä¸­çš„è¿‡æœŸè®°å½•"""
    while True:
        try:
            # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
            time.sleep(3600)
            logger.info("âš™ï¸ å¼€å§‹æ‰§è¡Œåå°æ¸…ç†ä»»åŠ¡...")

            # 1. æ¸…ç†å·²å¤„ç†çš„æ¶ˆæ¯IDè¡¨
            db.cleanup_processed_messages()
            logger.info("âœ… å·²æ¸…ç†è¿‡æœŸçš„æ¶ˆæ¯IDè®°å½•")

            # 2. æ¸…ç†å†…å­˜ä¸­çš„å†·å´è®°å½•
            try:
                from bot import cleanup_expired_cooldowns
                cleanup_expired_cooldowns()
                logger.info("âœ… å·²æ¸…ç†å†…å­˜ä¸­è¿‡æœŸçš„å†·å´çŠ¶æ€")
            except ImportError:
                logger.warning("æ— æ³•å¯¼å…¥botæ¨¡å—è¿›è¡Œå†·å´æ¸…ç†ï¼Œè·³è¿‡")

        except Exception as e:
            logger.error(f"åå°æ¸…ç†ä»»åŠ¡å¼‚å¸¸: {e}")

if __name__ == '__main__':
    # ã€Windowså…¼å®¹æ€§ä¿®å¤ã€‘å¿…é¡»åœ¨æœ€å¼€å§‹è°ƒç”¨
    multiprocessing.freeze_support()

    import atexit
    import signal
    import time

    # ã€æ ¸å¿ƒä¿®å¤ã€‘åªåœ¨ä¸»è¿›ç¨‹æ‰§è¡Œåˆå§‹åŒ–
    initialize_runtime()

    # å…¨å±€å˜é‡ç”¨äºæ§åˆ¶ä¼˜é›…å…³é—­
    import threading
    shutdown_event = threading.Event()

    def signal_handler(signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·ï¼Œä¼˜é›…å…³é—­"""
        print(f"\nğŸ›‘ Received signal {signum}, initiating graceful shutdown...")
        shutdown_event.set()

        # è®¾ç½®æŠ“å–çŠ¶æ€ä¸ºåœæ­¢
        current_status = db.get_scrape_status()
        if current_status.get('is_scraping', False):
            db.update_scrape_status(
                stop_signal=True,
                message='ç³»ç»Ÿæ­£åœ¨å…³é—­ï¼Œå·²åœæ­¢æŠ“å–ä»»åŠ¡'
            )
            print("â¹ï¸  å·²åœæ­¢æ‰€æœ‰æŠ“å–ä»»åŠ¡")

        # ç­‰å¾…æŠ“å–çº¿ç¨‹ç»“æŸï¼ˆæœ€å¤šç­‰å¾…10ç§’ï¼‰
        global current_scrape_thread, scrape_thread_lock
        with scrape_thread_lock:
            if current_scrape_thread and current_scrape_thread.is_alive():
                print("â³ ç­‰å¾…æŠ“å–çº¿ç¨‹ç»“æŸ...")
                current_scrape_thread.join(timeout=10.0)
                if current_scrape_thread.is_alive():
                    print("âš ï¸ æŠ“å–çº¿ç¨‹æœªèƒ½åœ¨10ç§’å†…ç»“æŸ")
                else:
                    print("âœ… æŠ“å–çº¿ç¨‹å·²ç»“æŸ")

        # ç«‹å³åœæ­¢Discordæœºå™¨äºº
        stop_discord_bot()

        # çŸ­æš‚ç­‰å¾…è®©å…¶ä»–çº¿ç¨‹æœ‰æœºä¼šæ¸…ç†
        time.sleep(0.2)
        print("ğŸ’¥ Force exiting...")
        import os
        os._exit(0)

    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # æ³¨å†Œé€€å‡ºæ—¶åœæ­¢æœºå™¨äººçš„å‡½æ•°
    atexit.register(stop_discord_bot)

    # å¯åŠ¨ Flask æœåŠ¡
    print("ğŸš€ æœåŠ¡å¯åŠ¨ä¸­...")
    try:
        # å…³é—­ debug æ¨¡å¼ï¼Œé¿å… Flask é‡è½½å™¨å¯¼è‡´åŒé‡åˆå§‹åŒ–
        # ã€å…³é”®ä¿®æ”¹ã€‘æ·»åŠ  use_reloader=False ç¦ç”¨Flaské‡è½½å™¨ï¼Œé¿å…åŒé‡è¿›ç¨‹
        app.run(host='0.0.0.0', port=5001, debug=False, threaded=True, use_reloader=False)
    except KeyboardInterrupt:
        print("\nğŸ›‘ Received KeyboardInterrupt, shutting down...")
        signal_handler(signal.SIGINT, None)
    except Exception as e:
        print(f"\nğŸ’¥ Unexpected error: {e}")
        signal_handler(signal.SIGINT, None)
    finally:
        print("ğŸ‘‹ Flask API shutdown complete")
