import faiss
import numpy as np
import os
import pickle
import logging
import json
from typing import List, Dict, Tuple
try:
    from .config import config
except ImportError:
    from config import config

logger = logging.getLogger(__name__)

class VectorEngine:
    """
    FAISS HNSWå‘é‡æœç´¢å¼•æ“
    çº¯æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ï¼Œæ— éœ€Docker
    æ”¯æŒç™¾ä¸‡çº§å‘é‡æ¯«ç§’çº§æŸ¥è¯¢
    """

    def __init__(self, index_file=None, id_map_file=None):
        self.index_file = index_file or config.FAISS_INDEX_FILE
        self.id_map_file = id_map_file or config.FAISS_ID_MAP_FILE

        self.dimension = config.VECTOR_DIMENSION
        self.index = None

        # FAISSåªèƒ½å­˜æ•´æ•°IDï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ˜ å°„ï¼šFAISSå†…éƒ¨ID -> æ•°æ®åº“(product_imagesè¡¨çš„ID)
        # è¿™ä¸ªåˆ—è¡¨çš„ç´¢å¼•æ˜¯FAISS IDï¼Œå€¼æ˜¯æ•°æ®åº“ID
        self.id_map = []

        self._load_or_create_index()

    def _load_or_create_index(self):
        """åŠ è½½æˆ–åˆ›å»ºFAISS HNSWç´¢å¼•"""
        if os.path.exists(self.index_file) and os.path.exists(self.id_map_file):
            logger.info("æ­£åœ¨åŠ è½½FAISSç´¢å¼•...")
            try:
                self.index = faiss.read_index(self.index_file)
                with open(self.id_map_file, 'rb') as f:
                    self.id_map = pickle.load(f)
                logger.info(f"âœ… FAISSç´¢å¼•åŠ è½½å®Œæˆï¼Œå½“å‰åŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
            except Exception as e:
                logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•: {e}")
                self._create_new_index()
        else:
            logger.info("åˆ›å»ºæ–°çš„FAISS HNSWç´¢å¼•...")
            self._create_new_index()

    def _create_new_index(self):
        """åˆ›å»ºæ–°çš„FAISS HNSWç´¢å¼•ï¼Œä¼˜åŒ–å‚æ•°è®¾ç½®"""
        logger.info("åˆ›å»ºæ–°çš„FAISS HNSWç´¢å¼•...")

        # HNSW64: å›¾ç»“æ„ï¼ŒæŸ¥è¯¢æå¿«ï¼Œå‡†ç¡®ç‡é«˜
        # InnerProduct (IP) åœ¨å½’ä¸€åŒ–å‘é‡ä¸Šç­‰åŒäºä½™å¼¦ç›¸ä¼¼åº¦
        self.index = faiss.IndexHNSWFlat(
            self.dimension,
            config.FAISS_HNSW_M,
            faiss.METRIC_INNER_PRODUCT
        )

        # è®¾ç½®æ„å»ºå‚æ•° (å…¼å®¹ä¸åŒç‰ˆæœ¬çš„FAISS)
        ef_construction_set = False
        ef_search_set = False

        try:
            # å°è¯•è®¾ç½®HNSWå‚æ•° (æ–°ç‰ˆæœ¬FAISS >= 1.7.0)
            if hasattr(self.index, 'efConstruction'):
                self.index.efConstruction = config.FAISS_EF_CONSTRUCTION  # æ„å»ºæ—¶çš„æ·±åº¦ï¼Œè¶Šé«˜è¶Šå‡†ä½†æ„å»ºè¶Šæ…¢
                ef_construction_set = True
                logger.info(f"è®¾ç½®efConstruction = {config.FAISS_EF_CONSTRUCTION}")

            if hasattr(self.index, 'efSearch'):
                self.index.efSearch = config.FAISS_EF_SEARCH  # æœç´¢æ—¶çš„æ·±åº¦ï¼Œè¶Šé«˜è¶Šå‡†ä½†æœç´¢è¶Šæ…¢
                ef_search_set = True
                logger.info(f"è®¾ç½®efSearch = {config.FAISS_EF_SEARCH}")

        except AttributeError:
            logger.warning("FAISSç‰ˆæœ¬ä¸æ”¯æŒefConstruction/efSearchå‚æ•°ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")

        # å¦‚æœæ— æ³•è®¾ç½®å‚æ•°ï¼Œæä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if not ef_construction_set or not ef_search_set:
            logger.info("ğŸ’¡ FAISSæ€§èƒ½ä¼˜åŒ–å»ºè®®:")
            logger.info(f"   - å½“å‰FAISSç‰ˆæœ¬: {faiss.__version__}")
            logger.info("   - å»ºè®®å‡çº§åˆ°FAISS >= 1.7.0ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
            logger.info("   - æˆ–è€…ä½¿ç”¨: pip install faiss-cpu --upgrade")

        # è®¾ç½®å…¶ä»–å…¼å®¹æ€§å‚æ•°
        try:
            # è®¾ç½®HNSWçš„Må‚æ•° (å¦‚æœæ”¯æŒ)
            if hasattr(self.index, 'hnsw'):
                logger.info(f"HNSW Må‚æ•° = {config.FAISS_HNSW_M}")
        except:
            pass

        self.id_map = []

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.index_file), exist_ok=True)

        logger.info("âœ… FAISS HNSWç´¢å¼•åˆ›å»ºå®Œæˆ")

    def save(self):
        """ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜ (ç™¾ä¸‡çº§æ•°æ®ä¿å­˜å¤§çº¦éœ€è¦å‡ ç§’)"""
        try:
            faiss.write_index(self.index, self.index_file)
            with open(self.id_map_file, 'wb') as f:
                pickle.dump(self.id_map, f)
            logger.info("FAISSç´¢å¼•å·²ä¿å­˜åˆ°ç£ç›˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")

    def add_vector(self, db_id: int, vector: np.ndarray) -> bool:
        """æ·»åŠ å‘é‡åˆ°FAISSç´¢å¼•"""
        try:
            # ç¡®ä¿å‘é‡æ˜¯æ­£ç¡®çš„å½¢çŠ¶å’Œç±»å‹
            if isinstance(vector, list):
                vector = np.array(vector, dtype='float32')
            elif vector.dtype != np.float32:
                vector = vector.astype('float32')

            vector = vector.reshape(1, -1)  # ç¡®ä¿æ˜¯[1, dim]å½¢çŠ¶

            # æ·»åŠ åˆ°FAISS
            self.index.add(vector)

            # è®°å½•IDæ˜ å°„ï¼šFAISSå†…éƒ¨ID -> æ•°æ®åº“ID
            faiss_id = self.index.ntotal - 1  # æ–°æ·»åŠ çš„å‘é‡ID
            if len(self.id_map) <= faiss_id:
                self.id_map.extend([None] * (faiss_id - len(self.id_map) + 1))
            self.id_map[faiss_id] = db_id

            return True

        except Exception as e:
            logger.error(f"æ·»åŠ å‘é‡å¤±è´¥: {e}")
            return False

    def search(self, query_vector: np.ndarray, top_k: int = 1) -> List[Dict]:
        """æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡"""
        import time
        start_time = time.time()

        if self.index.ntotal == 0:
            logger.info("FAISSç´¢å¼•ä¸ºç©ºï¼Œè·³è¿‡æœç´¢")
            return []

        try:
            # ç¡®ä¿æŸ¥è¯¢å‘é‡æ ¼å¼æ­£ç¡®
            if isinstance(query_vector, list):
                query_vector = np.array(query_vector, dtype='float32')
            elif query_vector.dtype != np.float32:
                query_vector = query_vector.astype('float32')

            query_vector = query_vector.reshape(1, -1)

            logger.info(f"å¼€å§‹FAISSæœç´¢ï¼Œç´¢å¼•å¤§å°: {self.index.ntotal}, top_k: {top_k}")

            # æ‰§è¡Œæœç´¢
            # å¼ºåˆ¶ä½¿ç”¨å•çº¿ç¨‹è¿›è¡Œæœç´¢ï¼Œé˜²æ­¢åœ¨ Flask/MacOS ç¯å¢ƒä¸‹å‘ç”Ÿ OpenMP æ­»é”
            faiss.omp_set_num_threads(1)
            search_start = time.time()
            distances, indices = self.index.search(query_vector, top_k)
            search_time = time.time() - search_start
            logger.info(f"FAISSæœç´¢å®Œæˆï¼Œè€—æ—¶: {search_time:.3f}ç§’")

            results = []
            for i in range(min(top_k, len(indices[0]))):
                faiss_id = indices[0][i]
                score = distances[0][i]

                if faiss_id != -1 and faiss_id < len(self.id_map) and self.id_map[faiss_id] is not None:
                    db_id = self.id_map[faiss_id]
                    results.append({
                        'db_id': db_id,
                        'score': float(score)
                    })

            total_time = time.time() - start_time
            logger.info(f"æœç´¢æ€»è€—æ—¶: {total_time:.3f}ç§’, è¿”å›{len(results)}ä¸ªç»“æœ")
            return results

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []

    def remove_vector_by_db_id(self, db_id: int) -> bool:
        """
        ä»FAISSç´¢å¼•ä¸­åˆ é™¤å‘é‡ã€‚ç”±äºFAISSä¸æ”¯æŒç›´æ¥åˆ é™¤å•ä¸ªå‘é‡ï¼Œ
        æˆ‘ä»¬æ ‡è®°åˆ é™¤å¹¶å®šæœŸé‡å»ºç´¢å¼•ï¼ˆæ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ï¼‰ã€‚
        """
        try:
            # æ ‡è®°è¦åˆ é™¤çš„å‘é‡
            vector_removed = False
            for i, mapped_id in enumerate(self.id_map):
                if mapped_id == db_id:
                    self.id_map[i] = None
                    vector_removed = True
                    logger.info(f"æ ‡è®°å‘é‡åˆ é™¤: db_id={db_id}, faiss_id={i}")
                    break

            # æ€§èƒ½ä¼˜åŒ–ï¼šä¸ç«‹å³é‡å»ºç´¢å¼•ï¼Œåªä¿å­˜çŠ¶æ€
            # åªæœ‰å½“åˆ é™¤çš„å‘é‡æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼æ—¶æ‰é‡å»º
            if vector_removed:
                deleted_count = sum(1 for id_val in self.id_map if id_val is None)
                total_count = len(self.id_map)
                deletion_ratio = deleted_count / total_count if total_count > 0 else 0

                # å¦‚æœåˆ é™¤æ¯”ä¾‹è¶…è¿‡30%ï¼Œåˆ™é‡å»ºç´¢å¼•æ¸…ç†ç¢ç‰‡
                if deletion_ratio > 0.3:
                    logger.info(f"åˆ é™¤æ¯”ä¾‹({deletion_ratio:.1%})è¿‡é«˜ï¼Œé‡å»ºç´¢å¼•æ¸…ç†ç¢ç‰‡")
                    self._rebuild_index_after_removal()
                else:
                    # åªä¿å­˜ç´¢å¼•çŠ¶æ€ï¼Œä¸é‡å»º
                    self.save()

            return True
        except Exception as e:
            logger.error(f"åˆ é™¤å‘é‡å¤±è´¥: {e}")
            return False

    def _rebuild_index_after_removal(self):
        """åˆ é™¤å‘é‡åé‡å»ºç´¢å¼•ï¼ˆä¼˜åŒ–ç‰ˆï¼šç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­å·²å­˜çš„ featuresï¼Œä¸é‡æ–°è·‘æ¨¡å‹ï¼‰"""
        try:
            try:
                from database import db
            except ImportError:
                from .database import db

            valid_vectors = []

            # åªä¿ç•™é‚£äº›ä»ç„¶â€œæœªè¢«æ ‡è®°åˆ é™¤â€çš„ db_id
            alive_db_ids = {mapped_id for mapped_id in self.id_map if mapped_id is not None}

            with db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT id, features FROM product_images WHERE id IS NOT NULL AND features IS NOT NULL")
                for row in cursor.fetchall():
                    img_id = row['id']
                    if img_id not in alive_db_ids:
                        continue

                    features_str = row['features']
                    try:
                        vec = np.array(json.loads(features_str), dtype='float32')
                        if vec.shape[0] != self.dimension:
                            continue
                        valid_vectors.append((img_id, vec))
                    except Exception:
                        continue

            # é‡å»ºç´¢å¼•
            self._create_new_index()
            for img_id, vec in valid_vectors:
                self.add_vector(img_id, vec)

            self.save()
            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŒ…å« {len(valid_vectors)} ä¸ªå‘é‡")

        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")

    def rebuild_index(self, vectors_data: List[Tuple[int, np.ndarray]]) -> bool:
        """
        é‡å»ºæ•´ä¸ªç´¢å¼• (ç”¨äºæ¸…ç†å·²åˆ é™¤çš„å‘é‡æˆ–æ‰¹é‡æ›´æ–°)

        vectors_data: [(db_id, vector), ...]
        """
        try:
            logger.info("å¼€å§‹é‡å»ºFAISSç´¢å¼•...")

            # åˆ é™¤æ—§çš„ç´¢å¼•æ–‡ä»¶
            try:
                if os.path.exists(self.index_file):
                    os.remove(self.index_file)
                if os.path.exists(self.id_map_file):
                    os.remove(self.id_map_file)
            except Exception as e:
                logger.warning(f"åˆ é™¤æ—§ç´¢å¼•æ–‡ä»¶å¤±è´¥: {e}")

            # åˆ›å»ºæ–°ç´¢å¼•
            self._create_new_index()

            # é‡æ–°æ·»åŠ æ‰€æœ‰å‘é‡
            for db_id, vector in vectors_data:
                self.add_vector(db_id, vector)

            # ç«‹å³ä¿å­˜æ–°ç´¢å¼•
            self.save()

            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆï¼ŒåŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
            return True

        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {e}")
            # å°è¯•é‡æ–°åŠ è½½æ—§ç´¢å¼•
            try:
                self._load_or_create_index()
            except:
                pass
            return False

    def count(self) -> int:
        """è¿”å›å½“å‰ç´¢å¼•ä¸­çš„å‘é‡æ•°é‡"""
        return self.index.ntotal

    def get_stats(self) -> Dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
        ef_construction = getattr(self.index, 'efConstruction', 'ä¸æ”¯æŒ')
        ef_search = getattr(self.index, 'efSearch', 'ä¸æ”¯æŒ')

        return {
            'total_vectors': self.index.ntotal,
            'dimension': self.dimension,
            'index_type': 'HNSW',
            'metric_type': 'InnerProduct (Cosine)',
            'ef_construction': ef_construction,
            'ef_search': ef_search,
            'memory_usage_mb': self._estimate_memory_usage(),
            'faiss_version': faiss.__version__,
            'performance_tips': self._get_performance_tips()
        }

    def _get_performance_tips(self) -> List[str]:
        """è·å–æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        tips = []

        # æ£€æŸ¥FAISSç‰ˆæœ¬
        try:
            version_parts = faiss.__version__.split('.')
            major = int(version_parts[0])
            minor = int(version_parts[1])

            if major < 1 or (major == 1 and minor < 7):
                tips.append("å»ºè®®å‡çº§FAISSåˆ°1.7.0+ç‰ˆæœ¬ä»¥è·å¾—efConstruction/efSearchå‚æ•°æ”¯æŒ")
        except:
            tips.append("æ— æ³•æ£€æµ‹FAISSç‰ˆæœ¬ï¼Œå»ºè®®å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬")

        # æ£€æŸ¥efå‚æ•°
        if not hasattr(self.index, 'efConstruction'):
            tips.append("å½“å‰FAISSç‰ˆæœ¬ä¸æ”¯æŒefConstructionå‚æ•°ï¼Œæœç´¢æ€§èƒ½å¯èƒ½å—é™")

        if not hasattr(self.index, 'efSearch'):
            tips.append("å½“å‰FAISSç‰ˆæœ¬ä¸æ”¯æŒefSearchå‚æ•°ï¼Œå»ºè®®æ‰‹åŠ¨è®¾ç½®æœç´¢å‚æ•°")

        # æ£€æŸ¥å‘é‡æ•°é‡
        if self.index.ntotal < 1000:
            tips.append("å‘é‡æ•°é‡è¾ƒå°‘ï¼Œè€ƒè™‘å¢åŠ æ›´å¤šå•†å“æ•°æ®ä»¥æé«˜æœç´¢å‡†ç¡®æ€§")

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        memory_mb = self._estimate_memory_usage()
        if memory_mb > 1000:  # è¶…è¿‡1GB
            tips.append(f"å†…å­˜ä½¿ç”¨é‡è¾ƒå¤§ ({memory_mb:.1f}MB)ï¼Œå»ºè®®ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")

        return tips if tips else ["ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œæ— æ€§èƒ½ä¼˜åŒ–å»ºè®®"]

    def _estimate_memory_usage(self) -> float:
        """ä¼°ç®—å†…å­˜ä½¿ç”¨é‡ (MB)"""
        # HNSWç´¢å¼•å†…å­˜ä¼°ç®—ï¼šå‘é‡æ•°æ® + å›¾ç»“æ„
        vector_memory = self.index.ntotal * self.dimension * 4  # float32 = 4 bytes
        graph_memory = self.index.ntotal * config.FAISS_HNSW_M * 4  # é‚»å±…æŒ‡é’ˆ
        total_bytes = vector_memory + graph_memory
        return total_bytes / (1024 * 1024)

# å…¨å±€å•ä¾‹
_engine = None

def get_vector_engine() -> VectorEngine:
    global _engine
    if _engine is None:
        _engine = VectorEngine()
    return _engine
